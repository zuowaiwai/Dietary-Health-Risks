#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NHANES æ•°æ®æ–‡ä»¶æ•´ç†å’Œé¢„å¤„ç†è„šæœ¬
åŠŸèƒ½ï¼š
1. å°†æ•£ä¹±çš„NHANES .xptæ–‡ä»¶æŒ‰ç…§æ•°æ®ç±»åˆ«å’Œè°ƒæŸ¥å‘¨æœŸè¿›è¡Œåˆ†ç±»æ•´ç†
2. å°†XPTæ–‡ä»¶è½¬æ¢ä¸ºCSVæ ¼å¼
3. æ ‡å‡†åŒ–å…³é”®åˆ—å
4. åˆå¹¶å¹´åº¦æ•°æ®ä¸ºå•ä¸ªæ–‡ä»¶
ä½œè€…ï¼šAIåŠ©æ‰‹
æ—¥æœŸï¼š2024
"""

import os
import shutil
import re
import pandas as pd
import logging
import json
from pathlib import Path
from tqdm import tqdm
from datetime import datetime

# ================== é…ç½®åŒºåŸŸ ==================
# æºç›®å½• - åŒ…å«æ‰€æœ‰æœªæ•´ç†çš„.xptæ–‡ä»¶
SOURCE_DIR = "NHANES_Organized/_Uncategorized"

# ç›®æ ‡ç›®å½• - æ•´ç†åçš„æ–‡ä»¶å°†å­˜æ”¾åœ¨è¿™é‡Œ
DEST_DIR = "NHANES_Organized"

# CSVè¾“å‡ºç›®å½• - è½¬æ¢åçš„CSVæ–‡ä»¶å­˜æ”¾ç›®å½•
CSV_OUTPUT_DIR = "NHANES_PROCESSED_CSV"

# CSVåŸå§‹è¾“å‡ºç›®å½• - ä¸å«æ ‡ç­¾å¢å¼ºçš„CSVæ–‡ä»¶å­˜æ”¾ç›®å½•
CSV_OUTPUT_DIR_RAW = "Nhanes_processed_csv_raw"

# å˜é‡æè¿°ç›®å½• - åŒ…å«å˜é‡æ ‡ç­¾å’Œæ•°å€¼æ ‡ç­¾ä¿¡æ¯
VARIABLES_DIR = "NHANES_Variables"

# æ—¥å¿—æ–‡ä»¶è·¯å¾„
LOG_FILE = "nhanes_preprocessing.log"

# ================== æ—¥å¿—é…ç½® ==================

def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )
    return logging.getLogger(__name__)

# ================== æ˜ å°„å­—å…¸ ==================

# å¹´ä»½æ˜ å°„ï¼šä»æ–‡ä»¶ååç¼€åˆ°è°ƒæŸ¥å‘¨æœŸ
YEAR_MAP = {
    '_B': '2001-2002',
    '_C': '2003-2004', 
    '_D': '2005-2006',
    '_E': '2007-2008',
    '_F': '2009-2010',
    '_G': '2011-2012',
    '_H': '2013-2014',
    '_I': '2015-2016',
    '_J': '2017-2018',
    '_L': '2021-2022',
    'P_': '2019-2020',  # P_å‰ç¼€è¡¨ç¤ºç–«æƒ…æœŸé—´æ•°æ®
    '': '1999-2000'     # æ— åç¼€çš„æ–‡ä»¶é€šå¸¸æ˜¯1999-2000å¹´
}

# æ ‡å‡†åˆ—åæ˜ å°„ï¼šç”¨äºç»Ÿä¸€å…³é”®åˆ—å
COLUMN_RENAME_MAP = {
    'SEQN': 'RespondentSequenceNumber',
    'SDDSRVYR': 'DataReleaseCycle',
    'RIDSTATR': 'InterviewStatus',
    'RIAGENDR': 'Gender',
    'RIDAGEYR': 'AgeInYearsAtScreening',
    'RIDAGEMN': 'AgeInMonthsAtScreening',
    'RIDRETH1': 'RaceEthnicityRecode1',
    'RIDRETH3': 'RaceEthnicityRecode3',
    'DMDBORN4': 'CountryOfBirth',
    'WTINT2YR': 'FullSample2YearInterviewWeight',
    'WTMEC2YR': 'FullSample2YearMECExamWeight',
    # å¯ä»¥æ ¹æ®éœ€è¦ç»§ç»­æ·»åŠ 
}

# ç±»åˆ«æ˜ å°„ï¼šä»æ–‡ä»¶åå‰ç¼€åˆ°ç±»åˆ«è·¯å¾„
CATEGORY_MAP = {
    # A. é—®å·è°ƒæŸ¥æ•°æ®
    # 1. äººå£ç»Ÿè®¡å­¦ä¿¡æ¯
    'DEMO': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '1_äººå£ç»Ÿè®¡å­¦ä¿¡æ¯'),
    
    # 2. è†³é£Ÿæ•°æ®
    'DR1TOT': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_æ€»æ‘„å…¥é‡'),
    'DR2TOT': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_æ€»æ‘„å…¥é‡'),
    'DR1IFF': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_å•ä¸ªé£Ÿç‰©'),
    'DR2IFF': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_å•ä¸ªé£Ÿç‰©'),
    'DRXFCD': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_é£Ÿç‰©ç¼–ç '),
    'DRXFMT': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_é£Ÿç‰©ç¼–ç '),
    'DRXMCD': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_é£Ÿç‰©ç¼–ç '),
    'DRXTOT': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_æ€»æ‘„å…¥é‡'),
    'DRXIFF': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿæ•°æ®_å•ä¸ªé£Ÿç‰©'),
    'DBQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_è†³é£Ÿè¡Œä¸ºé—®å·'),
    'FFQDC': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_é£Ÿç‰©é¢‘ç‡é—®å·'),
    'FFQRAW': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '2_é£Ÿç‰©é¢‘ç‡é—®å·'),
    
    # 3. è†³é£Ÿè¡¥å……å‰‚
    'DSQTOT': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_æ€»é‡'),
    'DSQIDS': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_æ˜ç»†'),
    'DS1TOT': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_ç¬¬ä¸€å¤©'),
    'DS1IDS': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_ç¬¬ä¸€å¤©'),
    'DS2TOT': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_ç¬¬äºŒå¤©'),
    'DS2IDS': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_ç¬¬äºŒå¤©'),
    'DSQ1': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_é—®å·1'),
    'DSQ2': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_é—®å·2'),
    'DSBI': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_å“ç‰Œä¿¡æ¯'),
    'DSII': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_æˆåˆ†ä¿¡æ¯'),
    'DSPI': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_äº§å“ä¿¡æ¯'),
    'DSQFILE1': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_æ–‡ä»¶1'),
    'DSQFILE2': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '3_è†³é£Ÿè¡¥å……å‰‚_æ–‡ä»¶2'),
    
    # 4. ç”Ÿæ´»æ–¹å¼
    'PAQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_ä½“åŠ›æ´»åŠ¨'),
    'PAQIAF': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_ä½“åŠ›æ´»åŠ¨_ä¸ªäºº'),
    'SMQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_å¸çƒŸ_ä¸ªäºº'),
    'SMQFAM': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_å¸çƒŸ_å®¶åº­'),
    'SMQRTU': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_å¸çƒŸ_è¿‘æœŸä½¿ç”¨'),
    'SMQSHS': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_äºŒæ‰‹çƒŸ'),
    'SMQMEC': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_å¸çƒŸ_MEC'),
    'ALQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_é¥®é…’'),
    'ALQY': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_é¥®é…’_é’å¹´'),
    'SLQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_ç¡çœ '),
    'SXQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_æ€§è¡Œä¸º'),
    'DUQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '4_ç”Ÿæ´»æ–¹å¼_è¯ç‰©ä½¿ç”¨'),
    
    # 5. ç–¾ç—…å²å’Œå¥åº·çŠ¶å†µ
    'MCQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_ç–¾ç—…å²_ä¸€èˆ¬'),
    'DIQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_ç–¾ç—…å²_ç³–å°¿ç—…'),
    'BPQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_ç–¾ç—…å²_è¡€å‹'),
    'CDQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_ç–¾ç—…å²_å¿ƒè¡€ç®¡'),
    'CKQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_ç–¾ç—…å²_æ…¢æ€§è‚¾ç—…'),
    'KIQ_U': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_ç–¾ç—…å²_è‚¾è„_æ³Œå°¿'),
    'HSQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_å¥åº·çŠ¶å†µ_å½“å‰'),
    'HUQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_å¥åº·çŠ¶å†µ_åŒ»ç–—åˆ©ç”¨'),
    'HIQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_å¥åº·çŠ¶å†µ_åŒ»ç–—ä¿é™©'),
    'HEQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_å¥åº·çŠ¶å†µ_å¬åŠ›'),
    'CFQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_è®¤çŸ¥åŠŸèƒ½'),
    'DLQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_ç‰©ç†åŠŸèƒ½_æ®‹ç–¾'),
    'PFQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '5_ç‰©ç†åŠŸèƒ½'),
    
    # 6. è¯ç‰©å’Œä¿å¥å“
    'RXQ_RX': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '6_å¤„æ–¹è¯ç‰©'),
    'RXQANA': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '6_å¤„æ–¹è¯ç‰©_åˆ†æ'),
    'RXQ_DRUG': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '6_å¤„æ–¹è¯ç‰©_è¯¦ç»†'),
    
    # 7. å¥³æ€§å¥åº·
    'RHQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '7_å¥³æ€§å¥åº·_ç”Ÿæ®–'),
    'OHQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '7_å¥³æ€§å¥åº·_å£æœé¿å­•è¯'),
    
    # 8. å…ç–«å’Œç–«è‹—
    'IMQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '8_å…ç–«_ç–«è‹—å²'),
    
    # 9. èŒä¸šå’Œç¯å¢ƒ
    'OCQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '9_èŒä¸š_å·¥ä½œ'),
    'OPD': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '9_èŒä¸š_å†œè¯'),
    
    # 10. ä½æˆ¿å’Œç¯å¢ƒ
    'HOQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '10_ä½æˆ¿_ç‰¹å¾'),
    'DEQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '10_çš®è‚¤ç—…_é˜²æ™’'),
    
    # 11. ç²¾ç¥å¥åº·
    'DPQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '11_ç²¾ç¥å¥åº·_æŠ‘éƒ'),
    'CIQDEP': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '11_ç²¾ç¥å¥åº·_æŠ‘éƒ_CIDI'),
    'CIQGAD': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '11_ç²¾ç¥å¥åº·_ç„¦è™‘_CIDI'),
    'CIQPAN': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '11_ç²¾ç¥å¥åº·_æƒŠæ_CIDI'),
    'CIQPANIC': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '11_ç²¾ç¥å¥åº·_æƒŠæ_CIDI'),
    
    # 12. ä½“é‡å’ŒèŠ‚é£Ÿ
    'WHQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '12_ä½“é‡_å†å²'),
    'WHQMEC': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '12_ä½“é‡_MEC'),
    'WDQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '12_ä½“é‡_èŠ‚é£Ÿ'),
    
    # 13. å…¶ä»–é—®å·
    'AUQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_å¬åŠ›é—®å·'),
    'VTQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_æŒ¥å‘æ€§æ¯’ç‰©'),
    'VIQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_è§†åŠ›_çœ¼ç§‘'),
    'TBQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_ç»“æ ¸ç—…'),
    'CSQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_åŒ»ç–—æ¡ä»¶ç­›æŸ¥'),
    'SSQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_ç¤¾ä¼šæ”¯æŒ'),
    'BHQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_è¡Œä¸ºå¥åº·'),
    'CBQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_æ¶ˆè´¹è¡Œä¸º'),
    'CBQPFA': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_æ¶ˆè´¹è¡Œä¸º_PFAS'),
    'CBQPFC': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_æ¶ˆè´¹è¡Œä¸º_PFAS'),
    'PUQMEC': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_æ€è™«å‰‚'),
    'PUQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_æ€è™«å‰‚_è¯¦ç»†'),
    'ACQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_ä½å®¿æ¡ä»¶'),
    'BAQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_æ–‡èº«'),
    'ECQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_å¹¼å„¿æŠ¤ç†'),
    'AQQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_ç©ºæ°”è´¨é‡'),
    'ARQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_å…³èŠ‚ç‚'),
    'DTQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_ç‰™ç§‘'),
    'FNQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_é£Ÿå“å®‰å…¨'),
    'FSQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_é£Ÿå“å®‰å…¨_è¯¦ç»†'),
    'RDQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_å‘¼å¸é“å¥åº·'),
    'OSQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_èŒä¸šå¥åº·'),
    'MPQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_ç²¾ç¥å¥åº·_è¯¦ç»†'),
    'INQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_æ”¶å…¥é—®å·'),
    'HCQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_ä¸™è‚é—®å·'),
    'PSQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_å‰åˆ—è…ºç—‡çŠ¶'),
    'KIQ_P': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_å„¿ç§‘è‚¾è„'),
    'KIQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_è‚¾è„ç–¾ç—…'),
    'RXQASA': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_é˜¿å¸åŒ¹æ—ä½¿ç”¨'),
    'RXQ_ANA': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_è¯ç‰©åˆ†æ'),
    'AGQ': ('A_é—®å·è°ƒæŸ¥æ•°æ®', '13_å…¶ä»–_å“®å–˜é—®å·'),
    
    # B. ä½“æ ¼æ£€æŸ¥æ•°æ®
    # 1. äººä½“æµ‹é‡
    'BMX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '1_äººä½“æµ‹é‡_èº«ä½“'),
    
    # 2. è¡€å‹
    'BPX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '2_è¡€å‹_æ°´é“¶'),
    'BPXO': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '2_è¡€å‹_ç¤ºæ³¢å™¨'),
    
    # 3. å£è…”å¥åº·
    'OHX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '3_å£è…”å¥åº·_æ£€æŸ¥'),
    'OHXDEN': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '3_å£è…”å¥åº·_ç‰™é½¿'),
    'OHXREF': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '3_å£è…”å¥åº·_æ¨è'),
    'OHXPER': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '3_å£è…”å¥åº·_ç‰™å‘¨'),
    
    # 4. éª¨å¯†åº¦
    'DXX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_å…¨èº«'),
    'DXXAG': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_å¹´é¾„è°ƒæ•´'),
    'DXXFEM': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_è‚¡éª¨'),
    'DXXSPN': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_è„ŠæŸ±'),
    'DXXFRX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_éª¨æŠ˜å²'),
    'DXXL1': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_L1æ¤ä½“'),
    'DXXL2': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_L2æ¤ä½“'),
    'DXXL3': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_L3æ¤ä½“'),
    'DXXL4': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_L4æ¤ä½“'),
    'DXXT5': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_T5æ¤ä½“'),
    'DXXT6': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_T6æ¤ä½“'),
    'DXXT7': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_T7æ¤ä½“'),
    'DXXT8': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_T8æ¤ä½“'),
    'DXXT10': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_T10æ¤ä½“'),
    'DXXT11': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_T11æ¤ä½“'),
    'DXXT12': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_T12æ¤ä½“'),
    'DXXAAC': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_è…¹ä¸»åŠ¨è„‰é’™åŒ–'),
    'DXXVFA': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_æ¤ä½“éª¨æŠ˜è¯„ä¼°'),
    'DXX_2': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '4_éª¨å¯†åº¦_é™„åŠ æµ‹é‡'),
    
    # 5. å¬åŠ›æµ‹è¯•
    'AUX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '5_å¬åŠ›_çº¯éŸ³'),
    'AUXAR': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '5_å¬åŠ›_å£°åå°„'),
    'AUXTYM': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '5_å¬åŠ›_é¼“è†œ'),
    'AUXWBR': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '5_å¬åŠ›_å®½å¸¦åå°„'),
    
    # 6. çš®è‚¤ç—…å­¦
    'DEX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '6_çš®è‚¤_æ£€æŸ¥'),
    
    # 7. å¿ƒè¡€ç®¡å¥åº·
    'CVX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '7_å¿ƒè¡€ç®¡_æ£€æŸ¥'),
    
    # 8. çœ¼ç§‘æ£€æŸ¥
    'VIX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '8_çœ¼ç§‘_è§†åŠ›æ£€æŸ¥'),
    'OPXRET': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '8_çœ¼ç§‘_è§†ç½‘è†œæ‘„å½±'),
    'OPXFDT': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '8_çœ¼ç§‘_çœ¼åº•æ£€æŸ¥'),
    
    # 9. ä½“åŠ›æ´»åŠ¨ç›‘æµ‹
    'PAXHR': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '9_ä½“åŠ›æ´»åŠ¨_å¿ƒç‡'),
    'PAXHD': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '9_ä½“åŠ›æ´»åŠ¨_å¤´éƒ¨ä½ç½®'),
    'PAXDAY': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '9_ä½“åŠ›æ´»åŠ¨_æ—¥é—´æ•°æ®'),
    
    # 10. å…¶ä»–æ£€æŸ¥
    'BAX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '10_å…¶ä»–_æ–‡èº«'),
    'BIX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '10_å…¶ä»–_ç”Ÿç‰©é˜»æŠ—'),
    'ARX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '10_å…¶ä»–_å…³èŠ‚ç‚'),
    'ENX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '10_å…¶ä»–_ç¯å¢ƒæ°®æ°§åŒ–ç‰©'),
    'CSX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '10_å…¶ä»–_åŒ»ç–—æ¡ä»¶ç­›æŸ¥'),
    'SPX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '10_å…¶ä»–_è‚ºæ´»é‡'),
    'MSX': ('B_ä½“æ ¼æ£€æŸ¥æ•°æ®', '10_å…¶ä»–_ç²¾ç¥çŠ¶æ€'),
    
    # C. å®éªŒå®¤æ•°æ®
    # 1. å¸¸è§„ç”ŸåŒ– - è¡€è„‚
    'TCHOL': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€è„‚_æ€»èƒ†å›ºé†‡'),
    'HDL': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€è„‚_HDL'),
    'LDL': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€è„‚_LDL'),
    'TRIGLY': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€è„‚_ç”˜æ²¹ä¸‰é…¯'),
    'APOB': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€è„‚_è½½è„‚è›‹ç™½B'),
    
    # 1. å¸¸è§„ç”ŸåŒ– - è¡€ç³–
    'GLU': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€ç³–_è‘¡è„ç³–'),
    'GHB': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€ç³–_ç³–åŒ–è¡€çº¢è›‹ç™½'),
    'INS': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€ç³–_èƒ°å²›ç´ '),
    'OGTT': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è¡€ç³–_ç³–è€é‡'),
    
    # 1. å¸¸è§„ç”ŸåŒ– - å…¶ä»–
    'BIOPRO': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_ç”ŸåŒ–æŒ‡æ ‡'),
    'THYROD': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_ç”²çŠ¶è…ºåŠŸèƒ½'),
    'PSA': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_å‰åˆ—è…ºç‰¹å¼‚æ€§æŠ—åŸ'),
    'PTH': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_ç”²çŠ¶æ—è…ºæ¿€ç´ '),
    'PP': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_è›‹ç™½è´¨'),
    'PH': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_é…¸ç¢±åº¦'),
    'HCY': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_åŒå‹åŠèƒ±æ°¨é…¸'),
    'MMA': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_ç”²åŸºä¸™äºŒé…¸'),
    'IHG': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_æ— æœºæ±'),
    'MGX': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_é•'),
    'FASTQX': ('C_å®éªŒå®¤æ•°æ®', '1_å¸¸è§„ç”ŸåŒ–_ç¦é£Ÿé—®å·'),
    
    # 2. è¡€æ¶²å­¦
    'CBC': ('C_å®éªŒå®¤æ•°æ®', '2_è¡€æ¶²å­¦_å…¨è¡€ç»†èƒè®¡æ•°'),
    
    # 3. è¥å…»çŠ¶å†µ
    'FERTIN': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_é“è›‹ç™½'),
    'FETIB': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_é“ç»“åˆåŠ›'),
    'TFR': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_è½¬é“è›‹ç™½å—ä½“'),
    'VID': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_ç»´ç”Ÿç´ D'),
    'VIT': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_ç»´ç”Ÿç´ '),
    'VIC': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_ç»´ç”Ÿç´ C'),
    'FOLATE': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_å¶é…¸'),
    'FOLFMS': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_å¶é…¸ä»£è°¢ç‰©'),
    'B12': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_ç»´ç”Ÿç´ B12'),
    'CARB': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_ç±»èƒ¡èåœç´ '),
    'TFA': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_åå¼è„‚è‚ªé…¸'),
    'PHYTO': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_æ¤ç‰©é›Œæ¿€ç´ '),
    'PERNT': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_å…¨æ°Ÿå’Œå¤šæ°ŸåŒ–åˆç‰©'),
    'PERNTS': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_å…¨æ°Ÿå’Œå¤šæ°ŸåŒ–åˆç‰©_è¡€æ¸…'),
    'CAFE': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_å’–å•¡å› '),
    'EPH': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_éº»é»„ç¢±'),
    'EPHPP': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_éº»é»„ç¢±ä»£è°¢ç‰©'),
    'EPP': ('C_å®éªŒå®¤æ•°æ®', '3_è¥å…»çŠ¶å†µ_åŸå•‰'),
    
    # 4. ç‚ç—‡æ ‡å¿—ç‰©
    'CRP': ('C_å®éªŒå®¤æ•°æ®', '4_ç‚ç—‡_Cååº”è›‹ç™½'),
    'HSCRP': ('C_å®éªŒå®¤æ•°æ®', '4_ç‚ç—‡_é«˜æ•Cååº”è›‹ç™½'),
    
    # 5. è‚è‚¾åŠŸèƒ½
    'ALB_CR': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_ç™½è›‹ç™½è‚Œé…'),
    'UCFLOW': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿æµç‡'),
    'UCPREG': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿å¦Šå¨ '),
    'UCOT': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿è‚Œé…'),
    'UC': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿è‚Œé…_ç®€ç‰ˆ'),
    'UCM': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿è‚Œé…_å¾®é‡ç™½è›‹ç™½'),
    'UCOSMO': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿æ¸—é€å‹'),
    'UTASS': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿ç ·'),
    'UAS': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿ç ·_è¯¦ç»†'),
    'UTAS': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿æ€»ç ·'),
    'UIO': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿ç¢˜'),
    'UM': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿æ±'),
    'UMS': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿æ±_æ±ä»£è°¢ç‰©'),
    'UAM': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿ç ·ä»£è°¢ç‰©'),
    'UADM': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿ç ·äºŒç”²åŸºé…¸'),
    'UPP': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿åŸå•‰åŸ'),
    'UNI': ('C_å®éªŒå®¤æ•°æ®', '5_è‚è‚¾åŠŸèƒ½_å°¿é•'),
    
    # 6. æ„ŸæŸ“æ€§ç–¾ç—…
    'HEPB_S': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ä¹™è‚è¡¨é¢æŠ—åŸ'),
    'HEPA': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ç”²è‚'),
    'HEPBD': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ä¹™è‚DNA'),
    'HEPC': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ä¸™è‚'),
    'HEPD': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ä¸è‚'),
    'HEPE': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_æˆŠè‚'),
    'VNA': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ç—…æ¯’æ€§è‚ç‚'),
    'VNAS': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ç—…æ¯’æ€§è‚ç‚_è¡€æ¸…'),
    'HIV': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_HIV'),
    'HSV': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ç–±ç–¹ç—…æ¯’'),
    'CMV': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_å·¨ç»†èƒç—…æ¯’'),
    'CHLMDA': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_è¡£åŸä½“'),
    'TRICH': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_æ»´è™«'),
    'TGEMA': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_å¼“å½¢è™«'),
    'TB': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ç»“æ ¸ç—…'),
    'TST': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ç»“æ ¸èŒç´ çš®è¯•'),
    'TELO': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_ç«¯ç²’é•¿åº¦'),
    'HPVSWR': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_HPV_å£è…”'),
    'ORHPV': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_HPV_å£è…”æ£€æµ‹'),
    'HPVP': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_HPV_ç§å¯†éƒ¨ä½'),
    'HPVSER': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_HPV_è¡€æ¸…'),
    'HPVSWC': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_HPV_å®«é¢ˆ'),
    'HPVSRM': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_HPV_å®¤æ¸©'),
    'MMRV': ('C_å®éªŒå®¤æ•°æ®', '6_æ„ŸæŸ“_MMRV_ç–«è‹—'),
    
    # 7. å…ç–«ååº”
    'AL_IGE': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_è¿‡æ•åŸIgE'),
    'ALD': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_è¿‡æ•åŸ'),
    'ALDS': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_è¿‡æ•åŸç­›æŸ¥'),
    'ALDUST': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_è¿‡æ•åŸå°˜è¨'),
    'AGP': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_é…¸æ€§ç³–è›‹ç™½'),
    'SSAGP': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_è¡€æ¸…é…¸æ€§ç³–è›‹ç™½'),
    'SS': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_è¡€æ¸…å­¦'),
    'SSCMVG': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_å·¨ç»†èƒç—…æ¯’IgG'),
    'SSBFR': ('C_å®éªŒå®¤æ•°æ®', '7_å…ç–«_è¡€æ¸…æŠ—ä½“'),
    
    # 8. ç¯å¢ƒæš´éœ² - é‡é‡‘å±
    'PBCD': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_é‡é‡‘å±_é“…é•‰'),
    'UHG': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_é‡é‡‘å±_å°¿æ±'),
    'UHM': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_é‡é‡‘å±_å°¿æ±ç”²åŸº'),
    'IHGEM': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_é‡é‡‘å±_æ±'),
    'CUSEZN': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_é‡é‡‘å±_é“œç¡’é”Œ'),
    
    # 8. ç¯å¢ƒæš´éœ² - æœ‰æœºæ±¡æŸ“ç‰©
    'VOC': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_æŒ¥å‘æ€§'),
    'VOCWB': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_å…¨è¡€æŒ¥å‘æ€§'),
    'UVOC': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_å°¿æŒ¥å‘æ€§'),
    'DEET': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_DEET'),
    'ETHOX': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_ä¹™æ°§åŸºåŒ–åˆç‰©'),
    'ETHOXS': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_ä¹™æ°§åŸºä»£è°¢ç‰©'),
    'FLXCLN': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_æ°Ÿæ°¯çƒƒ'),
    'FLDEP': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_æ°ŸåŒ–ç‰©è¡€æµ†'),
    'FLDEW': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_æ°ŸåŒ–ç‰©æ°´'),
    'PFC': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_å…¨æ°ŸåŒ–åˆç‰©'),
    'PAH': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_å¤šç¯èŠ³çƒƒ'),
    'PAHS': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_å¤šç¯èŠ³çƒƒä»£è°¢ç‰©'),
    'PHPYPA': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_é‚»è‹¯äºŒç”²é…¸ç›é…šç±»'),
    'PHTHTE': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_é‚»è‹¯äºŒç”²é…¸é…¯'),
    'FORMAS': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_ç”²é†›'),
    'FR': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æœ‰æœºç‰©_é˜»ç‡ƒå‰‚'),
    
    # 8. ç¯å¢ƒæš´éœ² - æŒä¹…æ€§æœ‰æœºæ±¡æŸ“ç‰©
    'PCBPOL': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æŒä¹…æ€§æœ‰æœºç‰©_PCB'),
    'DOXPOL': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æŒä¹…æ€§æœ‰æœºç‰©_äºŒæ¶è‹±'),
    'BFRPOL': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æŒä¹…æ€§æœ‰æœºç‰©_æº´åŒ–é˜»ç‡ƒå‰‚'),
    'POC': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_æŒä¹…æ€§æœ‰æœºç‰©'),
    
    # 8. ç¯å¢ƒæš´éœ² - å†œè¯
    'PSTPOL': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_å†œè¯_æœ‰æœºç£·'),
    'UPHOPM': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_å†œè¯_å°¿æœ‰æœºç£·ä»£è°¢ç‰©'),
    'PEST': ('C_å®éªŒå®¤æ•°æ®', '8_ç¯å¢ƒæš´éœ²_å†œè¯'),
    
    # 9. æ¯’ç‰©å’Œè¯ç‰©
    'COT': ('C_å®éªŒå®¤æ•°æ®', '9_æ¯’ç‰©_å°¼å¤ä¸ä»£è°¢ç‰©'),
    'COTNAL': ('C_å®éªŒå®¤æ•°æ®', '9_æ¯’ç‰©_å°¼å¤ä¸TSNAs'),
    'TSNA': ('C_å®éªŒå®¤æ•°æ®', '9_æ¯’ç‰©_çƒŸè‰ç‰¹æœ‰äºšç¡èƒº'),
    'CRCO': ('C_å®éªŒå®¤æ•°æ®', '9_æ¯’ç‰©_é“¬é’´'),
    
    # 10. ç‰¹æ®Šæ£€æµ‹
    'FAS': ('C_å®éªŒå®¤æ•°æ®', '10_ç‰¹æ®Šæ£€æµ‹_è„‚è‚ªé…¸'),
    'AA': ('C_å®éªŒå®¤æ•°æ®', '10_ç‰¹æ®Šæ£€æµ‹_èŠ³é¦™èƒº'),
    'AAS': ('C_å®éªŒå®¤æ•°æ®', '10_ç‰¹æ®Šæ£€æµ‹_èŠ³é¦™èƒºä»£è°¢ç‰©'),
    'AMDGDS': ('C_å®éªŒå®¤æ•°æ®', '10_ç‰¹æ®Šæ£€æµ‹_AMDåŸºå› åˆ†å‹'),
    'AMDGYD': ('C_å®éªŒå®¤æ•°æ®', '10_ç‰¹æ®Šæ£€æµ‹_AMDåŸºå› å‹'),
    
    # 11. åŸºå› å’Œåˆ†å­æ ‡å¿—ç‰©
    'PUQMEC': ('C_å®éªŒå®¤æ•°æ®', '11_åŸºå› _æ€è™«å‰‚é—®å·'),
    
    # 12. å…¶ä»–ç‰¹æ®Šæ£€æµ‹
    'WPIN': ('C_å®éªŒå®¤æ•°æ®', '12_ç‰¹æ®Šæ£€æµ‹_æ°´å¹³è›‹ç™½'),
    'SEQ': ('C_å®éªŒå®¤æ•°æ®', '12_ç‰¹æ®Šæ£€æµ‹_åŸºå› æµ‹åº'),
    
    # Lå¼€å¤´çš„å®éªŒå®¤æ–‡ä»¶
    'L': ('C_å®éªŒå®¤æ•°æ®', '13_æ—©æœŸå®éªŒå®¤æ•°æ®'),
    'L06UAS': ('C_å®éªŒå®¤æ•°æ®', '13_æ—©æœŸå®éªŒå®¤æ•°æ®_å°¿ç ·'),
    'L13_2': ('C_å®éªŒå®¤æ•°æ®', '13_æ—©æœŸå®éªŒå®¤æ•°æ®_13_2'),
    'L28POC': ('C_å®éªŒå®¤æ•°æ®', '13_æ—©æœŸå®éªŒå®¤æ•°æ®_28POC'),
    'L36': ('C_å®éªŒå®¤æ•°æ®', '13_æ—©æœŸå®éªŒå®¤æ•°æ®_36'),
}

def parse_filename(filename):
    """
    è§£æNHANESæ–‡ä»¶åï¼Œæå–å‰ç¼€å’Œåç¼€
    
    å‚æ•°:
        filename: æ–‡ä»¶å (ä¾‹å¦‚: 'DEMO_J.XPT', 'P_DEMO.XPT', 'WHQ.XPT')
    
    è¿”å›:
        tuple: (prefix, suffix) ä¾‹å¦‚: ('DEMO', '_J'), ('DEMO', 'P_'), ('WHQ', '')
    """
    # ç§»é™¤æ–‡ä»¶æ‰©å±•åå¹¶è½¬ä¸ºå¤§å†™
    name = filename.upper().replace('.XPT', '')
    
    # å¤„ç†P_å‰ç¼€çš„æƒ…å†µ (å¦‚ P_DEMO.XPT)
    if name.startswith('P_'):
        prefix = name[2:]  # ç§»é™¤P_
        suffix = 'P_'
        return prefix, suffix
    
    # å¤„ç†æ ‡å‡†åç¼€çš„æƒ…å†µ (å¦‚ DEMO_J.XPT)
    for suffix in ['_B', '_C', '_D', '_E', '_F', '_G', '_H', '_I', '_J', '_L']:
        if name.endswith(suffix):
            prefix = name[:-2]  # ç§»é™¤åç¼€
            return prefix, suffix
    
    # å¤„ç†æ— åç¼€çš„æƒ…å†µ (å¦‚ WHQ.XPT)
    return name, ''

def find_category(prefix):
    """
    æ ¹æ®å‰ç¼€æŸ¥æ‰¾æ–‡ä»¶ç±»åˆ«
    
    å‚æ•°:
        prefix: æ–‡ä»¶å‰ç¼€
        
    è¿”å›:
        tuple: (ä¸»ç±»åˆ«, å­ç±»åˆ«) æˆ– None
    """
    # ç›´æ¥åŒ¹é…
    if prefix in CATEGORY_MAP:
        return CATEGORY_MAP[prefix]
    
    # æ¨¡ç³ŠåŒ¹é… - æŸ¥æ‰¾åŒ…å«å‰ç¼€çš„é”®
    for key in CATEGORY_MAP:
        if prefix.startswith(key) or key in prefix:
            return CATEGORY_MAP[key]
    
    # ç‰¹æ®Šå¤„ç†Lå¼€å¤´çš„æ–‡ä»¶
    if prefix.startswith('L') and len(prefix) > 1:
        return CATEGORY_MAP['L']
    
    return None

def load_variable_info(table_name, variable_name):
    """
    åŠ è½½å˜é‡çš„æ ‡ç­¾ä¿¡æ¯
    
    å‚æ•°:
        table_name: æ•°æ®è¡¨åï¼ˆå¦‚DEMO_Jï¼‰
        variable_name: å˜é‡åï¼ˆå¦‚SEQNï¼‰
    
    è¿”å›:
        dict: åŒ…å«å˜é‡ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    try:
        info_file = Path(VARIABLES_DIR) / table_name / variable_name / f"{variable_name}_info.json"
        if info_file.exists():
            with open(info_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception:
        pass
    return None

def load_variable_values(table_name, variable_name):
    """
    åŠ è½½å˜é‡çš„æ•°å€¼æ ‡ç­¾æ˜ å°„
    
    å‚æ•°:
        table_name: æ•°æ®è¡¨åï¼ˆå¦‚DEMO_Jï¼‰
        variable_name: å˜é‡åï¼ˆå¦‚RIAGENDRï¼‰
    
    è¿”å›:
        dict: æ•°å€¼åˆ°æ ‡ç­¾çš„æ˜ å°„å­—å…¸ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    try:
        values_file = Path(VARIABLES_DIR) / table_name / variable_name / f"{variable_name}_values.csv"
        if values_file.exists():
            df = pd.read_csv(values_file)
            # åˆ›å»ºä»ç¼–ç åˆ°æè¿°çš„æ˜ å°„
            value_map = {}
            for _, row in df.iterrows():
                code = str(row['Code or Value']).strip()
                desc = str(row['Value Description']).strip()
                if code != '.' and code != 'nan':  # æ’é™¤ç¼ºå¤±å€¼
                    value_map[code] = desc
            return value_map
    except Exception:
        pass
    return None

def enhance_csv_with_labels(csv_file_path, table_name, logger):
    """
    å¢å¼ºCSVæ–‡ä»¶ï¼šæ›¿æ¢åˆ—åä¸ºæ ‡ç­¾ï¼Œæ›¿æ¢æ•°å€¼ä¸ºæè¿°
    
    å‚æ•°:
        csv_file_path: CSVæ–‡ä»¶è·¯å¾„
        table_name: æ•°æ®è¡¨å
        logger: æ—¥å¿—è®°å½•å™¨
    
    è¿”å›:
        bool: å¤„ç†æ˜¯å¦æˆåŠŸ
    """
    try:
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
        original_columns = df.columns.tolist()
        
        # 1. æ›¿æ¢åˆ—åä¸ºæ ‡ç­¾
        column_rename_map = {}
        for col in df.columns:
            if col == 'YearRange':  # è·³è¿‡æˆ‘ä»¬æ·»åŠ çš„å¹´ä»½åˆ—
                continue
                
            var_info = load_variable_info(table_name, col)
            if var_info and 'Label' in var_info:
                # ä½¿ç”¨æ ‡ç­¾ä½œä¸ºæ–°åˆ—åï¼Œå¦‚æœæ ‡ç­¾ä¸ºç©ºåˆ™ä¿æŒåŸå
                label = var_info['Label'].strip()
                if label:
                    column_rename_map[col] = label
                    
        if column_rename_map:
            df = df.rename(columns=column_rename_map)
            logger.info(f"é‡å‘½ååˆ— {list(column_rename_map.keys())} -> {list(column_rename_map.values())}")
        
        # 2. æ›¿æ¢æ•°å€¼ä¸ºæè¿°
        enhanced_columns = 0
        for original_col in original_columns:
            if original_col == 'YearRange':  # è·³è¿‡å¹´ä»½åˆ—
                continue
                
            # è·å–å½“å‰åˆ—åï¼ˆå¯èƒ½å·²è¢«é‡å‘½åï¼‰
            current_col = column_rename_map.get(original_col, original_col)
            
            if current_col in df.columns:
                value_map = load_variable_values(table_name, original_col)
                if value_map:
                    # å°†æ•°å€¼æ›¿æ¢ä¸ºæè¿°
                    def replace_value(val):
                        if pd.isna(val):
                            return val
                        # å¤„ç†æµ®ç‚¹æ•°å’Œæ•´æ•°
                        if isinstance(val, (int, float)):
                            # å°è¯•å°†æµ®ç‚¹æ•°è½¬ä¸ºæ•´æ•°å­—ç¬¦ä¸²ï¼ˆå¦‚æœæ˜¯æ•´æ•°å€¼ï¼‰
                            if val == int(val):
                                val_str = str(int(val))
                            else:
                                val_str = str(val)
                        else:
                            val_str = str(val).strip()
                        return value_map.get(val_str, val)
                    
                    try:
                        original_values = df[current_col].dropna().unique()
                    except AttributeError:
                        # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œè·³è¿‡
                        continue
                        
                    df[current_col] = df[current_col].apply(replace_value)
                    enhanced_columns += 1
                    
                    # è®°å½•æ›¿æ¢ä¿¡æ¯
                    replaced_values = []
                    for val in original_values:
                        if not pd.isna(val):
                            val_str = str(val).strip()
                            if val_str in value_map:
                                replaced_values.append(f"{val_str}->{value_map[val_str]}")
                    
                    if replaced_values:
                        logger.info(f"åˆ— '{current_col}' æ›¿æ¢æ•°å€¼: {', '.join(replaced_values[:3])}" + 
                                  (f" ç­‰{len(replaced_values)}é¡¹" if len(replaced_values) > 3 else ""))
        
        # ä¿å­˜å¢å¼ºåçš„CSVæ–‡ä»¶
        df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"æˆåŠŸå¢å¼ºCSVæ–‡ä»¶: {csv_file_path.name} (é‡å‘½å{len(column_rename_map)}åˆ—, å¢å¼º{enhanced_columns}åˆ—æ•°å€¼)")
        return True
        
    except Exception as e:
        logger.error(f"å¢å¼ºCSVæ–‡ä»¶ {csv_file_path.name} æ—¶å‡ºé”™: {e}")
        return False

def convert_xpt_to_csv(xpt_file_path, csv_file_path, year_range, logger, enhance_labels=True):
    """
    å°†XPTæ–‡ä»¶è½¬æ¢ä¸ºCSVæ ¼å¼ï¼Œå¹¶æ ‡å‡†åŒ–åˆ—å
    
    å‚æ•°:
        xpt_file_path: XPTæ–‡ä»¶è·¯å¾„
        csv_file_path: è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„
        year_range: å¹´ä»½èŒƒå›´å­—ç¬¦ä¸²
        logger: æ—¥å¿—è®°å½•å™¨
        enhance_labels: æ˜¯å¦å¢å¼ºæ ‡ç­¾ï¼ˆæ›¿æ¢åˆ—åå’Œæ•°å€¼ï¼‰
    
    è¿”å›:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    try:
        # è¯»å–XPTæ–‡ä»¶
        df = pd.read_sas(xpt_file_path, format='xport')
        
        # æ ‡å‡†åŒ–åˆ—å - åªé‡å‘½åå­˜åœ¨çš„åˆ—
        columns_to_rename = {}
        for old_col, new_col in COLUMN_RENAME_MAP.items():
            if old_col in df.columns:
                columns_to_rename[old_col] = new_col
        
        if columns_to_rename:
            df = df.rename(columns=columns_to_rename)
            logger.info(f"æ ‡å‡†åŒ–åˆ—å: {list(columns_to_rename.keys())}")
        
        # æ·»åŠ å¹´ä»½èŒƒå›´åˆ—
        df['YearRange'] = year_range
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        csv_file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºCSVï¼Œä½¿ç”¨UTF-8ç¼–ç 
        df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')
        
        logger.info(f"è½¬æ¢æˆåŠŸ: {xpt_file_path.name} -> {csv_file_path}")
        
        # å¦‚æœå¯ç”¨æ ‡ç­¾å¢å¼ºï¼Œåˆ™åº”ç”¨å˜é‡æ ‡ç­¾å’Œæ•°å€¼æ ‡ç­¾
        if enhance_labels:
            # ä»æ–‡ä»¶åæå–è¡¨åï¼ˆå»æ‰åç¼€ï¼‰
            filename = xpt_file_path.stem.upper()
            # è§£æè¡¨åï¼ˆå»æ‰å¹´ä»½åç¼€ï¼‰
            prefix, suffix = parse_filename(filename + '.XPT')
            
            # æ„å»ºæ­£ç¡®çš„è¡¨å
            if suffix == 'P_':
                table_name = 'P_' + prefix
            else:
                table_name = prefix + suffix
            
            logger.info(f"å°è¯•å¢å¼ºæ–‡ä»¶ {csv_file_path.name}ï¼Œè¡¨å: {table_name}")
            
            # åº”ç”¨æ ‡ç­¾å¢å¼º
            enhance_csv_with_labels(csv_file_path, table_name, logger)
        
        return True
        
    except Exception as e:
        logger.error(f"è½¬æ¢æ–‡ä»¶ {xpt_file_path.name} æ—¶å‡ºé”™: {e}")
        return False

def merge_category_files(category_dir, logger):
    """
    åˆå¹¶ç±»åˆ«ç›®å½•ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶ä¸ºå•ä¸ªæ–‡ä»¶
    
    å‚æ•°:
        category_dir: ç±»åˆ«ç›®å½•è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
    """
    category_path = Path(category_dir)
    csv_files = list(category_path.glob("*.csv"))
    
    if not csv_files:
        return
    
    logger.info(f"å¤„ç†ç±»åˆ«: {category_path.name}")
    
    merged_dataframes = []
    
    for csv_file in csv_files:
        try:
            # è·³è¿‡å·²åˆå¹¶çš„æ–‡ä»¶
            if '_merged' in csv_file.name:
                continue
                
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            
            # ä»æ–‡ä»¶åæå–å¹´ä»½èŒƒå›´ï¼ˆå¦‚æœæ²¡æœ‰YearRangeåˆ—ï¼‰
            if 'YearRange' not in df.columns:
                # æ–‡ä»¶åæ ¼å¼: PREFIX_YEAR-RANGE.csv
                filename_parts = csv_file.stem.split('_')
                if len(filename_parts) >= 2:
                    year_range = filename_parts[-1]
                    df['YearRange'] = year_range
            
            merged_dataframes.append(df)
            
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶ {csv_file.name} æ—¶å‡ºé”™: {e}")
    
    if merged_dataframes:
        # åˆå¹¶æ‰€æœ‰DataFrame
        merged_df = pd.concat(merged_dataframes, ignore_index=True, sort=False)
        
        # ç”Ÿæˆåˆå¹¶æ–‡ä»¶å - ä½¿ç”¨ç›®å½•åï¼ˆå·²åŒ…å«å‰ç¼€ï¼‰
        merged_filename = f"{category_path.name}_merged.csv"
        merged_filepath = category_path / merged_filename
        
        # ä¿å­˜åˆå¹¶æ–‡ä»¶
        merged_df.to_csv(merged_filepath, index=False, encoding='utf-8-sig')
        
        logger.info(f"åˆå¹¶å®Œæˆ: {len(csv_files)} ä¸ªæ–‡ä»¶ -> {merged_filename}, æ€»è®°å½•æ•°: {len(merged_df)}")
    else:
        logger.warning(f"æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„CSVæ–‡ä»¶")

def process_xpt_to_csv_raw(source_root_dir, output_dir, logger):
    """
    å¤„ç†æ‰€æœ‰XPTæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºCSVæ ¼å¼ï¼ˆä¸å«æ ‡ç­¾å¢å¼ºï¼‰å¹¶ç»„ç»‡åˆ°æ–°çš„ç›®å½•ç»“æ„
    
    å‚æ•°:
        source_root_dir: æºæ ¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
    """
    source_path = Path(source_root_dir)
    output_path = Path(output_dir)
    
    logger.info("å¼€å§‹XPTåˆ°CSVè½¬æ¢ï¼ˆåŸå§‹æ ¼å¼ï¼Œä¸å«æ ‡ç­¾å¢å¼ºï¼‰...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'processed': 0,
        'success': 0,
        'errors': 0,
        'skipped': 0
    }
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰XPTæ–‡ä»¶
    xpt_files = list(source_path.rglob("*.xpt")) + list(source_path.rglob("*.XPT"))
    
    logger.info(f"æ‰¾åˆ° {len(xpt_files)} ä¸ªXPTæ–‡ä»¶")
    
    for xpt_file in tqdm(xpt_files, desc="è½¬æ¢XPTæ–‡ä»¶ï¼ˆåŸå§‹æ ¼å¼ï¼‰"):
        try:
            filename = xpt_file.name
            stats['processed'] += 1
            
            # è§£ææ–‡ä»¶å
            prefix, suffix = parse_filename(filename)
            year_range = YEAR_MAP.get(suffix, 'æœªçŸ¥å¹´ä»½')
            
            # æŸ¥æ‰¾ç±»åˆ«
            category_info = find_category(prefix)
            
            if not category_info:
                logger.warning(f"è·³è¿‡æœªè¯†åˆ«æ–‡ä»¶: {filename}")
                stats['skipped'] += 1
                continue
            
            main_category, sub_category = category_info
            
            # æ„å»ºCSVæ–‡ä»¶è·¯å¾„ - ä½¿ç”¨å‰ç¼€_å­ç±»åˆ«ä½œä¸ºç›®å½•å
            csv_filename = f"{prefix}_{year_range}.csv"
            csv_output_dir = output_path / main_category / f"{prefix}_{sub_category}"
            csv_file_path = csv_output_dir / csv_filename
            
            # è½¬æ¢æ–‡ä»¶ï¼ˆç¦ç”¨æ ‡ç­¾å¢å¼ºï¼‰
            if convert_xpt_to_csv(xpt_file, csv_file_path, year_range, logger, enhance_labels=False):
                stats['success'] += 1
            else:
                stats['errors'] += 1
                
        except Exception as e:
            stats['errors'] += 1
            logger.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info("="*80)
    logger.info("XPTè½¬CSVç»Ÿè®¡ï¼ˆåŸå§‹æ ¼å¼ï¼‰:")
    logger.info(f"å¤„ç†æ€»æ•°: {stats['processed']}")
    logger.info(f"è½¬æ¢æˆåŠŸ: {stats['success']}")
    logger.info(f"è½¬æ¢å¤±è´¥: {stats['errors']}")
    logger.info(f"è·³è¿‡æ–‡ä»¶: {stats['skipped']}")
    
    return stats

def process_xpt_to_csv(source_root_dir, output_dir, logger):
    """
    å¤„ç†æ‰€æœ‰XPTæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºCSVæ ¼å¼å¹¶ç»„ç»‡åˆ°æ–°çš„ç›®å½•ç»“æ„
    
    å‚æ•°:
        source_root_dir: æºæ ¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
    """
    source_path = Path(source_root_dir)
    output_path = Path(output_dir)
    
    logger.info("å¼€å§‹XPTåˆ°CSVè½¬æ¢...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'processed': 0,
        'success': 0,
        'errors': 0,
        'skipped': 0
    }
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰XPTæ–‡ä»¶
    xpt_files = list(source_path.rglob("*.xpt")) + list(source_path.rglob("*.XPT"))
    
    logger.info(f"æ‰¾åˆ° {len(xpt_files)} ä¸ªXPTæ–‡ä»¶")
    
    for xpt_file in tqdm(xpt_files, desc="è½¬æ¢XPTæ–‡ä»¶"):
        try:
            filename = xpt_file.name
            stats['processed'] += 1
            
            # è§£ææ–‡ä»¶å
            prefix, suffix = parse_filename(filename)
            year_range = YEAR_MAP.get(suffix, 'æœªçŸ¥å¹´ä»½')
            
            # æŸ¥æ‰¾ç±»åˆ«
            category_info = find_category(prefix)
            
            if not category_info:
                logger.warning(f"è·³è¿‡æœªè¯†åˆ«æ–‡ä»¶: {filename}")
                stats['skipped'] += 1
                continue
            
            main_category, sub_category = category_info
            
            # æ„å»ºCSVæ–‡ä»¶è·¯å¾„ - ä½¿ç”¨å‰ç¼€_å­ç±»åˆ«ä½œä¸ºç›®å½•å
            csv_filename = f"{prefix}_{year_range}.csv"
            csv_output_dir = output_path / main_category / f"{prefix}_{sub_category}"
            csv_file_path = csv_output_dir / csv_filename
            
            # è½¬æ¢æ–‡ä»¶ï¼ˆé»˜è®¤å¯ç”¨æ ‡ç­¾å¢å¼ºï¼‰
            if convert_xpt_to_csv(xpt_file, csv_file_path, year_range, logger, enhance_labels=True):
                stats['success'] += 1
            else:
                stats['errors'] += 1
                
        except Exception as e:
            stats['errors'] += 1
            logger.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info("="*80)
    logger.info("XPTè½¬CSVç»Ÿè®¡:")
    logger.info(f"å¤„ç†æ€»æ•°: {stats['processed']}")
    logger.info(f"è½¬æ¢æˆåŠŸ: {stats['success']}")
    logger.info(f"è½¬æ¢å¤±è´¥: {stats['errors']}")
    logger.info(f"è·³è¿‡æ–‡ä»¶: {stats['skipped']}")
    
    return stats

def merge_all_categories(csv_root_dir, logger):
    """
    åˆå¹¶æ‰€æœ‰ç±»åˆ«çš„å¹´åº¦æ•°æ®
    
    å‚æ•°:
        csv_root_dir: CSVæ ¹ç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
    """
    csv_path = Path(csv_root_dir)
    
    if not csv_path.exists():
        logger.error(f"CSVç›®å½•ä¸å­˜åœ¨: {csv_root_dir}")
        return
    
    logger.info("å¼€å§‹åˆå¹¶å¹´åº¦æ•°æ®...")
    
    # æŸ¥æ‰¾æ‰€æœ‰æœ€åº•å±‚çš„å­ç›®å½•ï¼ˆåŒ…å«CSVæ–‡ä»¶çš„ç›®å½•ï¼‰
    category_dirs = []
    for root, dirs, files in os.walk(csv_path):
        # å¦‚æœç›®å½•åŒ…å«CSVæ–‡ä»¶ä¸”æ²¡æœ‰å­ç›®å½•ï¼Œåˆ™è®¤ä¸ºæ˜¯ç±»åˆ«ç›®å½•
        csv_files = [f for f in files if f.endswith('.csv') and not f.endswith('_merged.csv')]
        if csv_files and not dirs:
            category_dirs.append(Path(root))
    
    logger.info(f"æ‰¾åˆ° {len(category_dirs)} ä¸ªç±»åˆ«ç›®å½•")
    
    for category_dir in tqdm(category_dirs, desc="åˆå¹¶ç±»åˆ«æ•°æ®"):
        merge_category_files(category_dir, logger)
    
    # åˆ›å»ºæ€»åˆå¹¶ç›®å½•
    create_master_merged_directory(csv_root_dir, logger)
    
    logger.info("æ‰€æœ‰ç±»åˆ«æ•°æ®åˆå¹¶å®Œæˆï¼")

def create_master_merged_directory(csv_root_dir, logger):
    """
    åˆ›å»ºæ€»çš„åˆå¹¶æ–‡ä»¶ç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰æŒ‰å‰ç¼€åˆ†ç»„çš„åˆå¹¶æ–‡ä»¶
    
    å‚æ•°:
        csv_root_dir: CSVæ ¹ç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
    """
    csv_path = Path(csv_root_dir)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä¿®æ”¹å‘½åä»¥åŒºåˆ†æ–°æ—§æ ¼å¼ï¼‰
    output_dir_name = f"{csv_path.name}_merged_by_prefix"
    output_dir = csv_path.parent / output_dir_name
    output_dir.mkdir(exist_ok=True)
    
    logger.info(f"åˆ›å»ºæ€»åˆå¹¶ç›®å½•: {output_dir_name}")
    
    # æŸ¥æ‰¾æ‰€æœ‰åˆå¹¶æ–‡ä»¶
    merged_files = list(csv_path.rglob("*_merged.csv"))
    
    copied_count = 0
    for merged_file in merged_files:
        try:
            # å¤åˆ¶æ–‡ä»¶åˆ°æ€»ç›®å½•
            dest_file = output_dir / merged_file.name
            shutil.copy2(merged_file, dest_file)
            copied_count += 1
            
        except Exception as e:
            logger.error(f"å¤åˆ¶åˆå¹¶æ–‡ä»¶å¤±è´¥ {merged_file.name}: {e}")
    
    logger.info(f"æ€»åˆå¹¶ç›®å½•åˆ›å»ºå®Œæˆï¼Œå¤åˆ¶äº† {copied_count} ä¸ªåˆå¹¶æ–‡ä»¶åˆ° {output_dir_name}")
    
    return output_dir

def enhance_existing_csvs(csv_root_dir, logger):
    """
    å¯¹ç°æœ‰çš„CSVæ–‡ä»¶åº”ç”¨æ ‡ç­¾å¢å¼º
    
    å‚æ•°:
        csv_root_dir: CSVæ ¹ç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
    """
    csv_path = Path(csv_root_dir)
    
    if not csv_path.exists():
        logger.error(f"CSVç›®å½•ä¸å­˜åœ¨: {csv_root_dir}")
        return
    
    logger.info("å¼€å§‹å¢å¼ºç°æœ‰CSVæ–‡ä»¶çš„æ ‡ç­¾...")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'processed': 0,
        'success': 0,
        'errors': 0,
        'skipped': 0
    }
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = list(csv_path.rglob("*.csv"))
    
    # è¿‡æ»¤æ‰åˆå¹¶æ–‡ä»¶
    csv_files = [f for f in csv_files if '_merged' not in f.name]
    
    logger.info(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    
    for csv_file in tqdm(csv_files, desc="å¢å¼ºCSVæ–‡ä»¶"):
        try:
            stats['processed'] += 1
            
            # ä»æ–‡ä»¶åæ¨æ–­è¡¨å
            filename = csv_file.stem  # ä¾‹å¦‚: DEMO_2017-2018
            
            # ç§»é™¤å¹´ä»½åç¼€ï¼Œè·å–è¡¨å
            parts = filename.split('_')
            if len(parts) >= 2:
                table_name = '_'.join(parts[:-1])  # å»æ‰æœ€åçš„å¹´ä»½éƒ¨åˆ†
            else:
                table_name = parts[0]
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„å˜é‡æè¿°æ–‡ä»¶å¤¹
            table_dir = Path(VARIABLES_DIR) / table_name
            if not table_dir.exists():
                logger.warning(f"è·³è¿‡æ–‡ä»¶ï¼ˆæœªæ‰¾åˆ°å˜é‡æè¿°ï¼‰: {csv_file.name}")
                stats['skipped'] += 1
                continue
            
            # åº”ç”¨æ ‡ç­¾å¢å¼º
            if enhance_csv_with_labels(csv_file, table_name, logger):
                stats['success'] += 1
            else:
                stats['errors'] += 1
                
        except Exception as e:
            stats['errors'] += 1
            logger.error(f"å¤„ç†CSVæ–‡ä»¶ {csv_file.name} æ—¶å‡ºé”™: {e}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info("="*80)
    logger.info("CSVæ ‡ç­¾å¢å¼ºç»Ÿè®¡:")
    logger.info(f"å¤„ç†æ€»æ•°: {stats['processed']}")
    logger.info(f"å¢å¼ºæˆåŠŸ: {stats['success']}")
    logger.info(f"å¢å¼ºå¤±è´¥: {stats['errors']}")
    logger.info(f"è·³è¿‡æ–‡ä»¶: {stats['skipped']}")
    
    return stats

def auto_process_nhanes_data():
    """
    è‡ªåŠ¨å¤„ç†NHANESæ•°æ®çš„ä¸»å‡½æ•°
    """
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    
    logger.info("="*60)
    logger.info("NHANESæ•°æ®è‡ªåŠ¨å¤„ç†å¼€å§‹")
    logger.info(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("="*60)
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(DEST_DIR):
        logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DEST_DIR}")
        return False
    
    # æ£€æŸ¥å˜é‡æè¿°ç›®å½•
    if not os.path.exists(VARIABLES_DIR):
        logger.warning(f"å˜é‡æè¿°ç›®å½•ä¸å­˜åœ¨: {VARIABLES_DIR}")
        logger.warning("å°†è·³è¿‡æ ‡ç­¾å¢å¼ºåŠŸèƒ½")
    
    try:
        # æ­¥éª¤1: è½¬æ¢XPTåˆ°CSV
        logger.info("æ­¥éª¤1: å¼€å§‹XPTåˆ°CSVè½¬æ¢...")
        stats = process_xpt_to_csv(DEST_DIR, CSV_OUTPUT_DIR, logger)
        
        if stats['success'] == 0:
            logger.warning("æ²¡æœ‰æˆåŠŸè½¬æ¢ä»»ä½•æ–‡ä»¶ï¼Œè·³è¿‡åç»­æ­¥éª¤")
            return False
        
        # æ­¥éª¤2: åˆå¹¶å¹´åº¦æ•°æ®
        logger.info("æ­¥éª¤2: å¼€å§‹åˆå¹¶å¹´åº¦æ•°æ®...")
        merge_all_categories(CSV_OUTPUT_DIR, logger)
        
        logger.info("="*60)
        logger.info("NHANESæ•°æ®å¤„ç†å®Œæˆï¼")
        logger.info(f"è¾“å‡ºç›®å½•: {CSV_OUTPUT_DIR}")
        logger.info("="*60)
        
        return True
        
    except Exception as e:
        logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False

def organize_files(source_dir, dest_dir, dry_run=False):
    """
    æ•´ç†NHANESæ–‡ä»¶
    
    å‚æ•°:
        source_dir: æºç›®å½•è·¯å¾„
        dest_dir: ç›®æ ‡ç›®å½•è·¯å¾„
        dry_run: æ˜¯å¦åªæ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…ç§»åŠ¨æ–‡ä»¶
    """
    source_path = Path(source_dir)
    dest_path = Path(dest_dir)
    
    if not source_path.exists():
        print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return
    
    # è·å–æ‰€æœ‰.xptæ–‡ä»¶
    xpt_files = list(source_path.glob("*.xpt")) + list(source_path.glob("*.XPT"))
    
    if not xpt_files:
        print(f"âŒ åœ¨æºç›®å½•ä¸­æœªæ‰¾åˆ°.xptæ–‡ä»¶: {source_dir}")
        return
    
    print(f"ğŸ“ æºç›®å½•: {source_dir}")
    print(f"ğŸ“ ç›®æ ‡ç›®å½•: {dest_dir}")
    print(f"ğŸ“Š æ‰¾åˆ° {len(xpt_files)} ä¸ª.xptæ–‡ä»¶")
    
    if dry_run:
        print("ğŸ” æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…ç§»åŠ¨æ–‡ä»¶")
    
    print("\n" + "="*80)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'moved': 0,
        'uncategorized': 0,
        'errors': 0
    }
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for file_path in tqdm(xpt_files, desc="å¤„ç†æ–‡ä»¶"):
        try:
            filename = file_path.name
            
            # è§£ææ–‡ä»¶å
            prefix, suffix = parse_filename(filename)
            
            # æŸ¥æ‰¾å¹´ä»½
            year = YEAR_MAP.get(suffix, 'æœªçŸ¥å¹´ä»½')
            
            # æŸ¥æ‰¾ç±»åˆ«
            category_info = find_category(prefix)
            
            if category_info:
                main_category, sub_category = category_info
                
                # æ„å»ºç›®æ ‡è·¯å¾„
                target_dir = dest_path / main_category / sub_category / year
                target_file = target_dir / filename
                
                # åˆ›å»ºç›®å½•
                if not dry_run:
                    target_dir.mkdir(parents=True, exist_ok=True)
                
                # ç§»åŠ¨æ–‡ä»¶
                if not dry_run:
                    if target_file.exists():
                        print(f"âš ï¸  ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                        continue
                    shutil.move(str(file_path), str(target_file))
                
                stats['moved'] += 1
                print(f"âœ… å·²ç§»åŠ¨: {filename} -> {main_category}/{sub_category}/{year}/")
                
            else:
                # æ— æ³•è¯†åˆ«çš„æ–‡ä»¶æ”¾åˆ°_Uncategorizedç›®å½•
                uncategorized_dir = dest_path / "_Uncategorized"
                target_file = uncategorized_dir / filename
                
                if not dry_run:
                    uncategorized_dir.mkdir(parents=True, exist_ok=True)
                    if not target_file.exists():
                        shutil.move(str(file_path), str(target_file))
                
                stats['uncategorized'] += 1
                print(f"âš ï¸  æœªè¯†åˆ«æ–‡ä»¶: {filename} -> _Uncategorized/")
                
        except Exception as e:
            stats['errors'] += 1
            print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*80)
    print("ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"âœ… æˆåŠŸç§»åŠ¨: {stats['moved']} ä¸ªæ–‡ä»¶")
    print(f"âš ï¸  æœªè¯†åˆ«æ–‡ä»¶: {stats['uncategorized']} ä¸ªæ–‡ä»¶")
    print(f"âŒ é”™è¯¯: {stats['errors']} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ æ€»è®¡: {len(xpt_files)} ä¸ªæ–‡ä»¶")
    
    if not dry_run:
        print(f"\nğŸ‰ æ–‡ä»¶æ•´ç†å®Œæˆï¼è¯·æŸ¥çœ‹ç›®æ ‡ç›®å½•: {dest_dir}")
    else:
        print(f"\nğŸ” æ¨¡æ‹Ÿè¿è¡Œå®Œæˆï¼å¦‚éœ€å®é™…ç§»åŠ¨æ–‡ä»¶ï¼Œè¯·è®¾ç½® dry_run=False")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ NHANES æ•°æ®å¤„ç†å’Œé¢„å¤„ç†å·¥å…·")
    print("="*60)
    
    print("åŠŸèƒ½é€‰é¡¹:")
    print("1. æ•´ç†XPTæ–‡ä»¶åˆ°åˆ†ç±»ç›®å½•")
    print("2. å°†XPTæ–‡ä»¶è½¬æ¢ä¸ºCSVæ ¼å¼ï¼ˆå«æ ‡ç­¾å¢å¼ºï¼ŒæŒ‰å‰ç¼€åˆ†æ–‡ä»¶å¤¹ï¼‰") 
    print("3. å°†XPTæ–‡ä»¶è½¬æ¢ä¸ºCSVæ ¼å¼ï¼ˆä¸å«æ ‡ç­¾å¢å¼ºï¼ŒæŒ‰å‰ç¼€åˆ†æ–‡ä»¶å¤¹ï¼‰")
    print("4. åˆå¹¶ç±»åˆ«å¹´åº¦æ•°æ®ï¼ˆæŒ‰å‰ç¼€åˆ†åˆ«åˆå¹¶ï¼‰")
    print("5. å®Œæ•´å¤„ç†æµç¨‹ (XPT -> CSV -> åˆå¹¶)")
    print("6. ä»…å¤„ç†ç°æœ‰å·²ç»„ç»‡çš„æ•°æ® (XPT -> CSV -> åˆå¹¶)")
    print("7. å¢å¼ºç°æœ‰CSVæ–‡ä»¶çš„æ ‡ç­¾å’Œæ•°å€¼æè¿°")
    print("")
    print("æ³¨æ„ï¼šæ–°ç‰ˆæœ¬ä¼šå°†æ¯ä¸ªå˜é‡å‰ç¼€æ”¾åœ¨ç‹¬ç«‹çš„æ–‡ä»¶å¤¹ä¸­")
    print("     ä¾‹å¦‚ï¼šDR1IFF_2_è†³é£Ÿæ•°æ®_å•ä¸ªé£Ÿç‰©/ï¼ŒDR2IFF_2_è†³é£Ÿæ•°æ®_å•ä¸ªé£Ÿç‰©/")
    
    choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (1-7): ").strip()
    
    if choice == "1":
        # åŸæœ‰çš„æ–‡ä»¶æ•´ç†åŠŸèƒ½
        if not os.path.exists(SOURCE_DIR):
            print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
            print("è¯·æ£€æŸ¥é…ç½®åŒºåŸŸä¸­çš„ SOURCE_DIR è®¾ç½®")
            return
        
        print(f"æºç›®å½•: {SOURCE_DIR}")
        print(f"ç›®æ ‡ç›®å½•: {DEST_DIR}")
        
        response = input("\næ˜¯å¦å…ˆè¿›è¡Œæ¨¡æ‹Ÿè¿è¡ŒæŸ¥çœ‹æ•ˆæœï¼Ÿ(y/n): ").lower().strip()
        
        if response == 'y':
            print("\nğŸ” å¼€å§‹æ¨¡æ‹Ÿè¿è¡Œ...")
            organize_files(SOURCE_DIR, DEST_DIR, dry_run=True)
            
            response2 = input("\næ¨¡æ‹Ÿè¿è¡Œå®Œæˆã€‚æ˜¯å¦ç»§ç»­å®é™…ç§»åŠ¨æ–‡ä»¶ï¼Ÿ(y/n): ").lower().strip()
            if response2 == 'y':
                print("\nğŸ“¦ å¼€å§‹å®é™…ç§»åŠ¨æ–‡ä»¶...")
                organize_files(SOURCE_DIR, DEST_DIR, dry_run=False)
            else:
                print("æ“ä½œå–æ¶ˆã€‚")
        else:
            print("\nğŸ“¦ å¼€å§‹ç§»åŠ¨æ–‡ä»¶...")
            organize_files(SOURCE_DIR, DEST_DIR, dry_run=False)
    
    elif choice == "2":
        # XPTåˆ°CSVè½¬æ¢ï¼ˆå«æ ‡ç­¾å¢å¼ºï¼‰
        source_input = input(f"\nè¯·è¾“å…¥XPTæ–‡ä»¶æºç›®å½• (é»˜è®¤: {DEST_DIR}): ").strip()
        if not source_input:
            source_input = DEST_DIR
            
        if not os.path.exists(source_input):
            print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_input}")
            return
            
        print(f"æºç›®å½•: {source_input}")
        print(f"CSVè¾“å‡ºç›®å½•: {CSV_OUTPUT_DIR}")
        
        confirm = input("\nç¡®è®¤å¼€å§‹è½¬æ¢ï¼Ÿ(y/n): ").lower().strip()
        if confirm == 'y':
            logger = setup_logging()
            process_xpt_to_csv(source_input, CSV_OUTPUT_DIR, logger)
        else:
            print("æ“ä½œå–æ¶ˆã€‚")
    
    elif choice == "3":
        # XPTåˆ°CSVè½¬æ¢ï¼ˆä¸å«æ ‡ç­¾å¢å¼ºï¼ŒåŸå§‹æ ¼å¼ï¼‰
        source_input = input(f"\nè¯·è¾“å…¥XPTæ–‡ä»¶æºç›®å½• (é»˜è®¤: {DEST_DIR}): ").strip()
        if not source_input:
            source_input = DEST_DIR
            
        if not os.path.exists(source_input):
            print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {source_input}")
            return
            
        print(f"æºç›®å½•: {source_input}")
        print(f"CSVè¾“å‡ºç›®å½•: {CSV_OUTPUT_DIR_RAW}")
        print("æ³¨æ„ï¼šæ­¤åŠŸèƒ½ä¸ä¼šè¿›è¡Œæ ‡ç­¾å¢å¼ºï¼Œä¿æŒåŸå§‹å˜é‡åå’Œæ•°å€¼ç¼–ç ")
        
        confirm = input("\nç¡®è®¤å¼€å§‹è½¬æ¢ï¼Ÿ(y/n): ").lower().strip()
        if confirm == 'y':
            logger = setup_logging()
            # è°ƒç”¨process_xpt_to_csv_rawå‡½æ•°ï¼Œä¸å¯ç”¨æ ‡ç­¾å¢å¼º
            process_xpt_to_csv_raw(source_input, CSV_OUTPUT_DIR_RAW, logger)
        else:
            print("æ“ä½œå–æ¶ˆã€‚")
    
    elif choice == "4":
        # åˆå¹¶ç±»åˆ«æ•°æ®
        csv_input = input(f"\nè¯·è¾“å…¥CSVç›®å½• (é»˜è®¤: {CSV_OUTPUT_DIR}): ").strip()
        if not csv_input:
            csv_input = CSV_OUTPUT_DIR
            
        if not os.path.exists(csv_input):
            print(f"âŒ CSVç›®å½•ä¸å­˜åœ¨: {csv_input}")
            return
            
        print(f"CSVç›®å½•: {csv_input}")
        
        confirm = input("\nç¡®è®¤å¼€å§‹åˆå¹¶ï¼Ÿ(y/n): ").lower().strip()
        if confirm == 'y':
            logger = setup_logging()
            merge_all_categories(csv_input, logger)
        else:
            print("æ“ä½œå–æ¶ˆã€‚")
    
    elif choice == "5":
        # å®Œæ•´å¤„ç†æµç¨‹
        if not os.path.exists(SOURCE_DIR):
            print(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
            print("è¯·æ£€æŸ¥é…ç½®åŒºåŸŸä¸­çš„ SOURCE_DIR è®¾ç½®")
            return
            
        print("å®Œæ•´å¤„ç†æµç¨‹:")
        print(f"1. æ•´ç†æ–‡ä»¶: {SOURCE_DIR} -> {DEST_DIR}")
        print(f"2. è½¬æ¢æ ¼å¼: {DEST_DIR} -> {CSV_OUTPUT_DIR}")
        print(f"3. åˆå¹¶æ•°æ®: {CSV_OUTPUT_DIR}")
        
        confirm = input("\nç¡®è®¤å¼€å§‹å®Œæ•´å¤„ç†ï¼Ÿ(y/n): ").lower().strip()
        if confirm == 'y':
            logger = setup_logging()
            
            print("\næ­¥éª¤1: æ•´ç†XPTæ–‡ä»¶...")
            organize_files(SOURCE_DIR, DEST_DIR, dry_run=False)
            
            print("\næ­¥éª¤2: è½¬æ¢ä¸ºCSVæ ¼å¼...")
            process_xpt_to_csv(DEST_DIR, CSV_OUTPUT_DIR, logger)
            
            print("\næ­¥éª¤3: åˆå¹¶å¹´åº¦æ•°æ®...")
            merge_all_categories(CSV_OUTPUT_DIR, logger)
            
            print("\nğŸ‰ å®Œæ•´å¤„ç†æµç¨‹å®Œæˆï¼")
        else:
            print("æ“ä½œå–æ¶ˆã€‚")
    
    elif choice == "6":
        # å¤„ç†ç°æœ‰å·²ç»„ç»‡çš„æ•°æ®
        source_input = input(f"\nè¯·è¾“å…¥å·²ç»„ç»‡çš„NHANESæ•°æ®ç›®å½• (é»˜è®¤: {DEST_DIR}): ").strip()
        if not source_input:
            source_input = DEST_DIR
            
        if not os.path.exists(source_input):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {source_input}")
            return
            
        print("å¤„ç†ç°æœ‰æ•°æ®:")
        print(f"1. è½¬æ¢æ ¼å¼: {source_input} -> {CSV_OUTPUT_DIR}")
        print(f"2. åˆå¹¶æ•°æ®: {CSV_OUTPUT_DIR}")
        
        confirm = input("\nç¡®è®¤å¼€å§‹å¤„ç†ï¼Ÿ(y/n): ").lower().strip()
        if confirm == 'y':
            logger = setup_logging()
            
            print("\næ­¥éª¤1: è½¬æ¢ä¸ºCSVæ ¼å¼...")
            process_xpt_to_csv(source_input, CSV_OUTPUT_DIR, logger)
            
            print("\næ­¥éª¤2: åˆå¹¶å¹´åº¦æ•°æ®...")
            merge_all_categories(CSV_OUTPUT_DIR, logger)
            
            print("\nğŸ‰ å¤„ç†å®Œæˆï¼")
        else:
            print("æ“ä½œå–æ¶ˆã€‚")
    
    elif choice == "7":
        # å¢å¼ºç°æœ‰CSVæ–‡ä»¶æ ‡ç­¾
        csv_input = input(f"\nè¯·è¾“å…¥CSVç›®å½• (é»˜è®¤: {CSV_OUTPUT_DIR}): ").strip()
        if not csv_input:
            csv_input = CSV_OUTPUT_DIR
            
        if not os.path.exists(csv_input):
            print(f"âŒ CSVç›®å½•ä¸å­˜åœ¨: {csv_input}")
            return
            
        # æ£€æŸ¥å˜é‡æè¿°ç›®å½•
        if not os.path.exists(VARIABLES_DIR):
            print(f"âŒ å˜é‡æè¿°ç›®å½•ä¸å­˜åœ¨: {VARIABLES_DIR}")
            print("è¯·ç¡®ä¿NHANES_Variablesæ–‡ä»¶å¤¹å­˜åœ¨ä¸”åŒ…å«å˜é‡æè¿°ä¿¡æ¯")
            return
            
        print(f"CSVç›®å½•: {csv_input}")
        print(f"å˜é‡æè¿°ç›®å½•: {VARIABLES_DIR}")
        print("\næ­¤åŠŸèƒ½å°†ï¼š")
        print("- å°†å˜é‡åæ›¿æ¢ä¸ºæ›´æ˜“è¯»çš„æ ‡ç­¾")
        print("- å°†æ•°å€¼ç¼–ç æ›¿æ¢ä¸ºå¯¹åº”çš„æ–‡æœ¬æè¿°")
        
        confirm = input("\nç¡®è®¤å¼€å§‹å¢å¼ºï¼Ÿ(y/n): ").lower().strip()
        if confirm == 'y':
            logger = setup_logging()
            enhance_existing_csvs(csv_input, logger)
            print("\nğŸ‰ æ ‡ç­¾å¢å¼ºå®Œæˆï¼")
        else:
            print("æ“ä½œå–æ¶ˆã€‚")
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¿è¡Œç¨‹åºã€‚")

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        # äº¤äº’å¼æ¨¡å¼
        main()
    else:
        # é»˜è®¤è‡ªåŠ¨å¤„ç†æ¨¡å¼
        print("ğŸ¥ NHANESæ•°æ®è‡ªåŠ¨å¤„ç†æ¨¡å¼")
        print("å¦‚éœ€äº¤äº’å¼é€‰æ‹©ï¼Œè¯·ä½¿ç”¨: python organize_nhanes_files.py --interactive")
        print("="*60)
        
        success = auto_process_nhanes_data()
        
        if success:
            print("\nâœ… å¤„ç†å®Œæˆï¼è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯ã€‚")
        else:
            print("\nâŒ å¤„ç†å¤±è´¥ï¼è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–é”™è¯¯ä¿¡æ¯ã€‚")
            sys.exit(1) 