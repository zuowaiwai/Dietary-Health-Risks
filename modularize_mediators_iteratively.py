#!/usr/bin/env python3
"""
modularize_mediators_iteratively.py

å†…å­˜é«˜æ•ˆçš„ä¸­ä»‹å˜é‡æ¨¡å—åŒ–åˆ†æè„šæœ¬
é‡‡ç”¨å­—å…¸é©±åŠ¨çš„è¿­ä»£å¼å’Œå¹¶è¡ŒåŒ–æ–¹æ³•å¤„ç†3000+ä¸ªé«˜ç»´ã€é«˜ç¼ºå¤±ç‡çš„ä¸­ä»‹å˜é‡

ä½œè€…: AIåŠ©æ‰‹
æ—¥æœŸ: 2024
ç‰ˆæœ¬: 2.0
"""

import pandas as pd
import numpy as np
import os
import sys
import argparse
import logging
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import gc
import pickle
from datetime import datetime
import psutil

# ç§‘å­¦è®¡ç®—å’Œæœºå™¨å­¦ä¹ 
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
from matplotlib import font_manager

# å¹¶è¡Œå¤„ç†
from joblib import Parallel, delayed
import multiprocessing

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')

# ä¿®å¤ä¸­æ–‡å­—ä½“æ˜¾ç¤ºé—®é¢˜
def setup_chinese_fonts():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    try:
        # å°è¯•ä¸åŒçš„ä¸­æ–‡å­—ä½“
        chinese_fonts = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans', 'WenQuanYi Micro Hei', 'Arial Unicode MS']
        
        for font_name in chinese_fonts:
            try:
                # æ£€æŸ¥å­—ä½“æ˜¯å¦å¯ç”¨
                font_path = font_manager.findfont(font_name, fallback_to_default=False)
                if font_path and 'default' not in font_path.lower():
                    plt.rcParams['font.sans-serif'] = [font_name] + plt.rcParams['font.sans-serif']
                    print(f"âœ… æˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {font_name}")
                    break
            except:
                continue
        else:
            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤å­—ä½“å¹¶å…³é—­ä¸­æ–‡æ˜¾ç¤º
            print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ˜¾ç¤º")
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        
        # è®¾ç½®è´Ÿå·æ­£å¸¸æ˜¾ç¤º
        plt.rcParams['axes.unicode_minus'] = False
        
        # è®¾ç½®å­—ä½“å¤§å°
        plt.rcParams['font.size'] = 10
        plt.rcParams['axes.titlesize'] = 12
        plt.rcParams['figure.titlesize'] = 14
        
    except Exception as e:
        print(f"âš ï¸ å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        # ä½¿ç”¨é»˜è®¤å­—ä½“
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

# åˆå§‹åŒ–å­—ä½“è®¾ç½®
setup_chinese_fonts()

class MemoryEfficientMediatorAnalyzer:
    """å†…å­˜é«˜æ•ˆçš„ä¸­ä»‹å˜é‡åˆ†æå™¨"""
    
    def __init__(self, 
                 variable_dict_path: str = "all_nhanes_variables.csv",
                 data_dir: str = "NHANES_PROCESSED_CSV_merged_by_prefix",
                 output_dir: str = "mediator_analysis_results_v2",
                 id_column: str = "RespondentSequenceNumber",
                 max_memory_gb: float = 100.0,  # è®¾ç½®ä¸º100GB
                 n_jobs: int = 128):  # è®¾ç½®ä¸º128ï¼ŒåŒ¹é…CPUæ ¸å¿ƒæ•°
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            variable_dict_path: å˜é‡å­—å…¸æ–‡ä»¶è·¯å¾„
            data_dir: æ•°æ®æ–‡ä»¶ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            id_column: å‚ä¸è€…IDåˆ—å
            max_memory_gb: æœ€å¤§å†…å­˜ä½¿ç”¨é™åˆ¶(GB)
            n_jobs: å¹¶è¡Œä½œä¸šæ•°
        """
        self.variable_dict_path = variable_dict_path
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.id_column = id_column
        self.max_memory_gb = max_memory_gb
        self.n_jobs = n_jobs if n_jobs != -1 else multiprocessing.cpu_count()
        
        # è®¾ç½®æ—¥å¿—ï¼ˆéœ€è¦å…ˆåˆ›å»ºåŸºæœ¬ç›®å½•ï¼‰
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.setup_logging()
        
        # åˆ›å»ºå®Œæ•´è¾“å‡ºç›®å½•ç»“æ„
        self.create_output_directories()
        
        # åˆå§‹åŒ–çŠ¶æ€å˜é‡
        self.variable_dict = None
        self.label_to_filepath_map = {}
        self.core_mediator_labels = []
        self.final_mediator_labels = []
        self.mediator_metadata = {}
        
        # è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        total_memory = psutil.virtual_memory().total / (1024**3)  # GB
        self.logger.info(f"ç³»ç»Ÿæ€»å†…å­˜: {total_memory:.1f}GB")
        self.logger.info(f"è®¾ç½®æœ€å¤§å†…å­˜é™åˆ¶: {max_memory_gb}GB ({(max_memory_gb/total_memory*100):.1f}% of total)")
        self.logger.info(f"é¢„ç•™ç³»ç»Ÿå†…å­˜: {(total_memory - max_memory_gb):.1f}GB")
        self.logger.info(f"è®¾ç½®å¹¶è¡Œä½œä¸šæ•°: {self.n_jobs} (CPUæ ¸å¿ƒæ€»æ•°: {multiprocessing.cpu_count()})")
        
        # æ£€æŸ¥å†…å­˜è®¾ç½®æ˜¯å¦åˆç†
        if max_memory_gb > total_memory * 0.95:
            self.logger.warning(f"âš ï¸ å†…å­˜é™åˆ¶ ({max_memory_gb}GB) æ¥è¿‘ç³»ç»Ÿæ€»å†…å­˜ ({total_memory:.1f}GB)ï¼Œå¯èƒ½å½±å“ç³»ç»Ÿç¨³å®šæ€§")
        elif max_memory_gb < total_memory * 0.5:
            self.logger.warning(f"âš ï¸ å†…å­˜é™åˆ¶ ({max_memory_gb}GB) å¯èƒ½è¿‡ä½ï¼Œå»ºè®®æé«˜åˆ°ç³»ç»Ÿæ€»å†…å­˜çš„70-85%ä¹‹é—´")
        
        # æ£€æŸ¥å¹¶è¡Œä½œä¸šæ•°è®¾ç½®æ˜¯å¦åˆç†
        if self.n_jobs > multiprocessing.cpu_count():
            self.logger.warning(f"âš ï¸ å¹¶è¡Œä½œä¸šæ•° ({self.n_jobs}) è¶…è¿‡CPUæ ¸å¿ƒæ•° ({multiprocessing.cpu_count()})ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        
        # æ£€æŸ¥ç³»ç»Ÿå†…å­˜å‹åŠ›
        memory_pressure = psutil.virtual_memory().percent
        if memory_pressure > 80:
            self.logger.warning(f"âš ï¸ ç³»ç»Ÿå†…å­˜å‹åŠ›è¾ƒå¤§ (ä½¿ç”¨ç‡: {memory_pressure}%)ï¼Œå»ºè®®å…ˆæ¸…ç†ä¸€äº›å†…å­˜å†è¿è¡Œ")
    
    def create_output_directories(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        # ä¸»è¾“å‡ºç›®å½•
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # å­ç›®å½•
        self.correlation_dir = self.output_dir / "correlation_analysis"
        self.clustering_dir = self.output_dir / "clustering_results"
        self.visualization_dir = self.output_dir / "visualizations"
        self.module_dir = self.output_dir / "module_representatives"
        self.metadata_dir = self.output_dir / "metadata"
        
        # åˆ›å»ºæ‰€æœ‰å­ç›®å½•
        for dir_path in [self.correlation_dir, self.clustering_dir, self.visualization_dir, 
                        self.module_dir, self.metadata_dir]:
            dir_path.mkdir(exist_ok=True, parents=True)
        
        if hasattr(self, 'logger'):
            self.logger.info(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„: {self.output_dir}")
        else:
            print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„: {self.output_dir}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = self.output_dir / f"mediator_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def check_memory_usage(self, operation: str = "") -> float:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = psutil.virtual_memory()
        memory_gb = memory_info.used / (1024**3)
        memory_percent = memory_info.percent
        
        # æ›´è¯¦ç»†çš„å†…å­˜ä½¿ç”¨æŠ¥å‘Š
        if memory_gb > self.max_memory_gb:
            self.logger.warning(
                f"å†…å­˜ä½¿ç”¨è¶…é™! {operation}\n"
                f"å½“å‰ä½¿ç”¨: {memory_gb:.2f}GB ({memory_percent:.1f}%)\n"
                f"æœ€å¤§é™åˆ¶: {self.max_memory_gb}GB\n"
                f"å¯ç”¨å†…å­˜: {(memory_info.total/1024**3 - memory_gb):.2f}GB\n"
                f"å†…å­˜å‹åŠ›: {'é«˜' if memory_percent > 90 else 'ä¸­ç­‰' if memory_percent > 70 else 'æ­£å¸¸'}"
            )
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        else:
            self.logger.debug(
                f"å†…å­˜ä½¿ç”¨æƒ…å†µ {operation}:\n"
                f"å½“å‰ä½¿ç”¨: {memory_gb:.2f}GB ({memory_percent:.1f}%)\n"
                f"æœ€å¤§é™åˆ¶: {self.max_memory_gb}GB\n"
                f"å†…å­˜ä½™é‡: {(self.max_memory_gb - memory_gb):.2f}GB\n"
                f"å†…å­˜å‹åŠ›: {'é«˜' if memory_percent > 90 else 'ä¸­ç­‰' if memory_percent > 70 else 'æ­£å¸¸'}"
            )
        
        return memory_gb
    
    def load_variable_dictionary(self) -> bool:
        """åŠ è½½å˜é‡å­—å…¸"""
        try:
            self.logger.info(f"ğŸ“š åŠ è½½å˜é‡å­—å…¸: {self.variable_dict_path}")
            self.variable_dict = pd.read_csv(self.variable_dict_path)
            
            required_cols = ['global_variable_id', 'file_name', 'variable_name', 'category']
            missing_cols = [col for col in required_cols if col not in self.variable_dict.columns]
            
            if missing_cols:
                self.logger.error(f"å˜é‡å­—å…¸ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
                return False
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½å˜é‡å­—å…¸ï¼Œå…± {len(self.variable_dict)} ä¸ªå˜é‡")
            
            # ç»Ÿè®¡å˜é‡ç±»å‹åˆ†å¸ƒ
            category_counts = self.variable_dict['category'].value_counts()
            self.logger.info("ğŸ“Š å˜é‡ç±»å‹åˆ†å¸ƒ:")
            for cat, count in category_counts.head(10).items():
                self.logger.info(f"   {cat}: {count}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½å˜é‡å­—å…¸å¤±è´¥: {e}")
            return False
    
    def build_label_to_filepath_map(self) -> bool:
        """æ„å»ºæ ‡ç­¾åˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„"""
        try:
            self.logger.info("ğŸ—ºï¸  æ„å»ºæ ‡ç­¾åˆ°æ–‡ä»¶è·¯å¾„æ˜ å°„...")
            
            self.label_to_filepath_map = {}
            
            # è·å–æ•°æ®ç›®å½•ä¸­æ‰€æœ‰CSVæ–‡ä»¶
            available_files = list(self.data_dir.glob("*.csv"))
            available_file_names = [f.name for f in available_files]
            
            self.logger.info(f"å‘ç° {len(available_files)} ä¸ªæ•°æ®æ–‡ä»¶")
            
            for _, row in self.variable_dict.iterrows():
                label = row['variable_name']
                dict_file_name = row['file_name']
                
                # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
                exact_path = self.data_dir / dict_file_name
                if exact_path.exists():
                    self.label_to_filepath_map[label] = exact_path
                    continue
                
                # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ™ºèƒ½åŒ¹é…
                # æå–æ–‡ä»¶åçš„æ ¸å¿ƒéƒ¨åˆ†ï¼ˆå»é™¤ç¬¬ä¸€ä¸ªéƒ¨åˆ†ä½œä¸ºå‰ç¼€ï¼‰
                if '_' in dict_file_name:
                    core_name = '_'.join(dict_file_name.split('_')[1:])  # å»é™¤ç¬¬ä¸€ä¸ªéƒ¨åˆ†
                    
                    # åœ¨å¯ç”¨æ–‡ä»¶ä¸­æŸ¥æ‰¾åŒ…å«æ ¸å¿ƒåç§°çš„æ–‡ä»¶ï¼Œå¹¶éªŒè¯å˜é‡å­˜åœ¨
                    best_match = None
                    for available_file in available_file_names:
                        if core_name in available_file:
                            candidate_path = self.data_dir / available_file
                            
                            # éªŒè¯è¯¥æ–‡ä»¶æ˜¯å¦çœŸçš„åŒ…å«è¿™ä¸ªå˜é‡
                            try:
                                # åªè¯»å–è¡¨å¤´éªŒè¯
                                header = pd.read_csv(candidate_path, nrows=0)
                                if label in header.columns:
                                    best_match = candidate_path
                                    break
                            except Exception:
                                continue
                    
                    if best_match:
                        self.label_to_filepath_map[label] = best_match
                    else:
                        # å¦‚æœéªŒè¯å¤±è´¥ï¼Œå°è¯•ä¸éªŒè¯çš„åŒ¹é…ï¼ˆå…œåº•ç­–ç•¥ï¼‰
                        for available_file in available_file_names:
                            if core_name in available_file:
                                matched_path = self.data_dir / available_file
                                self.label_to_filepath_map[label] = matched_path
                                break
                        else:
                            self.logger.debug(f"æ–‡ä»¶æœªæ‰¾åˆ°: {dict_file_name} -> {core_name}")
                else:
                    self.logger.debug(f"æ–‡ä»¶ä¸å­˜åœ¨: {exact_path}")
            
            self.logger.info(f"âœ… æˆåŠŸæ„å»ºæ˜ å°„ï¼Œè¦†ç›– {len(self.label_to_filepath_map)} ä¸ªå˜é‡")
            
            # ç»Ÿè®¡åŒ¹é…æƒ…å†µ
            total_variables = len(self.variable_dict)
            matched_variables = len(self.label_to_filepath_map)
            match_rate = matched_variables / total_variables if total_variables > 0 else 0
            self.logger.info(f"ğŸ“Š æ–‡ä»¶åŒ¹é…ç‡: {match_rate:.2%} ({matched_variables}/{total_variables})")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºæ˜ å°„å¤±è´¥: {e}")
            return False
    
    def identify_core_mediators(self) -> bool:
        """è¯†åˆ«æ ¸å¿ƒä¸­ä»‹å˜é‡"""
        try:
            self.logger.info("ğŸ” è¯†åˆ«æ ¸å¿ƒä¸­ä»‹å˜é‡...")
            
            # ç­›é€‰Bå’ŒB&Cç±»å‹å˜é‡
            mediator_vars = self.variable_dict[
                self.variable_dict['category'].isin(['B', 'B&C'])
            ]
            
            # æå–å˜é‡åï¼ˆæ ‡ç­¾ï¼‰
            self.core_mediator_labels = mediator_vars['variable_name'].unique().tolist()
            
            # è¿‡æ»¤æ‰ä¸å­˜åœ¨æ–‡ä»¶è·¯å¾„çš„å˜é‡
            self.core_mediator_labels = [
                label for label in self.core_mediator_labels 
                if label in self.label_to_filepath_map
            ]
            
            self.logger.info(f"âœ… è¯†åˆ«åˆ° {len(self.core_mediator_labels)} ä¸ªæ ¸å¿ƒä¸­ä»‹å˜é‡")
            
            if len(self.core_mediator_labels) == 0:
                self.logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¸­ä»‹å˜é‡!")
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è¯†åˆ«ä¸­ä»‹å˜é‡å¤±è´¥: {e}")
            return False
    
    def calculate_variable_metadata(self, label: str) -> Dict:
        """è®¡ç®—å•ä¸ªå˜é‡çš„å…ƒæ•°æ®"""
        try:
            file_path = self.label_to_filepath_map[label]
            
            # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«æ‰€éœ€çš„åˆ—
            try:
                header = pd.read_csv(file_path, nrows=0)
                available_columns = header.columns.tolist()
                
                # æ£€æŸ¥IDåˆ—æ˜¯å¦å­˜åœ¨
                if self.id_column not in available_columns:
                    # å¦‚æœæ²¡æœ‰æ ‡å‡†IDåˆ—ï¼Œå°è¯•æ‰¾ç¬¬ä¸€åˆ—ä½œä¸ºID
                    if len(available_columns) > 0:
                        actual_id_col = available_columns[0]
                    else:
                        raise ValueError("æ–‡ä»¶æ²¡æœ‰ä»»ä½•åˆ—")
                else:
                    actual_id_col = self.id_column
                
                # æ£€æŸ¥ç›®æ ‡å˜é‡æ˜¯å¦å­˜åœ¨
                if label not in available_columns:
                    raise ValueError(f"å˜é‡ '{label}' ä¸åœ¨æ–‡ä»¶ä¸­")
                
                # åªåŠ è½½IDåˆ—å’Œç›®æ ‡å˜é‡åˆ—
                df = pd.read_csv(file_path, usecols=[actual_id_col, label], low_memory=False)
                
            except Exception as e:
                raise ValueError(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            metadata = {
                'label': label,
                'file_path': str(file_path),
                'total_count': len(df),
                'missing_count': df[label].isna().sum(),
                'missing_rate': df[label].isna().mean(),
                'valid_count': df[label].notna().sum()
            }
            
            # è®¡ç®—æ–¹å·®ï¼ˆåªå¯¹æ•°å€¼å‹æ•°æ®ï¼‰
            valid_data = df[label].dropna()
            if len(valid_data) > 0:
                try:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
                    numeric_data = pd.to_numeric(valid_data, errors='coerce').dropna()
                    if len(numeric_data) > 1:
                        metadata['variance'] = float(numeric_data.var())
                        metadata['mean'] = float(numeric_data.mean())
                        metadata['std'] = float(numeric_data.std())
                        metadata['is_numeric'] = True
                    else:
                        metadata['variance'] = 0.0
                        metadata['mean'] = np.nan
                        metadata['std'] = np.nan
                        metadata['is_numeric'] = False
                except:
                    metadata['variance'] = 0.0
                    metadata['mean'] = np.nan
                    metadata['std'] = np.nan
                    metadata['is_numeric'] = False
            else:
                metadata['variance'] = 0.0
                metadata['mean'] = np.nan
                metadata['std'] = np.nan
                metadata['is_numeric'] = False
            
            return metadata
            
        except Exception as e:
            self.logger.warning(f"è®¡ç®—å˜é‡ {label} å…ƒæ•°æ®å¤±è´¥: {e}")
            return {
                'label': label,
                'file_path': '',
                'total_count': 0,
                'missing_count': 0,
                'missing_rate': 1.0,
                'valid_count': 0,
                'variance': 0.0,
                'mean': np.nan,
                'std': np.nan,
                'is_numeric': False
            }
    
    def compute_metadata_iteratively(self, force_recalculate: bool = False) -> bool:
        """è¿­ä»£è®¡ç®—æ‰€æœ‰ä¸­ä»‹å˜é‡çš„å…ƒæ•°æ®"""
        try:
            metadata_path = self.metadata_dir / "mediator_metadata.csv"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å…ƒæ•°æ®æ–‡ä»¶
            if metadata_path.exists() and not force_recalculate:
                self.logger.info("ğŸ“‹ å‘ç°å·²å­˜åœ¨çš„å…ƒæ•°æ®æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
                try:
                    metadata_df = pd.read_csv(metadata_path)
                    self.logger.info(f"âœ… æˆåŠŸåŠ è½½å·²å­˜åœ¨çš„å…ƒæ•°æ®ï¼Œå…± {len(metadata_df)} ä¸ªå˜é‡")
                    
                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                    self.mediator_metadata = {}
                    for _, row in metadata_df.iterrows():
                        self.mediator_metadata[row['label']] = row.to_dict()
                    
                    # éªŒè¯å…ƒæ•°æ®å®Œæ•´æ€§
                    existing_labels = set(self.mediator_metadata.keys())
                    expected_labels = set(self.core_mediator_labels)
                    
                    if existing_labels >= expected_labels:
                        self.logger.info("âœ… å…ƒæ•°æ®å®Œæ•´ï¼Œè·³è¿‡é‡æ–°è®¡ç®—")
                        
                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        missing_rate_stats = metadata_df['missing_rate'].describe()
                        self.logger.info("ğŸ“ˆ ç¼ºå¤±ç‡ç»Ÿè®¡:")
                        for stat, value in missing_rate_stats.items():
                            self.logger.info(f"   {stat}: {value:.4f}")
                        
                        return True
                    else:
                        missing_count = len(expected_labels - existing_labels)
                        self.logger.warning(f"âš ï¸  å…ƒæ•°æ®ä¸å®Œæ•´ï¼Œç¼ºå°‘ {missing_count} ä¸ªå˜é‡ï¼Œå°†é‡æ–°è®¡ç®—")
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸  åŠ è½½å·²å­˜åœ¨å…ƒæ•°æ®å¤±è´¥: {e}ï¼Œå°†é‡æ–°è®¡ç®—")
            else:
                if force_recalculate:
                    self.logger.info("ğŸ”„ å¼ºåˆ¶é‡æ–°è®¡ç®—å…ƒæ•°æ®...")
                else:
                    self.logger.info("ğŸ“Š é¦–æ¬¡è®¡ç®—å˜é‡å…ƒæ•°æ®...")
            
            # æ‰§è¡Œå…ƒæ•°æ®è®¡ç®—
            self.logger.info(f"ä½¿ç”¨ {self.n_jobs} ä¸ªè¿›ç¨‹å¹¶è¡Œè®¡ç®—...")
            
            # ä¿®å¤joblibå…¼å®¹æ€§é—®é¢˜ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
            try:
                results = Parallel(n_jobs=self.n_jobs, verbose=1, backend='threading')(
                    delayed(self.calculate_variable_metadata)(label) 
                    for label in self.core_mediator_labels
                )
            except Exception as e:
                self.logger.warning(f"å¹¶è¡Œè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å•çº¿ç¨‹: {e}")
                # å›é€€åˆ°å•çº¿ç¨‹å¤„ç†
                results = []
                for i, label in enumerate(self.core_mediator_labels):
                    if i % 100 == 0:
                        self.logger.info(f"å¤„ç†è¿›åº¦: {i}/{len(self.core_mediator_labels)}")
                    result = self.calculate_variable_metadata(label)
                    results.append(result)
            
            # è½¬æ¢ä¸ºå­—å…¸
            self.mediator_metadata = {result['label']: result for result in results}
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_df = pd.DataFrame(results)
            metadata_df.to_csv(metadata_path, index=False)
            self.logger.info(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {metadata_path}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            missing_rate_stats = metadata_df['missing_rate'].describe()
            self.logger.info("ğŸ“ˆ ç¼ºå¤±ç‡ç»Ÿè®¡:")
            for stat, value in missing_rate_stats.items():
                self.logger.info(f"   {stat}: {value:.4f}")
            
            # æ£€æŸ¥å†…å­˜
            self.check_memory_usage("è®¡ç®—å…ƒæ•°æ®å")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è®¡ç®—å…ƒæ•°æ®å¤±è´¥: {e}")
            return False
    
    def filter_variables(self, 
                        max_missing_rate: float = 0.9, 
                        min_variance: float = 1e-8) -> bool:
        """æ ¹æ®è´¨é‡æ ‡å‡†ç­›é€‰å˜é‡"""
        try:
            self.logger.info(f"ğŸ”¬ ç­›é€‰å˜é‡ (ç¼ºå¤±ç‡â‰¤{max_missing_rate}, æ–¹å·®â‰¥{min_variance})...")
            
            original_count = len(self.core_mediator_labels)
            
            self.final_mediator_labels = []
            filtered_reasons = {'high_missing': 0, 'low_variance': 0, 'non_numeric': 0}
            
            for label in self.core_mediator_labels:
                metadata = self.mediator_metadata[label]
                
                # æ£€æŸ¥ç¼ºå¤±ç‡
                if metadata['missing_rate'] > max_missing_rate:
                    filtered_reasons['high_missing'] += 1
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼å‹
                if not metadata['is_numeric']:
                    filtered_reasons['non_numeric'] += 1
                    continue
                
                # æ£€æŸ¥æ–¹å·®
                if metadata['variance'] < min_variance:
                    filtered_reasons['low_variance'] += 1
                    continue
                
                self.final_mediator_labels.append(label)
            
            self.logger.info(f"âœ… ç­›é€‰å®Œæˆ: {original_count} â†’ {len(self.final_mediator_labels)}")
            self.logger.info(f"   è¿‡æ»¤åŸå› : é«˜ç¼ºå¤±={filtered_reasons['high_missing']}, "
                           f"ä½æ–¹å·®={filtered_reasons['low_variance']}, "
                           f"éæ•°å€¼={filtered_reasons['non_numeric']}")
            
            if len(self.final_mediator_labels) < 2:
                self.logger.error("âŒ æœ‰æ•ˆå˜é‡æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ!")
                return False
            
            # ä¿å­˜æœ€ç»ˆå˜é‡åˆ—è¡¨
            final_vars_path = self.metadata_dir / "final_mediator_variables.txt"
            with open(final_vars_path, 'w', encoding='utf-8') as f:
                for label in self.final_mediator_labels:
                    f.write(f"{label}\n")
            
            self.logger.info(f"ğŸ’¾ æœ€ç»ˆå˜é‡åˆ—è¡¨å·²ä¿å­˜åˆ°: {final_vars_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å˜é‡ç­›é€‰å¤±è´¥: {e}")
            return False
    
    def get_id_column_for_file(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶çš„å®é™…IDåˆ—å"""
        try:
            header = pd.read_csv(file_path, nrows=0)
            columns = header.columns.tolist()
            
            # é¦–å…ˆå°è¯•æ ‡å‡†IDåˆ—
            if self.id_column in columns:
                return self.id_column
            
            # å¦‚æœæ²¡æœ‰æ ‡å‡†IDåˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—
            if len(columns) > 0:
                return columns[0]
            
            raise ValueError("æ–‡ä»¶æ²¡æœ‰ä»»ä½•åˆ—")
            
        except Exception as e:
            raise ValueError(f"æ— æ³•ç¡®å®šIDåˆ—: {e}")
    
    def calculate_pairwise_correlation(self, label1: str, label2: str) -> Tuple[int, int, float]:
        """è®¡ç®—ä¸¤ä¸ªå˜é‡çš„ç›¸å…³ç³»æ•°"""
        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            file_path1 = self.label_to_filepath_map[label1]
            file_path2 = self.label_to_filepath_map[label2]
            
            # è·å–æ¯ä¸ªæ–‡ä»¶çš„å®é™…IDåˆ—å
            id_col1 = self.get_id_column_for_file(file_path1)
            id_col2 = self.get_id_column_for_file(file_path2)
            
            # åŠ è½½æ•°æ®
            if file_path1 == file_path2:
                # åŒä¸€æ–‡ä»¶ï¼Œä¸€æ¬¡æ€§åŠ è½½
                # æ£€æŸ¥æ‰€æœ‰éœ€è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                header = pd.read_csv(file_path1, nrows=0)
                available_cols = header.columns.tolist()
                
                if id_col1 not in available_cols or label1 not in available_cols or label2 not in available_cols:
                    raise ValueError(f"æ–‡ä»¶ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ—")
                
                df = pd.read_csv(file_path1, usecols=[id_col1, label1, label2], low_memory=False)
                df1 = df[[id_col1, label1]].copy()
                df2 = df[[id_col1, label2]].copy()
                
                # é‡å‘½åIDåˆ—ä»¥ä¾¿åˆå¹¶
                df1.rename(columns={id_col1: 'ID'}, inplace=True)
                df2.rename(columns={id_col1: 'ID'}, inplace=True)
                
            else:
                # ä¸åŒæ–‡ä»¶ï¼Œåˆ†åˆ«åŠ è½½
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡ä»¶
                header1 = pd.read_csv(file_path1, nrows=0)
                if id_col1 not in header1.columns or label1 not in header1.columns:
                    raise ValueError(f"æ–‡ä»¶1ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ—: {[id_col1, label1]}")
                
                # æ£€æŸ¥ç¬¬äºŒä¸ªæ–‡ä»¶  
                header2 = pd.read_csv(file_path2, nrows=0)
                if id_col2 not in header2.columns or label2 not in header2.columns:
                    raise ValueError(f"æ–‡ä»¶2ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ—: {[id_col2, label2]}")
                
                df1 = pd.read_csv(file_path1, usecols=[id_col1, label1], low_memory=False)
                df2 = pd.read_csv(file_path2, usecols=[id_col2, label2], low_memory=False)
                
                # é‡å‘½åIDåˆ—ä»¥ä¾¿åˆå¹¶
                df1.rename(columns={id_col1: 'ID'}, inplace=True)
                df2.rename(columns={id_col2: 'ID'}, inplace=True)
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(df1, df2, on='ID', how='inner')
            
            # ç§»é™¤ç¼ºå¤±å€¼
            merged_df = merged_df.dropna()
            
            if len(merged_df) < 50:  # è¦æ±‚è‡³å°‘50ä¸ªæœ‰æ•ˆè§‚æµ‹
                return self.final_mediator_labels.index(label1), self.final_mediator_labels.index(label2), 0.0
            
            # è½¬æ¢ä¸ºæ•°å€¼å‹
            x = pd.to_numeric(merged_df[label1], errors='coerce')
            y = pd.to_numeric(merged_df[label2], errors='coerce')
            
            # å†æ¬¡ç§»é™¤è½¬æ¢å¤±è´¥çš„å€¼
            valid_mask = ~(x.isna() | y.isna())
            x = x[valid_mask]
            y = y[valid_mask]
            
            if len(x) < 50:
                return self.final_mediator_labels.index(label1), self.final_mediator_labels.index(label2), 0.0
            
            # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
            correlation, _ = pearsonr(x, y)
            
            # å¤„ç†NaNå€¼
            if np.isnan(correlation):
                correlation = 0.0
            
            return self.final_mediator_labels.index(label1), self.final_mediator_labels.index(label2), correlation
            
        except Exception as e:
            self.logger.warning(f"è®¡ç®—ç›¸å…³æ€§å¤±è´¥ {label1} vs {label2}: {e}")
            return self.final_mediator_labels.index(label1), self.final_mediator_labels.index(label2), 0.0
    
    def build_correlation_matrix_parallel(self) -> bool:
        """å¹¶è¡Œæ„å»ºç›¸å…³æ€§çŸ©é˜µ"""
        try:
            self.logger.info("ğŸ”— å¼€å§‹å¹¶è¡Œæ„å»ºç›¸å…³æ€§çŸ©é˜µ...")
            
            n_vars = len(self.final_mediator_labels)
            self.logger.info(f"å˜é‡æ•°é‡: {n_vars}, éœ€è¦è®¡ç®— {n_vars*(n_vars-1)//2} ä¸ªå˜é‡å¯¹")
            
            # åˆå§‹åŒ–ç›¸å…³æ€§çŸ©é˜µ
            corr_matrix = np.eye(n_vars)  # å¯¹è§’çº¿ä¸º1
            
            # åˆ›å»ºå˜é‡å¯¹
            variable_pairs = []
            for i in range(n_vars):
                for j in range(i+1, n_vars):
                    variable_pairs.append((self.final_mediator_labels[i], self.final_mediator_labels[j]))
            
            # åˆ†æ‰¹å¤„ç†
            batch_size = 10000
            n_batches = (len(variable_pairs) + batch_size - 1) // batch_size
            self.logger.info(f"å°†è®¡ç®—åˆ†ä¸º {n_batches} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æœ€å¤š {batch_size} ä¸ªå¯¹")
            
            for batch_idx in range(n_batches):
                start = batch_idx * batch_size
                end = min(start + batch_size, len(variable_pairs))
                batch_pairs = variable_pairs[start:end]
                
                self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx+1}/{n_batches} ({len(batch_pairs)} ä¸ªå¯¹)")
                
                # å¹¶è¡Œè®¡ç®—æ‰¹æ¬¡
                results = Parallel(n_jobs=self.n_jobs, verbose=10, batch_size=100)(
                    delayed(self.calculate_pairwise_correlation)(label1, label2) 
                    for label1, label2 in batch_pairs
                )
                
                # æ›´æ–°çŸ©é˜µ
                for i, j, corr in results:
                    corr_matrix[i, j] = corr
                    corr_matrix[j, i] = corr
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                intermediate_path = self.correlation_dir / f"correlation_matrix_batch_{batch_idx+1}.npy"
                np.save(intermediate_path, corr_matrix)
                self.logger.info(f"ğŸ’¾ ä¸­é—´ç›¸å…³æ€§çŸ©é˜µä¿å­˜åˆ°: {intermediate_path}")
                
                # æ£€æŸ¥å†…å­˜
                self.check_memory_usage(f"æ‰¹æ¬¡ {batch_idx+1} å®Œæˆå")
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            corr_matrix_path = self.correlation_dir / "correlation_matrix.npy"
            np.save(corr_matrix_path, corr_matrix)
            self.logger.info(f"ğŸ’¾ æœ€ç»ˆç›¸å…³æ€§çŸ©é˜µå·²ä¿å­˜åˆ°: {corr_matrix_path}")
            
            # ä¿å­˜ä¸ºCSV
            corr_df = pd.DataFrame(corr_matrix, 
                                 index=self.final_mediator_labels, 
                                 columns=self.final_mediator_labels)
            corr_csv_path = self.correlation_dir / "correlation_matrix.csv"
            corr_df.to_csv(corr_csv_path)
            
            # ç»Ÿè®¡
            upper_triangle = corr_matrix[np.triu_indices(n_vars, k=1)]
            corr_stats = pd.Series(upper_triangle).describe()
            self.logger.info("ğŸ“ˆ ç›¸å…³æ€§ç»Ÿè®¡:")
            for stat, value in corr_stats.items():
                self.logger.info(f"   {stat}: {value:.4f}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºç›¸å…³æ€§çŸ©é˜µå¤±è´¥: {e}")
            return False
    
    def perform_hierarchical_clustering(self, 
                                      linkage_method: str = 'ward',
                                      n_clusters: int = 20) -> bool:
        """æ‰§è¡Œå±‚æ¬¡èšç±»"""
        try:
            self.logger.info(f"ğŸŒ³ å¼€å§‹å±‚æ¬¡èšç±» (æ–¹æ³•: {linkage_method}, ç°‡æ•°: {n_clusters})...")
            
            # åŠ è½½ç›¸å…³æ€§çŸ©é˜µ
            corr_matrix_path = self.correlation_dir / "correlation_matrix.npy"
            corr_matrix = np.load(corr_matrix_path)
            
            # è½¬æ¢ä¸ºè·ç¦»çŸ©é˜µ
            distance_matrix = 1 - np.abs(corr_matrix)
            
            # ç¡®ä¿è·ç¦»çŸ©é˜µçš„å¯¹è§’çº¿ä¸º0
            np.fill_diagonal(distance_matrix, 0)
            
            # å°†è·ç¦»çŸ©é˜µè½¬æ¢ä¸ºå‹ç¼©å½¢å¼
            from scipy.spatial.distance import squareform
            distance_vector = squareform(distance_matrix, checks=False)
            
            # æ‰§è¡Œå±‚æ¬¡èšç±»
            self.logger.info("ğŸ”— æ‰§è¡Œå±‚æ¬¡èšç±»...")
            linkage_matrix = linkage(distance_vector, method=linkage_method)
            
            # ç”Ÿæˆæ ‘çŠ¶å›¾
            self.logger.info("ğŸ¨ ç”Ÿæˆæ ‘çŠ¶å›¾...")
            plt.figure(figsize=(20, 10))
            dendrogram(linkage_matrix, 
                      labels=self.final_mediator_labels,
                      leaf_rotation=90,
                      leaf_font_size=8)
            plt.title(f'Hierarchical Clustering Dendrogram ({linkage_method})')
            plt.xlabel('Variables')
            plt.ylabel('Distance')
            plt.tight_layout()
            
            dendrogram_path = self.visualization_dir / "dendrogram_correlation.png"
            plt.savefig(dendrogram_path, dpi=300, bbox_inches='tight')
            plt.close()
            self.logger.info(f"ğŸ–¼ï¸  æ ‘çŠ¶å›¾å·²ä¿å­˜åˆ°: {dendrogram_path}")
            
            # ç”Ÿæˆç°‡æ ‡ç­¾
            cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')
            
            # åˆ›å»ºæ¨¡å—æ˜ å°„
            module_mapping = pd.DataFrame({
                'Label': self.final_mediator_labels,
                'ModuleID': cluster_labels
            })
            
            # ä¿å­˜æ¨¡å—æ˜ å°„
            mapping_path = self.clustering_dir / "mediator_module_mapping_v2.csv"
            module_mapping.to_csv(mapping_path, index=False)
            self.logger.info(f"ğŸ’¾ æ¨¡å—æ˜ å°„å·²ä¿å­˜åˆ°: {mapping_path}")
            
            # ç»Ÿè®¡æ¨¡å—ä¿¡æ¯
            module_stats = module_mapping['ModuleID'].value_counts().sort_index()
            self.logger.info("ğŸ“Š æ¨¡å—ç»Ÿè®¡:")
            for module_id, count in module_stats.items():
                self.logger.info(f"   æ¨¡å— {module_id}: {count} ä¸ªå˜é‡")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å±‚æ¬¡èšç±»å¤±è´¥: {e}")
            return False
    
    def load_module_data(self, module_labels: List[str]) -> pd.DataFrame:
        """åŠ è½½æ¨¡å—ä¸­æ‰€æœ‰å˜é‡çš„æ•°æ®"""
        try:
            # æŒ‰æ–‡ä»¶åˆ†ç»„å˜é‡
            file_groups = {}
            for label in module_labels:
                file_path = self.label_to_filepath_map[label]
                if file_path not in file_groups:
                    file_groups[file_path] = []
                file_groups[file_path].append(label)
            
            # åŠ è½½å¹¶åˆå¹¶æ•°æ®
            module_dfs = []
            for file_path, labels_in_file in file_groups.items():
                # è·å–å®é™…çš„IDåˆ—å
                id_col = self.get_id_column_for_file(file_path)
                
                # æ£€æŸ¥æ‰€æœ‰åˆ—æ˜¯å¦å­˜åœ¨
                header = pd.read_csv(file_path, nrows=0)
                available_cols = header.columns.tolist()
                
                # è¿‡æ»¤å­˜åœ¨çš„åˆ—
                valid_labels = [label for label in labels_in_file if label in available_cols]
                if not valid_labels:
                    continue
                
                columns_to_load = [id_col] + valid_labels
                df = pd.read_csv(file_path, usecols=columns_to_load, low_memory=False)
                
                # é‡å‘½åIDåˆ—ä¸ºæ ‡å‡†åç§°
                df.rename(columns={id_col: 'ID'}, inplace=True)
                module_dfs.append(df)
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            if len(module_dfs) == 0:
                return pd.DataFrame()
            elif len(module_dfs) == 1:
                merged_df = module_dfs[0]
            else:
                merged_df = module_dfs[0]
                for df in module_dfs[1:]:
                    merged_df = pd.merge(merged_df, df, on='ID', how='outer')
            
            return merged_df
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å—æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def generate_module_representatives(self, 
                                      n_components: int = 1,
                                      imputation_neighbors: int = 5) -> bool:
        """ç”Ÿæˆæ¨¡å—ä»£è¡¨"""
        try:
            self.logger.info("ğŸ§¬ å¼€å§‹ç”Ÿæˆæ¨¡å—ä»£è¡¨...")
            
            # åŠ è½½æ¨¡å—æ˜ å°„
            mapping_path = self.clustering_dir / "mediator_module_mapping_v2.csv"
            module_mapping = pd.read_csv(mapping_path)
            
            # åˆå§‹åŒ–æœ€ç»ˆç»“æœDataFrameï¼ˆåªåŒ…å«IDï¼‰
            # æˆ‘ä»¬éœ€è¦è·å–æ‰€æœ‰å‚ä¸è€…çš„ID
            self.logger.info("ğŸ“‹ è·å–æ‰€æœ‰å‚ä¸è€…ID...")
            
            # ä»ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶è·å–å®Œæ•´çš„IDåˆ—è¡¨
            first_file = next(iter(self.label_to_filepath_map.values()))
            first_file_id_col = self.get_id_column_for_file(first_file)
            all_ids_df = pd.read_csv(first_file, usecols=[first_file_id_col], low_memory=False)
            # é‡å‘½åä¸ºæ ‡å‡†IDåˆ—å
            all_ids_df.rename(columns={first_file_id_col: 'ID'}, inplace=True)
            final_representatives = all_ids_df.copy()
            
            # æŒ‰æ¨¡å—å¤„ç†
            unique_modules = sorted(module_mapping['ModuleID'].unique())
            self.logger.info(f"å¤„ç† {len(unique_modules)} ä¸ªæ¨¡å—...")
            
            for module_idx, module_id in enumerate(unique_modules):
                try:
                    self.logger.info(f"ğŸ”¬ å¤„ç†æ¨¡å— {module_id} ({module_idx+1}/{len(unique_modules)})...")
                    
                    # è·å–æ¨¡å—ä¸­çš„å˜é‡
                    module_labels = module_mapping[
                        module_mapping['ModuleID'] == module_id
                    ]['Label'].tolist()
                    
                    self.logger.info(f"   æ¨¡å— {module_id} åŒ…å« {len(module_labels)} ä¸ªå˜é‡")
                    
                    # åŠ è½½æ¨¡å—æ•°æ®
                    module_df = self.load_module_data(module_labels)
                    
                    if module_df.empty:
                        self.logger.warning(f"   æ¨¡å— {module_id} æ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡")
                        continue
                    
                    # æ£€æŸ¥å†…å­˜
                    self.check_memory_usage(f"åŠ è½½æ¨¡å— {module_id} å")
                    
                    # å‡†å¤‡æ•°æ®ç”¨äºPCA
                    feature_columns = [col for col in module_df.columns if col != 'ID']
                    X = module_df[feature_columns].copy()
                    
                    # è½¬æ¢ä¸ºæ•°å€¼å‹
                    self.logger.info(f"   æ¨¡å— {module_id}: è½¬æ¢æ•°æ®ç±»å‹...")
                    for col in feature_columns:
                        X[col] = pd.to_numeric(X[col], errors='coerce')
                    
                    # æ£€æŸ¥æœ‰æ•ˆæ•°æ®
                    valid_samples = X.notna().any(axis=1)
                    if valid_samples.sum() < 50:
                        self.logger.warning(f"   æ¨¡å— {module_id} æœ‰æ•ˆæ ·æœ¬ä¸è¶³({valid_samples.sum()})ï¼Œè·³è¿‡")
                        continue
                    
                    # ç¼ºå¤±å€¼å¡«è¡¥
                    self.logger.info(f"   æ¨¡å— {module_id}: å¼€å§‹KNNç¼ºå¤±å€¼å¡«è¡¥...")
                    self.logger.info(f"   æ•°æ®ç»´åº¦: {X.shape}, ç¼ºå¤±ç‡: {X.isna().sum().sum() / (X.shape[0] * X.shape[1]):.2%}")
                    
                    # è®¡ç®—åˆé€‚çš„é‚»å±…æ•°
                    effective_neighbors = min(imputation_neighbors, valid_samples.sum()-1, 10)
                    self.logger.info(f"   ä½¿ç”¨KNNé‚»å±…æ•°: {effective_neighbors}")
                    
                    try:
                        imputer = KNNImputer(n_neighbors=effective_neighbors)
                        
                        # åˆ†å—å¤„ç†å¤§æ•°æ®ä»¥æ˜¾ç¤ºè¿›åº¦
                        if X.shape[0] > 10000:
                            self.logger.info(f"   å¤§æ•°æ®é›†æ£€æµ‹ï¼Œåˆ†å—å¤„ç†...")
                            chunk_size = 5000
                            n_chunks = (X.shape[0] + chunk_size - 1) // chunk_size
                            
                            X_imputed_chunks = []
                            for chunk_idx in range(n_chunks):
                                start_idx = chunk_idx * chunk_size
                                end_idx = min(start_idx + chunk_size, X.shape[0])
                                
                                self.logger.info(f"   KNNå¡«è¡¥è¿›åº¦: å— {chunk_idx+1}/{n_chunks} ({start_idx}:{end_idx})")
                                
                                X_chunk = X.iloc[start_idx:end_idx]
                                X_chunk_imputed = imputer.fit_transform(X_chunk)
                                X_imputed_chunks.append(X_chunk_imputed)
                            
                            X_imputed_array = np.vstack(X_imputed_chunks)
                        else:
                            self.logger.info(f"   æ‰§è¡ŒKNNå¡«è¡¥...")
                            X_imputed_array = imputer.fit_transform(X)
                        
                        X_imputed = pd.DataFrame(X_imputed_array, columns=feature_columns, index=X.index)
                        self.logger.info(f"   KNNå¡«è¡¥å®Œæˆ")
                        
                    except Exception as impute_error:
                        self.logger.warning(f"   KNNå¡«è¡¥å¤±è´¥: {impute_error}, ä½¿ç”¨å‡å€¼å¡«è¡¥")
                        X_imputed = X.fillna(X.mean())
                    
                    # æ ‡å‡†åŒ–
                    self.logger.info(f"   æ¨¡å— {module_id}: æ ‡å‡†åŒ–...")
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X_imputed)
                    
                    # PCA
                    self.logger.info(f"   æ¨¡å— {module_id}: PCAé™ç»´...")
                    pca = PCA(n_components=n_components)
                    X_pca = pca.fit_transform(X_scaled)
                    
                    # åˆ›å»ºä¸»æˆåˆ†DataFrame
                    pc_column = f"Module_{module_id}_PC1"
                    pc_df = pd.DataFrame({
                        'ID': module_df['ID'],
                        pc_column: X_pca[:, 0]
                    })
                    
                    # åˆå¹¶åˆ°æœ€ç»ˆç»“æœ
                    final_representatives = pd.merge(
                        final_representatives, pc_df, 
                        on='ID', how='left'
                    )
                    
                    # ä¿å­˜ä¸­é—´ç»“æœ
                    intermediate_path = self.module_dir / "module_representatives_partial.parquet"
                    final_representatives.to_parquet(intermediate_path, index=False)
                    self.logger.info(f"ğŸ’¾ ä¸­é—´æ¨¡å—ä»£è¡¨ä¿å­˜åˆ°: {intermediate_path}")
                    
                    # è®°å½•PCAä¿¡æ¯
                    explained_variance = pca.explained_variance_ratio_[0]
                    self.logger.info(f"   æ¨¡å— {module_id}: PC1è§£é‡Šæ–¹å·®æ¯” = {explained_variance:.4f}")
                    self.logger.info(f"   å®Œæˆè¿›åº¦: {module_idx+1}/{len(unique_modules)} ({(module_idx+1)/len(unique_modules)*100:.1f}%)")
                    
                    # æ¸…ç†å†…å­˜
                    del module_df, X, X_imputed, X_scaled, X_pca, pc_df
                    gc.collect()
                    
                except Exception as e:
                    self.logger.error(f"   å¤„ç†æ¨¡å— {module_id} å¤±è´¥: {e}")
                    continue
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            output_path = self.module_dir / "module_representatives_pca_v2.parquet"
            final_representatives.to_parquet(output_path, index=False)
            self.logger.info(f"ğŸ’¾ æœ€ç»ˆæ¨¡å—ä»£è¡¨å·²ä¿å­˜åˆ°: {output_path}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            n_modules_generated = len([col for col in final_representatives.columns if col.startswith("Module_")])
            self.logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {n_modules_generated} ä¸ªæ¨¡å—ä»£è¡¨")
            
            # æœ€ç»ˆå†…å­˜æ£€æŸ¥
            self.check_memory_usage("ç”Ÿæˆæ¨¡å—ä»£è¡¨å®Œæˆå")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆæ¨¡å—ä»£è¡¨å¤±è´¥: {e}")
            return False
    
    def visualize_correlation_matrix(self) -> bool:
        """å¯è§†åŒ–ç›¸å…³æ€§çŸ©é˜µ"""
        try:
            self.logger.info("ğŸ¨ ç”Ÿæˆç›¸å…³æ€§çŸ©é˜µå¯è§†åŒ–...")
            
            # åŠ è½½ç›¸å…³æ€§çŸ©é˜µ
            corr_matrix_path = self.correlation_dir / "correlation_matrix.npy"
            corr_matrix = np.load(corr_matrix_path)
            
            # åˆ›å»ºå¤§å›¾
            fig, axes = plt.subplots(2, 2, figsize=(20, 16))
            fig.suptitle('ä¸­ä»‹å˜é‡ç›¸å…³æ€§çŸ©é˜µåˆ†æ', fontsize=16, fontweight='bold')
            
            # 1. ç›¸å…³æ€§åˆ†å¸ƒç›´æ–¹å›¾
            upper_triangle = corr_matrix[np.triu_indices(corr_matrix.shape[0], k=1)]
            axes[0, 0].hist(upper_triangle, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            axes[0, 0].set_title('ç›¸å…³æ€§åˆ†å¸ƒç›´æ–¹å›¾')
            axes[0, 0].set_xlabel('ç›¸å…³ç³»æ•°')
            axes[0, 0].set_ylabel('é¢‘æ•°')
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. ç›¸å…³æ€§çƒ­åŠ›å›¾ï¼ˆé‡‡æ ·æ˜¾ç¤ºï¼‰
            sample_size = min(100, corr_matrix.shape[0])
            sample_indices = np.linspace(0, corr_matrix.shape[0]-1, sample_size, dtype=int)
            sample_matrix = corr_matrix[sample_indices][:, sample_indices]
            
            im = axes[0, 1].imshow(sample_matrix, cmap='RdBu_r', vmin=-1, vmax=1)
            axes[0, 1].set_title(f'ç›¸å…³æ€§çƒ­åŠ›å›¾ (é‡‡æ ·{sample_size}ä¸ªå˜é‡)')
            axes[0, 1].set_xlabel('å˜é‡ç´¢å¼•')
            axes[0, 1].set_ylabel('å˜é‡ç´¢å¼•')
            plt.colorbar(im, ax=axes[0, 1])
            
            # 3. ç›¸å…³æ€§ç»Ÿè®¡ç®±çº¿å›¾
            # è®¡ç®—æ¯ä¸ªå˜é‡çš„å¹³å‡ç›¸å…³æ€§
            mean_correlations = np.mean(np.abs(corr_matrix), axis=1)
            axes[1, 0].boxplot(mean_correlations)
            axes[1, 0].set_title('å˜é‡å¹³å‡ç›¸å…³æ€§åˆ†å¸ƒ')
            axes[1, 0].set_ylabel('å¹³å‡ç»å¯¹ç›¸å…³ç³»æ•°')
            axes[1, 0].grid(True, alpha=0.3)
            
            # 4. ç›¸å…³æ€§å¼ºåº¦åˆ†å¸ƒ
            # æŒ‰ç›¸å…³æ€§å¼ºåº¦åˆ†ç±»
            strong_corr = np.sum(np.abs(upper_triangle) > 0.7)
            moderate_corr = np.sum((np.abs(upper_triangle) > 0.3) & (np.abs(upper_triangle) <= 0.7))
            weak_corr = np.sum((np.abs(upper_triangle) > 0.1) & (np.abs(upper_triangle) <= 0.3))
            very_weak_corr = np.sum(np.abs(upper_triangle) <= 0.1)
            
            categories = ['å¼ºç›¸å…³\n(>0.7)', 'ä¸­ç­‰ç›¸å…³\n(0.3-0.7)', 'å¼±ç›¸å…³\n(0.1-0.3)', 'æå¼±ç›¸å…³\n(â‰¤0.1)']
            counts = [strong_corr, moderate_corr, weak_corr, very_weak_corr]
            colors = ['red', 'orange', 'yellow', 'lightblue']
            
            bars = axes[1, 1].bar(categories, counts, color=colors, alpha=0.7)
            axes[1, 1].set_title('ç›¸å…³æ€§å¼ºåº¦åˆ†å¸ƒ')
            axes[1, 1].set_ylabel('å˜é‡å¯¹æ•°é‡')
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, counts):
                height = bar.get_height()
                axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,
                               f'{count:,}\n({count/len(upper_triangle)*100:.1f}%)',
                               ha='center', va='bottom')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            viz_path = self.visualization_dir / "correlation_matrix_analysis.png"
            plt.savefig(viz_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"ğŸ’¾ ç›¸å…³æ€§çŸ©é˜µå¯è§†åŒ–å·²ä¿å­˜åˆ°: {viz_path}")
            
            # ä¿å­˜ç›¸å…³æ€§ç»Ÿè®¡åˆ°CSV
            corr_stats = {
                'ç»Ÿè®¡æŒ‡æ ‡': ['æ€»å˜é‡å¯¹æ•°é‡', 'å¼ºç›¸å…³(>0.7)', 'ä¸­ç­‰ç›¸å…³(0.3-0.7)', 'å¼±ç›¸å…³(0.1-0.3)', 'æå¼±ç›¸å…³(â‰¤0.1)'],
                'æ•°é‡': [len(upper_triangle), strong_corr, moderate_corr, weak_corr, very_weak_corr],
                'ç™¾åˆ†æ¯”': [100, strong_corr/len(upper_triangle)*100, moderate_corr/len(upper_triangle)*100, 
                          weak_corr/len(upper_triangle)*100, very_weak_corr/len(upper_triangle)*100]
            }
            corr_stats_df = pd.DataFrame(corr_stats)
            corr_stats_path = self.correlation_dir / "correlation_statistics.csv"
            corr_stats_df.to_csv(corr_stats_path, index=False)
            
            self.logger.info(f"ğŸ’¾ ç›¸å…³æ€§ç»Ÿè®¡å·²ä¿å­˜åˆ°: {corr_stats_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç›¸å…³æ€§çŸ©é˜µå¯è§†åŒ–å¤±è´¥: {e}")
            return False
    
    def visualize_clustering_results(self) -> bool:
        """å¯è§†åŒ–èšç±»ç»“æœ"""
        try:
            self.logger.info("ğŸ¨ ç”Ÿæˆèšç±»ç»“æœå¯è§†åŒ–...")
            
            # åŠ è½½æ¨¡å—æ˜ å°„
            mapping_path = self.clustering_dir / "mediator_module_mapping_v2.csv"
            module_mapping = pd.read_csv(mapping_path)
            
            # åˆ›å»ºå¤§å›¾
            fig, axes = plt.subplots(2, 2, figsize=(20, 16))
            fig.suptitle('ä¸­ä»‹å˜é‡èšç±»åˆ†æç»“æœ', fontsize=16, fontweight='bold')
            
            # 1. æ¨¡å—å¤§å°åˆ†å¸ƒ
            module_sizes = module_mapping['ModuleID'].value_counts().sort_index()
            axes[0, 0].bar(range(1, len(module_sizes)+1), module_sizes.values, alpha=0.7, color='lightcoral')
            axes[0, 0].set_title('å„æ¨¡å—å˜é‡æ•°é‡åˆ†å¸ƒ')
            axes[0, 0].set_xlabel('æ¨¡å—ID')
            axes[0, 0].set_ylabel('å˜é‡æ•°é‡')
            axes[0, 0].grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, size in enumerate(module_sizes.values):
                axes[0, 0].text(i+1, size, str(size), ha='center', va='bottom')
            
            # 2. æ¨¡å—å¤§å°é¥¼å›¾
            sizes = module_sizes.values
            labels = [f'æ¨¡å—{i}' for i in module_sizes.index]
            colors = plt.cm.Set3(np.linspace(0, 1, len(sizes)))
            
            axes[0, 1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            axes[0, 1].set_title('æ¨¡å—å¤§å°æ¯”ä¾‹åˆ†å¸ƒ')
            
            # 3. æ¨¡å—å¤§å°ç»Ÿè®¡
            size_stats = module_sizes.describe()
            axes[1, 0].text(0.1, 0.9, f"æ¨¡å—ç»Ÿè®¡ä¿¡æ¯:", fontsize=12, fontweight='bold', transform=axes[1, 0].transAxes)
            y_pos = 0.8
            for stat, value in size_stats.items():
                axes[1, 0].text(0.1, y_pos, f"{stat}: {value:.1f}", fontsize=10, transform=axes[1, 0].transAxes)
                y_pos -= 0.1
            
            axes[1, 0].set_xlim(0, 1)
            axes[1, 0].set_ylim(0, 1)
            axes[1, 0].set_title('æ¨¡å—å¤§å°ç»Ÿè®¡')
            axes[1, 0].axis('off')
            
            # 4. æ¨¡å—å¤§å°åˆ†å¸ƒç›´æ–¹å›¾
            axes[1, 1].hist(module_sizes.values, bins=10, alpha=0.7, color='lightgreen', edgecolor='black')
            axes[1, 1].set_title('æ¨¡å—å¤§å°åˆ†å¸ƒç›´æ–¹å›¾')
            axes[1, 1].set_xlabel('æ¨¡å—å¤§å°')
            axes[1, 1].set_ylabel('é¢‘æ•°')
            axes[1, 1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            viz_path = self.visualization_dir / "clustering_analysis.png"
            plt.savefig(viz_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"ğŸ’¾ èšç±»ç»“æœå¯è§†åŒ–å·²ä¿å­˜åˆ°: {viz_path}")
            
            # ä¿å­˜æ¨¡å—ç»Ÿè®¡åˆ°CSV
            module_stats = pd.DataFrame({
                'æ¨¡å—ID': module_sizes.index,
                'å˜é‡æ•°é‡': module_sizes.values,
                'ç™¾åˆ†æ¯”': module_sizes.values / len(module_mapping) * 100
            })
            module_stats_path = self.clustering_dir / "module_statistics.csv"
            module_stats.to_csv(module_stats_path, index=False)
            
            self.logger.info(f"ğŸ’¾ æ¨¡å—ç»Ÿè®¡å·²ä¿å­˜åˆ°: {module_stats_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ èšç±»ç»“æœå¯è§†åŒ–å¤±è´¥: {e}")
            return False
    
    def visualize_module_representatives(self) -> bool:
        """å¯è§†åŒ–æ¨¡å—ä»£è¡¨"""
        try:
            self.logger.info("ğŸ¨ ç”Ÿæˆæ¨¡å—ä»£è¡¨å¯è§†åŒ–...")
            
            # åŠ è½½æ¨¡å—ä»£è¡¨æ•°æ®
            partial_path = self.module_dir / "module_representatives_partial.parquet"
            if not partial_path.exists():
                self.logger.warning("âš ï¸ æ¨¡å—ä»£è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯è§†åŒ–")
                return False
            
            module_data = pd.read_parquet(partial_path)
            
            # è·å–æ¨¡å—åˆ—
            module_columns = [col for col in module_data.columns if col.startswith('Module_')]
            if not module_columns:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¨¡å—ä»£è¡¨åˆ—ï¼Œè·³è¿‡å¯è§†åŒ–")
                return False
            
            # åˆ›å»ºå¤§å›¾
            fig, axes = plt.subplots(2, 2, figsize=(20, 16))
            fig.suptitle('æ¨¡å—ä»£è¡¨åˆ†æ', fontsize=16, fontweight='bold')
            
            # 1. æ¨¡å—ä»£è¡¨åˆ†å¸ƒ
            for i, col in enumerate(module_columns[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªæ¨¡å—
                valid_data = module_data[col].dropna()
                if len(valid_data) > 0:
                    axes[0, 0].hist(valid_data, bins=30, alpha=0.6, label=f'æ¨¡å—{col.split("_")[1]}')
            
            axes[0, 0].set_title('æ¨¡å—ä»£è¡¨åˆ†å¸ƒ (å‰5ä¸ªæ¨¡å—)')
            axes[0, 0].set_xlabel('æ¨¡å—ä»£è¡¨å€¼')
            axes[0, 0].set_ylabel('é¢‘æ•°')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. æ¨¡å—ä»£è¡¨ç®±çº¿å›¾
            plot_data = []
            plot_labels = []
            for col in module_columns[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæ¨¡å—
                valid_data = module_data[col].dropna()
                if len(valid_data) > 0:
                    plot_data.append(valid_data)
                    plot_labels.append(f'æ¨¡å—{col.split("_")[1]}')
            
            if plot_data:
                axes[0, 1].boxplot(plot_data, labels=plot_labels)
                axes[0, 1].set_title('æ¨¡å—ä»£è¡¨ç®±çº¿å›¾ (å‰10ä¸ªæ¨¡å—)')
                axes[0, 1].set_ylabel('æ¨¡å—ä»£è¡¨å€¼')
                axes[0, 1].tick_params(axis='x', rotation=45)
                axes[0, 1].grid(True, alpha=0.3)
            
            # 3. æ¨¡å—ä»£è¡¨ç›¸å…³æ€§çƒ­åŠ›å›¾
            if len(module_columns) > 1:
                corr_matrix = module_data[module_columns].corr()
                im = axes[1, 0].imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)
                axes[1, 0].set_title('æ¨¡å—ä»£è¡¨ç›¸å…³æ€§çƒ­åŠ›å›¾')
                axes[1, 0].set_xticks(range(len(module_columns)))
                axes[1, 0].set_yticks(range(len(module_columns)))
                axes[1, 0].set_xticklabels([f'M{col.split("_")[1]}' for col in module_columns], rotation=45)
                axes[1, 0].set_yticklabels([f'M{col.split("_")[1]}' for col in module_columns])
                plt.colorbar(im, ax=axes[1, 0])
            
            # 4. æ¨¡å—ä»£è¡¨ç»Ÿè®¡ä¿¡æ¯
            stats_text = f"æ€»æ¨¡å—æ•°: {len(module_columns)}\n"
            stats_text += f"æ€»å‚ä¸è€…æ•°: {len(module_data)}\n\n"
            
            for col in module_columns[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ¨¡å—çš„ç»Ÿè®¡
                valid_data = module_data[col].dropna()
                if len(valid_data) > 0:
                    stats_text += f"{col}:\n"
                    stats_text += f"  æœ‰æ•ˆæ ·æœ¬: {len(valid_data)}\n"
                    stats_text += f"  å‡å€¼: {valid_data.mean():.3f}\n"
                    stats_text += f"  æ ‡å‡†å·®: {valid_data.std():.3f}\n\n"
            
            axes[1, 1].text(0.1, 0.9, stats_text, fontsize=10, transform=axes[1, 1].transAxes, 
                           verticalalignment='top', fontfamily='monospace')
            axes[1, 1].set_title('æ¨¡å—ä»£è¡¨ç»Ÿè®¡ä¿¡æ¯')
            axes[1, 1].axis('off')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            viz_path = self.visualization_dir / "module_representatives_analysis.png"
            plt.savefig(viz_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"ğŸ’¾ æ¨¡å—ä»£è¡¨å¯è§†åŒ–å·²ä¿å­˜åˆ°: {viz_path}")
            
            # ä¿å­˜æ¨¡å—ä»£è¡¨ç»Ÿè®¡åˆ°CSV
            module_stats = []
            for col in module_columns:
                valid_data = module_data[col].dropna()
                if len(valid_data) > 0:
                    module_stats.append({
                        'æ¨¡å—ID': col.split('_')[1],
                        'æœ‰æ•ˆæ ·æœ¬æ•°': len(valid_data),
                        'å‡å€¼': valid_data.mean(),
                        'æ ‡å‡†å·®': valid_data.std(),
                        'æœ€å°å€¼': valid_data.min(),
                        'æœ€å¤§å€¼': valid_data.max(),
                        'ç¼ºå¤±ç‡': (len(module_data) - len(valid_data)) / len(module_data) * 100
                    })
            
            if module_stats:
                module_stats_df = pd.DataFrame(module_stats)
                module_stats_path = self.module_dir / "module_representatives_statistics.csv"
                module_stats_df.to_csv(module_stats_path, index=False)
                self.logger.info(f"ğŸ’¾ æ¨¡å—ä»£è¡¨ç»Ÿè®¡å·²ä¿å­˜åˆ°: {module_stats_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å—ä»£è¡¨å¯è§†åŒ–å¤±è´¥: {e}")
            return False

    def run_complete_analysis(self, 
                            max_missing_rate: float = 0.9,
                            min_variance: float = 1e-8,
                            n_clusters: int = 20,
                            linkage_method: str = 'ward',
                            force_recalculate_metadata: bool = False,
                            start_from_stage: int = 1) -> bool:
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„ä¸­ä»‹å˜é‡æ¨¡å—åŒ–åˆ†æ...")
            
            if start_from_stage <= 1:
                # ç¬¬ä¸€é˜¶æ®µï¼šå…ƒæ•°æ®é¢„è®¡ç®—ä¸æ™ºèƒ½æŸ¥æ‰¾è¡¨æ„å»º
                self.logger.info("\n" + "="*60)
                self.logger.info("ç¬¬ä¸€é˜¶æ®µï¼šå…ƒæ•°æ®é¢„è®¡ç®—ä¸æ™ºèƒ½æŸ¥æ‰¾è¡¨æ„å»º")
                self.logger.info("="*60)
                
                if not self.load_variable_dictionary():
                    return False
                
                if not self.build_label_to_filepath_map():
                    return False
                
                if not self.identify_core_mediators():
                    return False
                
                if not self.compute_metadata_iteratively(force_recalculate_metadata):
                    return False
                
                if not self.filter_variables(max_missing_rate, min_variance):
                    return False
            else:
                self.logger.info("â­ï¸ è·³è¿‡ç¬¬ä¸€é˜¶æ®µï¼ˆå…ƒæ•°æ®é¢„è®¡ç®—ï¼‰ï¼ŒåŠ è½½å·²æœ‰ç»“æœ...")
                # åŠ è½½å¿…è¦çš„æ•°æ®ç»“æ„
                if not self.load_existing_results_stage1():
                    return False
            
            if start_from_stage <= 2:
                # ç¬¬äºŒé˜¶æ®µï¼šè¿­ä»£å¼å¹¶è¡Œæ„å»ºç›¸å…³æ€§çŸ©é˜µ
                self.logger.info("\n" + "="*60)
                self.logger.info("ç¬¬äºŒé˜¶æ®µï¼šè¿­ä»£å¼å¹¶è¡Œæ„å»ºç›¸å…³æ€§çŸ©é˜µ")
                self.logger.info("="*60)
                
                if not self.build_correlation_matrix_parallel():
                    return False
                
                # å¯è§†åŒ–ç›¸å…³æ€§çŸ©é˜µ
                if not self.visualize_correlation_matrix():
                    self.logger.warning("âš ï¸ ç›¸å…³æ€§çŸ©é˜µå¯è§†åŒ–å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
            else:
                self.logger.info("â­ï¸ è·³è¿‡ç¬¬äºŒé˜¶æ®µï¼ˆç›¸å…³æ€§çŸ©é˜µæ„å»ºï¼‰ï¼Œæ£€æŸ¥å·²æœ‰ç»“æœ...")
                if not self.check_correlation_matrix_exists():
                    return False
            
            if start_from_stage <= 3:
                # ç¬¬ä¸‰é˜¶æ®µï¼šåœ¨ç›¸å…³æ€§çŸ©é˜µä¸Šè¿›è¡Œèšç±»
                self.logger.info("\n" + "="*60)
                self.logger.info("ç¬¬ä¸‰é˜¶æ®µï¼šåœ¨ç›¸å…³æ€§çŸ©é˜µä¸Šè¿›è¡Œèšç±»")
                self.logger.info("="*60)
                
                if not self.perform_hierarchical_clustering(linkage_method, n_clusters):
                    return False
                
                # å¯è§†åŒ–èšç±»ç»“æœ
                if not self.visualize_clustering_results():
                    self.logger.warning("âš ï¸ èšç±»ç»“æœå¯è§†åŒ–å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
            else:
                self.logger.info("â­ï¸ è·³è¿‡ç¬¬ä¸‰é˜¶æ®µï¼ˆèšç±»åˆ†æï¼‰ï¼Œæ£€æŸ¥å·²æœ‰ç»“æœ...")
                if not self.check_clustering_results_exist():
                    return False
            
            # ç¬¬å››é˜¶æ®µï¼šè¿­ä»£å¼ç”Ÿæˆæ¨¡å—ä»£è¡¨
            self.logger.info("\n" + "="*60)
            self.logger.info("ç¬¬å››é˜¶æ®µï¼šè¿­ä»£å¼ç”Ÿæˆæ¨¡å—ä»£è¡¨")
            self.logger.info("="*60)
            
            if not self.generate_module_representatives():
                return False
            
            # å¯è§†åŒ–æ¨¡å—ä»£è¡¨
            if not self.visualize_module_representatives():
                self.logger.warning("âš ï¸ æ¨¡å—ä»£è¡¨å¯è§†åŒ–å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
            
            # åˆ†æå®Œæˆ
            self.logger.info("\n" + "="*60)
            self.logger.info("ğŸ‰ åˆ†æå®Œæˆ!")
            self.logger.info("="*60)
            self.logger.info(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {self.output_dir}")
            self.logger.info("ğŸ“‹ ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
            self.logger.info(f"   - å˜é‡å…ƒæ•°æ®: {self.metadata_dir}/mediator_metadata.csv")
            self.logger.info(f"   - æœ€ç»ˆå˜é‡åˆ—è¡¨: {self.metadata_dir}/final_mediator_variables.txt")
            self.logger.info(f"   - ç›¸å…³æ€§çŸ©é˜µ: {self.correlation_dir}/correlation_matrix.npy/.csv")
            self.logger.info(f"   - ç›¸å…³æ€§ç»Ÿè®¡: {self.correlation_dir}/correlation_statistics.csv")
            self.logger.info(f"   - èšç±»æ ‘çŠ¶å›¾: {self.visualization_dir}/dendrogram_correlation.png")
            self.logger.info(f"   - èšç±»åˆ†æ: {self.visualization_dir}/clustering_analysis.png")
            self.logger.info(f"   - æ¨¡å—æ˜ å°„: {self.clustering_dir}/mediator_module_mapping_v2.csv")
            self.logger.info(f"   - æ¨¡å—ç»Ÿè®¡: {self.clustering_dir}/module_statistics.csv")
            self.logger.info(f"   - æ¨¡å—ä»£è¡¨: {self.module_dir}/module_representatives_pca_v2.parquet")
            self.logger.info(f"   - æ¨¡å—ä»£è¡¨ç»Ÿè®¡: {self.module_dir}/module_representatives_statistics.csv")
            self.logger.info(f"   - ç›¸å…³æ€§åˆ†æ: {self.visualization_dir}/correlation_matrix_analysis.png")
            self.logger.info(f"   - æ¨¡å—ä»£è¡¨åˆ†æ: {self.visualization_dir}/module_representatives_analysis.png")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å®Œæ•´åˆ†æå¤±è´¥: {e}")
            return False
    
    def load_existing_results_stage1(self) -> bool:
        """åŠ è½½ç¬¬ä¸€é˜¶æ®µçš„å·²æœ‰ç»“æœ"""
        try:
            self.logger.info("ğŸ“‹ åŠ è½½ç¬¬ä¸€é˜¶æ®µå·²æœ‰ç»“æœ...")
            
            # åŠ è½½å˜é‡å­—å…¸
            if not self.load_variable_dictionary():
                return False
            
            # æ„å»ºæ˜ å°„
            if not self.build_label_to_filepath_map():
                return False
            
            # åŠ è½½å…ƒæ•°æ®
            metadata_path = self.metadata_dir / "mediator_metadata.csv"
            if not metadata_path.exists():
                self.logger.error(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
                return False
            
            metadata_df = pd.read_csv(metadata_path)
            self.mediator_metadata = {}
            for _, row in metadata_df.iterrows():
                self.mediator_metadata[row['label']] = row.to_dict()
            
            # åŠ è½½æœ€ç»ˆå˜é‡åˆ—è¡¨
            final_vars_path = self.metadata_dir / "final_mediator_variables.txt"
            if not final_vars_path.exists():
                self.logger.error(f"âŒ æœ€ç»ˆå˜é‡åˆ—è¡¨ä¸å­˜åœ¨: {final_vars_path}")
                return False
            
            with open(final_vars_path, 'r', encoding='utf-8') as f:
                self.final_mediator_labels = [line.strip() for line in f if line.strip()]
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœï¼Œå…± {len(self.final_mediator_labels)} ä¸ªå˜é‡")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœå¤±è´¥: {e}")
            return False
    
    def check_correlation_matrix_exists(self) -> bool:
        """æ£€æŸ¥ç›¸å…³æ€§çŸ©é˜µæ˜¯å¦å­˜åœ¨"""
        try:
            corr_matrix_path = self.correlation_dir / "correlation_matrix.npy"
            if not corr_matrix_path.exists():
                self.logger.error(f"âŒ ç›¸å…³æ€§çŸ©é˜µä¸å­˜åœ¨: {corr_matrix_path}")
                return False
            
            self.logger.info(f"âœ… ç›¸å…³æ€§çŸ©é˜µå·²å­˜åœ¨: {corr_matrix_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥ç›¸å…³æ€§çŸ©é˜µå¤±è´¥: {e}")
            return False
    
    def check_clustering_results_exist(self) -> bool:
        """æ£€æŸ¥èšç±»ç»“æœæ˜¯å¦å­˜åœ¨"""
        try:
            mapping_path = self.clustering_dir / "mediator_module_mapping_v2.csv"
            if not mapping_path.exists():
                self.logger.error(f"âŒ æ¨¡å—æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {mapping_path}")
                return False
            
            self.logger.info(f"âœ… èšç±»ç»“æœå·²å­˜åœ¨: {mapping_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥èšç±»ç»“æœå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å†…å­˜é«˜æ•ˆçš„ä¸­ä»‹å˜é‡æ¨¡å—åŒ–åˆ†æ')
    parser.add_argument('--variable_dict', type=str, 
                       default='all_nhanes_variables.csv',
                       help='å˜é‡å­—å…¸æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_dir', type=str,
                       default='NHANES_PROCESSED_CSV_merged_by_prefix',
                       help='æ•°æ®æ–‡ä»¶ç›®å½•')
    parser.add_argument('--output_dir', type=str,
                       default='mediator_analysis_results_v2',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max_missing_rate', type=float,
                       default=0.9,
                       help='æœ€å¤§å…è®¸ç¼ºå¤±ç‡ (é»˜è®¤: 0.9)')
    parser.add_argument('--min_variance', type=float,
                       default=1e-8,
                       help='æœ€å°æ–¹å·®é˜ˆå€¼ (é»˜è®¤: 1e-8)')
    parser.add_argument('--n_clusters', type=int,
                       default=20,
                       help='èšç±»ç°‡æ•° (é»˜è®¤: 20)')
    parser.add_argument('--linkage_method', type=str,
                       default='ward',
                       choices=['ward', 'complete', 'average', 'single'],
                       help='èšç±»é“¾æ¥æ–¹æ³• (é»˜è®¤: ward)')
    parser.add_argument('--max_memory_gb', type=float,
                       default=100.0,
                       help='æœ€å¤§å†…å­˜ä½¿ç”¨é™åˆ¶(GB) (é»˜è®¤: 100.0)')
    parser.add_argument('--n_jobs', type=int,
                       default=-1,
                       help='å¹¶è¡Œä½œä¸šæ•° (-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ)')
    parser.add_argument('--force_recalc_metadata', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°è®¡ç®—å…ƒæ•°æ®ï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰')
    parser.add_argument('--start_from_stage', type=int,
                       default=1,
                       choices=[1, 2, 3, 4],
                       help='ä»å“ªä¸ªé˜¶æ®µå¼€å§‹è¿è¡Œ: 1=å…ƒæ•°æ®é¢„è®¡ç®—, 2=ç›¸å…³æ€§çŸ©é˜µ, 3=èšç±»åˆ†æ, 4=æ¨¡å—ä»£è¡¨ç”Ÿæˆ')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = MemoryEfficientMediatorAnalyzer(
        variable_dict_path=args.variable_dict,
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        max_memory_gb=args.max_memory_gb,
        n_jobs=args.n_jobs
    )
    
    # è¿è¡Œåˆ†æ
    success = analyzer.run_complete_analysis(
        max_missing_rate=args.max_missing_rate,
        min_variance=args.min_variance,
        n_clusters=args.n_clusters,
        linkage_method=args.linkage_method,
        force_recalculate_metadata=args.force_recalc_metadata,
        start_from_stage=args.start_from_stage
    )
    
    if success:
        print("\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆ!")
        sys.exit(0)
    else:
        print("\nâŒ åˆ†æå¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    main() 