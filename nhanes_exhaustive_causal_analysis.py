#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NHANES å…¨å˜é‡ç©·ä¸¾å› æœè·¯å¾„åˆ†æè„šæœ¬
==========================================

æ­¤è„šæœ¬å¯¹æ‰€æœ‰NHANESå˜é‡è¿›è¡Œå…¨æ’åˆ—ç»„åˆåˆ†æï¼Œç³»ç»Ÿæ€§åœ°å‘ç°ï¼š
- 3å˜é‡å› æœé“¾ï¼šA â†’ B â†’ C
- 4å˜é‡å› æœé“¾ï¼šA â†’ B â†’ C â†’ D  
- 5å˜é‡å› æœé“¾ï¼šA â†’ B â†’ C â†’ D â†’ E

ä¸é¢„è®¾å˜é‡è§’è‰²ï¼Œæ¯ä¸ªå˜é‡éƒ½å¯èƒ½ä½œä¸ºæš´éœ²ã€ä¸­ä»‹æˆ–ç»“å±€ã€‚
è¾“å‡ºæ‰€æœ‰ç»“æœï¼Œä¸è¿›è¡Œæ˜¾è‘—æ€§ç­›é€‰ã€‚

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024å¹´
"""

import pandas as pd
import numpy as np
import os
import warnings
from pathlib import Path
import statsmodels.api as sm
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import logging
from itertools import permutations, combinations
import json
from datetime import datetime
import gc

# è®¾ç½®matplotlibä¸ºè‹±æ–‡ç¯å¢ƒï¼Œé¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('nhanes_exhaustive_analysis.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é…ç½®å‚æ•°
ALPHA = 0.05  # ç»Ÿè®¡æ˜¾è‘—æ€§æ°´å¹³
DATA_DIR = "./NHANES_PROCESSED_CSV"
OUTPUT_DIR = "./output_exhaustive_analysis"
MIN_SAMPLE_SIZE = 100  # æœ€å°æ ·æœ¬é‡è¦æ±‚
MAX_CHAIN_LENGTH = 5   # æœ€å¤§å› æœé“¾é•¿åº¦

class NHANESExhaustiveAnalysis:
    """NHANESå…¨å˜é‡ç©·ä¸¾å› æœè·¯å¾„åˆ†æç±»"""
    
    def __init__(self, data_dir=DATA_DIR, output_dir=OUTPUT_DIR):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.master_df = None
        self.analysis_variables = []
        self.results = {
            '3_chain': [],
            '4_chain': [],
            '5_chain': []
        }
        self.summary_stats = {}
        
    def discover_all_merged_files(self):
        """Discover all merged data files"""
        logger.info("ğŸ” Discovering all merged data files...")
        
        merged_files = []
        for root, dirs, files in os.walk(self.data_dir):
            for file in files:
                if file.endswith('_merged.csv'):
                    file_path = Path(root) / file
                    relative_path = file_path.relative_to(self.data_dir)
                    merged_files.append(relative_path)
        
        logger.info(f"ğŸ“ Found {len(merged_files)} merged data files")
        return sorted(merged_files)
    
    def load_and_merge_data(self):
        """Load and merge all available data"""
        logger.info("="*60)
        logger.info("Starting data loading and merging...")
        logger.info("="*60)
        
        # Discover all merged files
        all_merged_files = self.discover_all_merged_files()
        
        # Find demographics file as base
        demo_file = None
        for file_path in all_merged_files:
            if 'äººå£ç»Ÿè®¡å­¦ä¿¡æ¯' in str(file_path):
                demo_file = file_path
                break
        
        if demo_file is None:
            logger.error("âŒ Demographics file not found")
            return False
        
        # Load demographics data as base
        demo_path = self.data_dir / demo_file
        logger.info(f"ğŸ“ Loading base demographics data: {demo_path}")
        
        try:
            demo_chunks = pd.read_csv(demo_path, chunksize=10000)
            self.master_df = pd.concat(demo_chunks, ignore_index=True)
            logger.info(f"âœ… Demographics data loaded: {len(self.master_df):,} rows")
        except Exception as e:
            logger.error(f"âŒ Failed to load demographics data: {e}")
            return False
        
        # Merge all other data files
        merge_keys = ['RespondentSequenceNumber', 'YearRange']
        successful_merges = 0
        failed_merges = 0
        
        for file_path in all_merged_files:
            if file_path == demo_file:
                continue
                
            full_path = self.data_dir / file_path
            dataset_name = file_path.stem.replace('_merged', '')
            
            try:
                logger.info(f"ğŸ“ Loading {dataset_name}...")
                df_chunks = pd.read_csv(full_path, chunksize=10000)
                df = pd.concat(df_chunks, ignore_index=True)
                
                before_count = len(self.master_df)
                before_cols = self.master_df.shape[1]
                
                self.master_df = pd.merge(self.master_df, df, on=merge_keys, how='left')
                
                new_cols = self.master_df.shape[1] - before_cols
                logger.info(f"âœ… {dataset_name} merged: +{new_cols} columns, {len(self.master_df):,} rows")
                successful_merges += 1
                
            except Exception as e:
                logger.error(f"âŒ Failed to load {dataset_name}: {e}")
                failed_merges += 1
                continue
        
        logger.info("="*60)
        logger.info(f"ğŸ“Š Data merging summary:")
        logger.info(f"   âœ… Successfully merged: {successful_merges} datasets")
        logger.info(f"   âŒ Failed/skipped: {failed_merges} datasets")
        logger.info(f"   ğŸ“ Final dimensions: {self.master_df.shape[0]:,} rows Ã— {self.master_df.shape[1]:,} columns")
        logger.info("="*60)
        
        return True
    
    def identify_analysis_variables(self):
        """è¯†åˆ«å¹¶é€‰æ‹©ç”¨äºåˆ†æçš„å˜é‡"""
        logger.info("ğŸ” å¼€å§‹è¯†åˆ«åˆ†æå˜é‡...")
        
        # æ’é™¤çš„åˆ—ï¼ˆIDã€æƒé‡ã€å…ƒæ•°æ®ç­‰ï¼‰
        exclude_patterns = [
            'RespondentSequenceNumber', 'YearRange', 'Weight', 'PSU', 'Stratum',
            'Interview', 'Comment', 'Status', 'Flag', 'Code', 'ID', 'Cycle',
            'Sample', 'Variance', 'Exam', 'Component'
        ]
        
        # ç­›é€‰æ•°å€¼å‹å˜é‡
        numeric_columns = []
        for col in self.master_df.columns:
            # è·³è¿‡æ’é™¤æ¨¡å¼
            if any(pattern in col for pattern in exclude_patterns):
                continue
            
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼å‹
            try:
                numeric_vals = pd.to_numeric(self.master_df[col], errors='coerce')
                non_null_count = numeric_vals.notna().sum()
                
                # è¦æ±‚è‡³å°‘æœ‰1000ä¸ªéç©ºå€¼
                if non_null_count >= 1000:
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å˜å¼‚æ€§
                    unique_count = numeric_vals.nunique()
                    if unique_count >= 3:  # è‡³å°‘3ä¸ªä¸åŒå€¼
                        numeric_columns.append(col)
                        
            except:
                continue
        
        self.analysis_variables = numeric_columns
        
        # Categorize variables by type
        self.variable_categories = {
            'Demographics': [],
            'Dietary_Nutrition': [],
            'Lifestyle': [],
            'Disease_History': [],
            'Physical_Exam': [],
            'Laboratory': [],
            'Others': []
        }
        
        for var in self.analysis_variables:
            if any(keyword in var for keyword in ['Age', 'Gender', 'Race', 'Education', 'Income', 'Marital', 'Birth']):
                self.variable_categories['Demographics'].append(var)
            elif any(keyword in var for keyword in ['Energy', 'Protein', 'Fat', 'Fiber', 'Sugar', 'Sodium', 'Vitamin', 'Mineral', 'Dietary', 'Food']):
                self.variable_categories['Dietary_Nutrition'].append(var)
            elif any(keyword in var for keyword in ['Smoke', 'Alcohol', 'Physical', 'Activity', 'Exercise', 'Sleep']):
                self.variable_categories['Lifestyle'].append(var)
            elif any(keyword in var for keyword in ['diabetes', 'hypertension', 'Disease', 'told', 'Doctor', 'Medical']):
                self.variable_categories['Disease_History'].append(var)
            elif any(keyword in var for keyword in ['BMI', 'Weight', 'Height', 'Circumference', 'Blood Pressure', 'Body']):
                self.variable_categories['Physical_Exam'].append(var)
            elif any(keyword in var for keyword in ['Cholesterol', 'Glucose', 'Albumin', 'Creatinine', 'Hemoglobin', 'Laboratory', 'Urine', 'Blood']):
                self.variable_categories['Laboratory'].append(var)
            else:
                self.variable_categories['Others'].append(var)
        
        logger.info(f"ğŸ“‹ Variable categorization:")
        for category, vars_list in self.variable_categories.items():
            if vars_list:
                logger.info(f"   {category}: {len(vars_list)} variables")
        
        logger.info(f"ğŸ¯ Total identified {len(self.analysis_variables)} analysis variables")
        
        # ä¿å­˜å˜é‡åˆ—è¡¨
        var_info = {
            'total_variables': len(self.analysis_variables),
            'categories': {k: len(v) for k, v in self.variable_categories.items()},
            'variable_list': self.analysis_variables,
            'detailed_categories': self.variable_categories
        }
        
        with open(self.output_dir / 'analysis_variables.json', 'w', encoding='utf-8') as f:
            json.dump(var_info, f, ensure_ascii=False, indent=2)
        
        return len(self.analysis_variables) > 0
    
    def clean_and_prepare_data(self):
        """Data cleaning and preprocessing"""
        logger.info("ğŸ§¹ Starting data cleaning and preprocessing...")
        
        # Keep only analysis variables and merge keys
        keep_columns = self.analysis_variables + ['RespondentSequenceNumber', 'YearRange']
        self.master_df = self.master_df[keep_columns]
        
        logger.info(f"ğŸ“Š Data dimensions after cleaning: {self.master_df.shape}")
        
        # Numeric conversion
        for var in self.analysis_variables:
            if var in self.master_df.columns:
                # Convert to numeric type
                self.master_df[var] = pd.to_numeric(self.master_df[var], errors='coerce')
                
                # Handle NHANES coded missing values
                missing_codes = [7, 9, 77, 99, 777, 999, 7777, 9999]
                for code in missing_codes:
                    self.master_df.loc[self.master_df[var] == code, var] = np.nan
        
        # Calculate missing value statistics for each variable
        missing_stats = {}
        for var in self.analysis_variables:
            total = len(self.master_df)
            missing = self.master_df[var].isna().sum()
            missing_rate = missing / total * 100
            missing_stats[var] = {
                'total': total,
                'missing': missing,
                'available': total - missing,
                'missing_rate': missing_rate
            }
        
        # Save missing value statistics
        missing_df = pd.DataFrame(missing_stats).T
        missing_df.to_csv(self.output_dir / 'missing_value_statistics.csv', encoding='utf-8-sig')
        
        logger.info(f"ğŸ“ˆ Data quality statistics saved to: missing_value_statistics.csv")
        logger.info("âœ… Data cleaning completed")
        
        return True
    
    def perform_regression_test(self, df, outcome_var, predictor_vars):
        """æ‰§è¡Œå›å½’åˆ†æå¹¶è¿”å›è¯¦ç»†ç»“æœ"""
        try:
            # å‡†å¤‡æ•°æ®
            y = df[outcome_var].dropna()
            X = df[predictor_vars].dropna()
            
            # ç¡®ä¿Xå’Œyçš„ç´¢å¼•åŒ¹é…
            common_idx = y.index.intersection(X.index)
            y = y.loc[common_idx]
            X = X.loc[common_idx]
            
            if len(y) < MIN_SAMPLE_SIZE:
                return None
            
            # æ·»åŠ å¸¸æ•°é¡¹
            X = sm.add_constant(X)
            
            # æ£€æŸ¥ç»“å±€å˜é‡ç±»å‹
            unique_vals = y.nunique()
            if unique_vals == 2:
                # äºŒåˆ†ç±»å˜é‡ï¼Œä½¿ç”¨é€»è¾‘å›å½’
                model = sm.Logit(y, X)
            else:
                # è¿ç»­å˜é‡ï¼Œä½¿ç”¨çº¿æ€§å›å½’
                model = sm.OLS(y, X)
            
            # æ‹Ÿåˆæ¨¡å‹
            try:
                result = model.fit(disp=0)
                
                # è¿”å›ç¬¬ä¸€ä¸ªé¢„æµ‹å˜é‡çš„ç»“æœ
                if len(result.pvalues) > 1:
                    return {
                        'sample_size': len(y),
                        'p_value': result.pvalues.iloc[1],
                        'coefficient': result.params.iloc[1] if hasattr(result, 'params') else np.nan,
                        'std_error': result.bse.iloc[1] if hasattr(result, 'bse') else np.nan,
                        'r_squared': getattr(result, 'rsquared', np.nan),
                        'model_type': 'logistic' if unique_vals == 2 else 'linear'
                    }
                else:
                    return None
            except:
                return None
                
        except Exception as e:
            return None
    
    def analyze_chain_length(self, chain_length):
        """åˆ†ææŒ‡å®šé•¿åº¦çš„å› æœé“¾"""
        logger.info(f"ğŸ”— Starting analysis of {chain_length}-variable causal chains...")
        
        chain_results = []
        total_combinations = 0
        processed_combinations = 0
        
        # è®¡ç®—æ€»ç»„åˆæ•°ç”¨äºè¿›åº¦æ˜¾ç¤º
        if chain_length <= len(self.analysis_variables):
            from math import factorial
            total_combinations = factorial(len(self.analysis_variables)) // factorial(len(self.analysis_variables) - chain_length)
            logger.info(f"ğŸ“Š æ€»å…±éœ€è¦æµ‹è¯• {total_combinations:,} ä¸ª {chain_length}-å˜é‡ç»„åˆ")
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å˜é‡æ’åˆ—
        for var_combination in permutations(self.analysis_variables, chain_length):
            processed_combinations += 1
            
            if processed_combinations % 10000 == 0:
                progress = (processed_combinations / total_combinations * 100) if total_combinations > 0 else 0
                logger.info(f"ğŸ“ˆ è¿›åº¦: {processed_combinations:,}/{total_combinations:,} ({progress:.1f}%)")
            
            # åˆ›å»ºåˆ†ææ•°æ®é›†
            analysis_df = self.master_df[list(var_combination)].dropna()
            
            if len(analysis_df) < MIN_SAMPLE_SIZE:
                continue
            
            # æµ‹è¯•æ‰€æœ‰ç›¸é‚»å˜é‡å¯¹çš„å…³è”
            chain_valid = True
            chain_stats = []
            
            for i in range(chain_length - 1):
                predictor = var_combination[i]
                outcome = var_combination[i + 1]
                
                # æ‰§è¡Œå›å½’åˆ†æ
                reg_result = self.perform_regression_test(
                    analysis_df, outcome, [predictor]
                )
                
                if reg_result is None:
                    chain_valid = False
                    break
                
                chain_stats.append({
                    'step': i + 1,
                    'predictor': predictor,
                    'outcome': outcome,
                    'sample_size': reg_result['sample_size'],
                    'p_value': reg_result['p_value'],
                    'coefficient': reg_result['coefficient'],
                    'std_error': reg_result['std_error'],
                    'r_squared': reg_result['r_squared'],
                    'model_type': reg_result['model_type']
                })
            
            if chain_valid:
                # è®¡ç®—æ•´ä¸ªé“¾çš„ç»¼åˆç»Ÿè®¡
                all_p_values = [stat['p_value'] for stat in chain_stats]
                significant_links = sum(1 for p in all_p_values if p < ALPHA)
                
                chain_result = {
                    'chain_variables': list(var_combination),
                    'chain_length': chain_length,
                    'sample_size': min(stat['sample_size'] for stat in chain_stats),
                    'chain_stats': chain_stats,
                    'all_p_values': all_p_values,
                    'significant_links': significant_links,
                    'all_significant': significant_links == (chain_length - 1),
                    'geometric_mean_p': np.exp(np.mean(np.log(all_p_values))),
                    'min_p_value': min(all_p_values),
                    'max_p_value': max(all_p_values),
                    'chain_pathway': ' â†’ '.join(var_combination)
                }
                
                chain_results.append(chain_result)
        
        logger.info(f"âœ… {chain_length}-variable chain analysis completed: found {len(chain_results):,} valid chains")
        return chain_results
    
    def run_exhaustive_analysis(self):
        """Run exhaustive causal analysis"""
        logger.info("ğŸš€ Starting exhaustive causal pathway analysis...")
        
        # Limit variable count to prevent computational explosion
        if len(self.analysis_variables) > 50:
            logger.warning(f"âš ï¸  Too many variables ({len(self.analysis_variables)}), randomly selecting 50 for analysis")
            import random
            random.seed(42)
            self.analysis_variables = random.sample(self.analysis_variables, 50)
        
        # Analyze different chain lengths
        for chain_length in range(3, MAX_CHAIN_LENGTH + 1):
            try:
                results = self.analyze_chain_length(chain_length)
                self.results[f'{chain_length}_chain'] = results
                
                # Save results immediately (prevent memory overflow)
                self.save_chain_results(chain_length, results)
                
                # Clear memory
                gc.collect()
                
            except Exception as e:
                logger.error(f"âŒ Error analyzing {chain_length}-variable chains: {e}")
                continue
        
        logger.info("ğŸ‰ Exhaustive analysis completed!")
        return True
    
    def save_chain_results(self, chain_length, results):
        """ä¿å­˜æŒ‡å®šé•¿åº¦é“¾çš„ç»“æœ"""
        if not results:
            logger.info(f"âš ï¸  {chain_length}-å˜é‡é“¾æ— ç»“æœå¯ä¿å­˜")
            return
        
        # è½¬æ¢ä¸ºå¹³é¢DataFrameæ ¼å¼
        flat_results = []
        for result in results:
            base_info = {
                'chain_pathway': result['chain_pathway'],
                'chain_length': result['chain_length'],
                'sample_size': result['sample_size'],
                'significant_links': result['significant_links'],
                'all_significant': result['all_significant'],
                'geometric_mean_p': result['geometric_mean_p'],
                'min_p_value': result['min_p_value'],
                'max_p_value': result['max_p_value']
            }
            
            # æ·»åŠ æ¯ä¸€æ­¥çš„è¯¦ç»†ä¿¡æ¯
            for i, step_stat in enumerate(result['chain_stats']):
                step_info = base_info.copy()
                step_info.update({
                    'step_number': step_stat['step'],
                    'step_predictor': step_stat['predictor'],
                    'step_outcome': step_stat['outcome'],
                    'step_p_value': step_stat['p_value'],
                    'step_coefficient': step_stat['coefficient'],
                    'step_std_error': step_stat['std_error'],
                    'step_r_squared': step_stat['r_squared'],
                    'step_model_type': step_stat['model_type']
                })
                flat_results.append(step_info)
        
        # ä¿å­˜ä¸ºCSV
        results_df = pd.DataFrame(flat_results)
        
        # æŒ‰æ˜¾è‘—æ€§å’Œpå€¼æ’åº
        results_df = results_df.sort_values([
            'all_significant', 'significant_links', 'geometric_mean_p'
        ], ascending=[False, False, True])
        
        output_file = self.output_dir / f'{chain_length}_variable_chains_complete_results.csv'
        results_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        logger.info(f"ğŸ’¾ {chain_length}-å˜é‡é“¾ç»“æœå·²ä¿å­˜: {output_file}")
        logger.info(f"   ğŸ“Š æ€»è®¡: {len(results):,} ä¸ªé“¾ï¼Œ{len(flat_results):,} ä¸ªé“¾æ­¥")
        
        # ä¿å­˜æ˜¾è‘—æ€§æ±‡æ€»
        summary = {
            'total_chains': len(results),
            'all_significant_chains': sum(1 for r in results if r['all_significant']),
            'partially_significant_chains': sum(1 for r in results if r['significant_links'] > 0 and not r['all_significant']),
            'non_significant_chains': sum(1 for r in results if r['significant_links'] == 0),
            'average_sample_size': np.mean([r['sample_size'] for r in results]),
            'min_sample_size': min(r['sample_size'] for r in results),
            'max_sample_size': max(r['sample_size'] for r in results)
        }
        
        summary_file = self.output_dir / f'{chain_length}_variable_chains_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ æ±‡æ€»ç»Ÿè®¡:")
        logger.info(f"   ğŸ¯ å®Œå…¨æ˜¾è‘—é“¾: {summary['all_significant_chains']:,}")
        logger.info(f"   âš¡ éƒ¨åˆ†æ˜¾è‘—é“¾: {summary['partially_significant_chains']:,}")
        logger.info(f"   âŒ éæ˜¾è‘—é“¾: {summary['non_significant_chains']:,}")
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢çš„åˆ†ææŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆå…¨é¢åˆ†ææŠ¥å‘Š...")
        
        report_lines = []
        report_lines.append("=" * 100)
        report_lines.append("NHANES å…¨å˜é‡ç©·ä¸¾å› æœè·¯å¾„åˆ†ææŠ¥å‘Š")
        report_lines.append("=" * 100)
        report_lines.append(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"æ•°æ®é›†è§„æ¨¡: {self.master_df.shape[0]:,} è¡Œ Ã— {len(self.analysis_variables)} ä¸ªåˆ†æå˜é‡")
        report_lines.append(f"æœ€å°æ ·æœ¬é‡è¦æ±‚: {MIN_SAMPLE_SIZE:,}")
        report_lines.append(f"æ˜¾è‘—æ€§æ°´å¹³: Î± = {ALPHA}")
        report_lines.append("")
        
        # æ€»ç»“æ¯ç§é“¾é•¿åº¦çš„ç»“æœ
        total_chains = 0
        total_significant = 0
        
        for chain_length in range(3, MAX_CHAIN_LENGTH + 1):
            results = self.results.get(f'{chain_length}_chain', [])
            if results:
                significant_count = sum(1 for r in results if r['all_significant'])
                total_chains += len(results)
                total_significant += significant_count
                
                report_lines.append(f"{chain_length}-å˜é‡å› æœé“¾åˆ†æç»“æœ:")
                report_lines.append(f"  ğŸ“Š æ€»å‘ç°é“¾æ•°: {len(results):,}")
                report_lines.append(f"  ğŸ¯ å®Œå…¨æ˜¾è‘—é“¾: {significant_count:,} ({significant_count/len(results)*100:.1f}%)")
                report_lines.append(f"  ğŸ“ˆ éƒ¨åˆ†æ˜¾è‘—é“¾: {sum(1 for r in results if 0 < r['significant_links'] < chain_length-1):,}")
                report_lines.append(f"  âŒ éæ˜¾è‘—é“¾: {sum(1 for r in results if r['significant_links'] == 0):,}")
                report_lines.append("")
        
        report_lines.append("=" * 50)
        report_lines.append("æ€»ä½“ç»Ÿè®¡æ±‡æ€»")
        report_lines.append("=" * 50)
        report_lines.append(f"ğŸ”¢ æ€»è®¡å‘ç°å› æœé“¾: {total_chains:,}")
        report_lines.append(f"â­ æ€»è®¡æ˜¾è‘—å› æœé“¾: {total_significant:,}")
        report_lines.append(f"ğŸ“Š æ˜¾è‘—ç‡: {total_significant/total_chains*100:.2f}%" if total_chains > 0 else "ğŸ“Š æ˜¾è‘—ç‡: 0%")
        report_lines.append("")
        
        # å˜é‡åˆ†ç±»ç»Ÿè®¡
        report_lines.append("ğŸ“‹ åˆ†æå˜é‡åˆ†ç±»ç»Ÿè®¡:")
        for category, vars_list in self.variable_categories.items():
            if vars_list:
                report_lines.append(f"  {category}: {len(vars_list)} ä¸ªå˜é‡")
        report_lines.append("")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / 'comprehensive_analysis_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°
        print('\n'.join(report_lines))
        
        logger.info(f"ğŸ“„ å…¨é¢æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def create_summary_visualizations(self):
        """Create summary visualizations"""
        logger.info("ğŸ“ˆ Creating summary visualizations...")
        
        # Create plot directory
        plot_dir = self.output_dir / 'summary_plots'
        plot_dir.mkdir(exist_ok=True)
        
        # 1. Chain length distribution plot
        chain_lengths = []
        total_counts = []
        significant_counts = []
        
        for chain_length in range(3, MAX_CHAIN_LENGTH + 1):
            results = self.results.get(f'{chain_length}_chain', [])
            if results:
                chain_lengths.append(f'{chain_length}-Variable Chain')
                total_counts.append(len(results))
                significant_counts.append(sum(1 for r in results if r['all_significant']))
        
        if chain_lengths:
            fig, ax = plt.subplots(figsize=(12, 8))
            x = np.arange(len(chain_lengths))
            width = 0.35
            
            ax.bar(x - width/2, total_counts, width, label='Total Chains', alpha=0.8)
            ax.bar(x + width/2, significant_counts, width, label='Significant Chains', alpha=0.8)
            
            ax.set_xlabel('Causal Chain Length')
            ax.set_ylabel('Number of Chains')
            ax.set_title('Distribution of Causal Chains by Length')
            ax.set_xticks(x)
            ax.set_xticklabels(chain_lengths)
            ax.legend()
            
            # Add value labels
            for i, (total, sig) in enumerate(zip(total_counts, significant_counts)):
                ax.text(i - width/2, total + max(total_counts)*0.01, f'{total:,}', 
                       ha='center', va='bottom')
                ax.text(i + width/2, sig + max(total_counts)*0.01, f'{sig:,}', 
                       ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(plot_dir / 'chain_length_distribution.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ğŸ“Š Chain length distribution plot saved")
        
        # 2. Variable category participation analysis
        category_participation = {cat: 0 for cat in self.variable_categories.keys()}
        
        for chain_length in range(3, MAX_CHAIN_LENGTH + 1):
            results = self.results.get(f'{chain_length}_chain', [])
            for result in results:
                if result['all_significant']:  # Only count significant chains
                    for var in result['chain_variables']:
                        for cat, vars_list in self.variable_categories.items():
                            if var in vars_list:
                                category_participation[cat] += 1
                                break
        
        if any(category_participation.values()):
            fig, ax = plt.subplots(figsize=(12, 6))
            categories = list(category_participation.keys())
            counts = list(category_participation.values())
            
            bars = ax.bar(categories, counts)
            ax.set_xlabel('Variable Category')
            ax.set_ylabel('Occurrences in Significant Causal Chains')
            ax.set_title('Variable Category Participation in Significant Causal Chains')
            
            # Add value labels
            for bar, count in zip(bars, counts):
                if count > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01, 
                           f'{count:,}', ha='center', va='bottom')
            
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(plot_dir / 'category_participation.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ğŸ“Š Variable category participation plot saved")
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„ç©·ä¸¾åˆ†ææµç¨‹"""
        start_time = datetime.now()
        logger.info("ğŸš€ Starting NHANES exhaustive causal pathway analysis...")
        logger.info(f"â° Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # Step 1: Load and merge data
            if not self.load_and_merge_data():
                logger.error("âŒ Data loading failed, analysis terminated")
                return False
            
            # Step 2: Identify analysis variables
            if not self.identify_analysis_variables():
                logger.error("âŒ Variable identification failed, analysis terminated")
                return False
            
            # Step 3: Data cleaning and preprocessing
            if not self.clean_and_prepare_data():
                logger.error("âŒ Data preprocessing failed, analysis terminated")
                return False
            
            # Step 4: Run exhaustive analysis
            if not self.run_exhaustive_analysis():
                logger.error("âŒ Exhaustive analysis failed, analysis terminated")
                return False
            
            # Step 5: Generate reports and visualizations
            self.generate_comprehensive_report()
            self.create_summary_visualizations()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("ğŸ‰ Exhaustive variable analysis completed!")
            logger.info(f"â±ï¸  Total duration: {duration}")
            logger.info(f"ğŸ“ All results saved to: {self.output_dir}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Critical error during analysis: {e}")
            return False

def main():
    """Main function"""
    print("ğŸ”¬ NHANES Exhaustive Causal Pathway Analysis")
    print("=" * 80)
    print("âš ï¸  WARNING: This analysis tests ALL possible variable combinations - extremely computationally intensive!")
    print("ğŸ’¡ Recommended to run in high-performance computing environment")
    print("=" * 80)
    
    # Check data directory
    if not Path(DATA_DIR).exists():
        print(f"âŒ Error: Data directory does not exist: {DATA_DIR}")
        return
    
    # Create analysis instance and run
    analyzer = NHANESExhaustiveAnalysis()
    success = analyzer.run_complete_analysis()
    
    if success:
        print("\nâœ… Exhaustive analysis completed successfully!")
        print(f"ğŸ“ Results saved to: {analyzer.output_dir}")
        print("ğŸ“Š Please review the detailed CSV files, JSON statistics, and plots")
        print("ğŸ” Recommend using Excel or data analysis tools for further exploration")
    else:
        print("\nâŒ Errors occurred during analysis, please check log files")

if __name__ == "__main__":
    main() 