#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NHANES å› æœæ¨æ–­è‡ªåŠ¨åŒ–åˆ†æç³»ç»Ÿ
åŸºäºæš´éœ²â†’ä¸­ä»‹â†’ç»“å±€çš„å› æœé“¾è¿›è¡Œç³»ç»Ÿæ€§åˆ†æ

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
"""

import pandas as pd
import numpy as np
import os
import json
import logging
import re
import shutil
from datetime import datetime
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class NHANESCausalAnalyzer:
    def __init__(self, base_data_path="Nhanes_processed_csv_merged", 
                 variables_file="all_nhanes_variables.csv",
                 results_base_dir="causal_analysis_results"):
        """
        åˆå§‹åŒ–NHANESå› æœåˆ†æå™¨
        
        å‚æ•°:
        - base_data_path: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        - variables_file: å˜é‡ç´¢å¼•æ–‡ä»¶
        - results_base_dir: ç»“æœè¾“å‡ºç›®å½•
        """
        self.base_data_path = Path(base_data_path)
        self.variables_file = variables_file
        self.results_base_dir = Path(results_base_dir)
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_base_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæˆåŠŸå›¾è¡¨æ”¶é›†ç›®å½•
        self.successful_charts_dir = self.results_base_dir / "successful_visualizations"
        self.successful_charts_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        self.setup_logging()
        
        # åŠ è½½å˜é‡ä¿¡æ¯
        self.load_variables_info()
        
        # åŠ è½½åŸºå‡†æ•°æ®çš„RespondentSequenceNumber
        self.load_base_respondents()
        
        # åˆå§‹åŒ–ç»“æœè®°å½•
        self.analysis_results = []
        self.current_analysis_id = 0
        self.successful_analyses = 0
        self.failed_analyses = 0
        self.skipped_ab_invalid = 0  # å› A-Bæ— é‡åˆæˆ–æ— å˜å¼‚æ€§è€Œè·³è¿‡çš„åˆ†ææ•°
        self.skipped_data_invalid = 0  # å› å˜é‡æ•°æ®æ— æ•ˆè€Œè·³è¿‡çš„åˆ†ææ•°
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = self.results_base_dir / "logs"
        log_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®ä¸»æ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"nhanes_analysis_{timestamp}.log"
        
        # é…ç½®æ—¥å¿—æ ¼å¼
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.log_file = log_file
        
        self.logger.info("=" * 60)
        self.logger.info("NHANESå› æœæ¨æ–­è‡ªåŠ¨åŒ–åˆ†æç³»ç»Ÿå¯åŠ¨")
        self.logger.info("=" * 60)
        
    def clean_variable_name(self, name):
        """æ¸…ç†å˜é‡åï¼Œç”¨äºæ–‡ä»¶å¤¹å‘½å"""
        if pd.isna(name) or name is None:
            return "Unknown"
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
        clean_name = str(name)
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        clean_name = re.sub(r'[<>:"/\\|?*]', '_', clean_name)
        clean_name = re.sub(r'\s+', '_', clean_name)
        # é™åˆ¶é•¿åº¦
        if len(clean_name) > 50:
            clean_name = clean_name[:50]
        return clean_name
        
    def is_valid_analysis_variable(self, variable_name):
        """åˆ¤æ–­å˜é‡æ˜¯å¦é€‚åˆç”¨äºåˆ†æ"""
        if pd.isna(variable_name) or variable_name is None:
            return False
            
        variable_name = str(variable_name).lower()
        
        # æ’é™¤ä¸é€‚åˆåˆ†æçš„å˜é‡
        excluded_patterns = [
            'respondentsequencenumber',
            'sequence',
            'id',
            'weight',
            'comment',
            'status',
            'flag',
            'code',
            'cycle',
            'sample',
            'variance',
            'exam',
            'component',
            'yearrange',
            'interview',
            'stratum',
            'psu'
        ]
        
        for pattern in excluded_patterns:
            if pattern in variable_name:
                return False
                
        return True
        
    def load_variables_info(self):
        """åŠ è½½å˜é‡ä¿¡æ¯è¡¨"""
        self.logger.info("æ­£åœ¨åŠ è½½å˜é‡ä¿¡æ¯...")
        self.variables_df = pd.read_csv(self.variables_file)
        
        # æŒ‰variable_indexæ’åº
        self.variables_df = self.variables_df.sort_values('variable_index').reset_index(drop=True)
        
        # è¿‡æ»¤æœ‰æ•ˆçš„åˆ†æå˜é‡
        valid_mask = self.variables_df['variable_name'].apply(self.is_valid_analysis_variable)
        self.variables_df = self.variables_df[valid_mask].copy()
        
        # è·å–å„ç±»åˆ«å˜é‡
        self.exposure_vars = self.variables_df[self.variables_df['category'] == 'A'].copy()
        self.mediator_vars = self.variables_df[self.variables_df['category'].isin(['B', 'B&C'])].copy()
        self.outcome_vars = self.variables_df[self.variables_df['category'].isin(['C', 'B&C'])].copy()
        
        self.logger.info(f"æ‰¾åˆ°æœ‰æ•ˆå˜é‡æ€»æ•°: {len(self.variables_df)}")
        self.logger.info(f"æš´éœ²å˜é‡(A): {len(self.exposure_vars)}")
        self.logger.info(f"ä¸­ä»‹å˜é‡(B/B&C): {len(self.mediator_vars)}")
        self.logger.info(f"ç»“å±€å˜é‡(C/B&C): {len(self.outcome_vars)}")
        
        if len(self.exposure_vars) == 0 or len(self.mediator_vars) == 0 or len(self.outcome_vars) == 0:
            self.logger.error("é”™è¯¯: æŸäº›ç±»åˆ«çš„å˜é‡æ•°é‡ä¸º0ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            raise ValueError("å˜é‡ç±»åˆ«ä¸å®Œæ•´")
        
    def load_base_respondents(self):
        """åŠ è½½åŸºå‡†æ•°æ®çš„RespondentSequenceNumber"""
        base_file = self.base_data_path / "0_äººå£ç»Ÿè®¡å­¦ä¿¡æ¯_merged.csv"
        self.logger.info(f"æ­£åœ¨åŠ è½½åŸºå‡†æ•°æ®: {base_file}")
        
        try:
            # è¯»å–å®Œæ•´çš„åŸºå‡†æ•°æ®RespondentSequenceNumberåˆ—
            self.logger.info("è¯»å–å®Œæ•´åŸºå‡†æ•°æ®ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
            base_df = pd.read_csv(base_file, usecols=['RespondentSequenceNumber'])
            
            # å»é™¤ç¼ºå¤±å€¼å¹¶è½¬æ¢ä¸ºé›†åˆ
            respondent_ids = base_df['RespondentSequenceNumber'].dropna().tolist()
            self.base_respondents = set(respondent_ids)
            
            # è®°å½•RespondentSequenceNumberçš„èŒƒå›´
            min_id = min(self.base_respondents)
            max_id = max(self.base_respondents)
            
            self.logger.info(f"åŸºå‡†æ ·æœ¬æ•°é‡: {len(self.base_respondents)}")
            self.logger.info(f"RespondentSequenceNumberèŒƒå›´: {min_id} - {max_id}")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½åŸºå‡†æ•°æ®å¤±è´¥: {e}")
            raise
        
    def load_variable_data(self, file_name, variable_name, filter_base=True):
        """
        åŠ è½½æŒ‡å®šå˜é‡çš„æ•°æ®ï¼Œå¹¶è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
        
        å‚æ•°:
        - file_name: æ–‡ä»¶å
        - variable_name: å˜é‡å
        - filter_base: æ˜¯å¦ä½¿ç”¨åŸºå‡†æ ·æœ¬è¿‡æ»¤
        
        è¿”å›:
        - DataFrameåŒ…å«RespondentSequenceNumberå’Œå˜é‡æ•°æ®
        """
        file_path = self.base_data_path / file_name
        
        if not file_path.exists():
            self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
            
        try:
            # åªåŠ è½½éœ€è¦çš„åˆ—
            columns_to_load = ['RespondentSequenceNumber', variable_name]
            df = pd.read_csv(file_path, usecols=columns_to_load)
            
            self.logger.debug(f"ä» {file_name} åŠ è½½å˜é‡ {variable_name}ï¼ŒåŸå§‹æ•°æ®é‡: {len(df)}")
            
            # æ­¥éª¤1: å»é™¤ç¼ºå¤±å€¼
            df = df.dropna()
            if len(df) == 0:
                return None
            
            # æ­¥éª¤2: æ£€æŸ¥å¹¶å¤„ç†é‡å¤çš„RespondentSequenceNumber
            duplicates_count = df['RespondentSequenceNumber'].duplicated().sum()
            if duplicates_count > 0:
                df = df.drop_duplicates(subset=['RespondentSequenceNumber'], keep='first')
            
            # æ­¥éª¤3: æŒ‰RespondentSequenceNumberæ’åº
            df = df.sort_values('RespondentSequenceNumber').reset_index(drop=True)
            
            # å¯é€‰çš„åŸºå‡†æ ·æœ¬è¿‡æ»¤
            if filter_base and hasattr(self, 'base_respondents'):
                original_len = len(df)
                df = df[df['RespondentSequenceNumber'].isin(self.base_respondents)]
                self.logger.debug(f"åŸºå‡†æ ·æœ¬è¿‡æ»¤åæ•°æ®é‡: {len(df)} (åŸå§‹: {original_len})")
            
            # æ­¥éª¤4: æ£€æŸ¥å˜é‡æ•°æ®è´¨é‡
            variable_data = df[variable_name]
            
            # å¤„ç†åŒ…å«é•¿å­—ç¬¦ä¸²çš„æ•°æ®ï¼ˆå¯èƒ½æ˜¯æ•°æ®é”™è¯¯ï¼‰
            if variable_data.dtype == 'object':
                # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é•¿çš„å­—ç¬¦ä¸²
                max_length = variable_data.astype(str).str.len().max()
                if max_length > 100:  # å¦‚æœæœ‰è¶…è¿‡100å­—ç¬¦çš„å­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯æ•°æ®é”™è¯¯
                    self.logger.warning(f"å˜é‡ {variable_name} åŒ…å«å¼‚å¸¸é•¿çš„å­—ç¬¦ä¸²æ•°æ®ï¼Œæœ€å¤§é•¿åº¦: {max_length}")
                    # è¿‡æ»¤æ‰å¼‚å¸¸é•¿çš„å­—ç¬¦ä¸²
                    mask = variable_data.astype(str).str.len() <= 100
                    df = df[mask]
                    self.logger.debug(f"è¿‡æ»¤å¼‚å¸¸æ•°æ®åæ•°æ®é‡: {len(df)}")
            
            # ç®€åŒ–æ—¥å¿—ï¼šåªè®°å½•åŸºæœ¬ä¿¡æ¯
            if len(df) == 0:
                return None
            
            return df
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™ {file_path}: {e}")
            return None
    
    def determine_variable_type(self, data):
        """
        åˆ¤æ–­å˜é‡ç±»å‹ï¼šæ•°å€¼å‹æˆ–åˆ†ç±»å‹
        
        å‚æ•°:
        - data: pandas Series
        
        è¿”å›:
        - 'numerical' æˆ– 'categorical'
        """
        # æ£€æŸ¥æ•°æ®ç±»å‹
        if pd.api.types.is_numeric_dtype(data):
            # æ•°å€¼å‹ï¼Œä½†æ£€æŸ¥æ˜¯å¦ä¸ºç±»åˆ«ç¼–ç 
            unique_values = data.nunique()
            if unique_values <= 10 and data.min() >= 0 and data.max() <= 10:
                # å¯èƒ½æ˜¯ç¼–ç çš„åˆ†ç±»å˜é‡
                return 'categorical'
            else:
                return 'numerical'
        else:
            return 'categorical'
    
    def preprocess_variable(self, data, var_type):
        """
        é¢„å¤„ç†å˜é‡æ•°æ®
        
        å‚æ•°:
        - data: pandas Series
        - var_type: 'numerical' æˆ– 'categorical'
        
        è¿”å›:
        - é¢„å¤„ç†åçš„æ•°æ®
        """
        try:
            if var_type == 'numerical':
                # æ•°å€¼å‹å˜é‡ï¼šæ ‡å‡†åŒ–
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ— ç©·å¤§æˆ–éå¸¸å¤§çš„å€¼
                finite_mask = np.isfinite(data.astype(float))
                if not finite_mask.all():
                    self.logger.warning(f"æ•°å€¼å˜é‡åŒ…å«éæœ‰é™å€¼ï¼Œè¿‡æ»¤å‰æ•°é‡: {len(data)}, è¿‡æ»¤å: {finite_mask.sum()}")
                    data = data[finite_mask]
                
                scaler = StandardScaler()
                processed_data = scaler.fit_transform(data.values.reshape(-1, 1)).flatten()
                self.logger.debug(f"æ•°å€¼å˜é‡æ ‡å‡†åŒ–å®Œæˆï¼Œå‡å€¼: {processed_data.mean():.4f}, æ ‡å‡†å·®: {processed_data.std():.4f}")
                return processed_data
            else:
                # åˆ†ç±»å‹å˜é‡ï¼šæ ‡ç­¾ç¼–ç 
                # æ£€æŸ¥ç±»åˆ«æ•°é‡
                unique_values = data.nunique()
                if unique_values == 1:
                    self.logger.warning(f"åˆ†ç±»å˜é‡åªæœ‰1ä¸ªç±»åˆ«: {data.unique()}")
                    return None  # è¿”å›Noneè¡¨ç¤ºä¸é€‚åˆåˆ†æ
                elif unique_values > 20:
                    self.logger.warning(f"åˆ†ç±»å˜é‡ç±»åˆ«è¿‡å¤š: {unique_values}ï¼Œå¯èƒ½ä¸é€‚åˆåˆ†æ")
                
                le = LabelEncoder()
                processed_data = le.fit_transform(data.astype(str))
                self.logger.debug(f"åˆ†ç±»å˜é‡ç¼–ç å®Œæˆï¼Œç±»åˆ«æ•°: {unique_values}, ç¼–ç èŒƒå›´: {processed_data.min()}-{processed_data.max()}")
                return processed_data
                
        except Exception as e:
            self.logger.error(f"å˜é‡é¢„å¤„ç†å‡ºé”™: {e}")
            return None
    
    def calculate_correlations(self, exposure_data, mediator_data, outcome_data):
        """
        è®¡ç®—å˜é‡é—´çš„ç›¸å…³æ€§
        
        è¿”å›:
        - ç›¸å…³æ€§çŸ©é˜µå’Œæ˜¾è‘—æ€§æ£€éªŒç»“æœ
        """
        data_matrix = np.column_stack([exposure_data, mediator_data, outcome_data])
        
        # è®¡ç®—Pearsonç›¸å…³ç³»æ•°
        corr_matrix = np.corrcoef(data_matrix.T)
        
        # è®¡ç®—æ˜¾è‘—æ€§
        n = len(exposure_data)
        p_values = np.zeros((3, 3))
        
        for i in range(3):
            for j in range(3):
                if i != j:
                    corr = corr_matrix[i, j]
                    t_stat = corr * np.sqrt((n - 2) / (1 - corr**2))
                    p_values[i, j] = 2 * (1 - stats.t.cdf(np.abs(t_stat), n - 2))
        
        return corr_matrix, p_values
    
    def mediation_analysis(self, exposure, mediator, outcome, 
                          exposure_type, mediator_type, outcome_type):
        """
        è¿›è¡Œä¸­ä»‹åˆ†æ
        
        è¿”å›:
        - ä¸­ä»‹åˆ†æç»“æœå­—å…¸
        """
        results = {}
        
        try:
            # è·¯å¾„cï¼šæ€»æ•ˆåº” (X -> Y)
            if outcome_type == 'numerical':
                model_c = LinearRegression()
                model_c.fit(exposure.reshape(-1, 1), outcome)
                total_effect = model_c.coef_[0]
                results['total_effect'] = total_effect
            else:
                model_c = LogisticRegression()
                model_c.fit(exposure.reshape(-1, 1), outcome)
                total_effect = model_c.coef_[0][0]
                results['total_effect'] = total_effect
            
            # è·¯å¾„aï¼šX -> M
            if mediator_type == 'numerical':
                model_a = LinearRegression()
                model_a.fit(exposure.reshape(-1, 1), mediator)
                path_a = model_a.coef_[0]
                results['path_a'] = path_a
            else:
                model_a = LogisticRegression()
                model_a.fit(exposure.reshape(-1, 1), mediator)
                path_a = model_a.coef_[0][0]
                results['path_a'] = path_a
            
            # è·¯å¾„bå’Œc'ï¼šæ§åˆ¶Mæ—¶çš„X -> Y
            X_M = np.column_stack([exposure, mediator])
            
            if outcome_type == 'numerical':
                model_b = LinearRegression()
                model_b.fit(X_M, outcome)
                path_b = model_b.coef_[1]  # M -> Y
                direct_effect = model_b.coef_[0]  # X -> Y (æ§åˆ¶M)
                results['path_b'] = path_b
                results['direct_effect'] = direct_effect
            else:
                model_b = LogisticRegression()
                model_b.fit(X_M, outcome)
                path_b = model_b.coef_[0][1]  # M -> Y
                direct_effect = model_b.coef_[0][0]  # X -> Y (æ§åˆ¶M)
                results['path_b'] = path_b
                results['direct_effect'] = direct_effect
            
            # é—´æ¥æ•ˆåº”
            indirect_effect = path_a * path_b
            results['indirect_effect'] = indirect_effect
            
            # ä¸­ä»‹æ•ˆåº”æ¯”ä¾‹
            if abs(total_effect) > 1e-10:
                mediation_ratio = indirect_effect / total_effect
                results['mediation_ratio'] = mediation_ratio
            else:
                results['mediation_ratio'] = 0
            
        except Exception as e:
            print(f"ä¸­ä»‹åˆ†æå‡ºé”™: {e}")
            results = {
                'total_effect': 0,
                'path_a': 0,
                'path_b': 0,
                'direct_effect': 0,
                'indirect_effect': 0,
                'mediation_ratio': 0
            }
        
        return results
    
    def create_visualization(self, exposure_data, mediator_data, outcome_data,
                           exposure_name, mediator_name, outcome_name,
                           mediation_results, save_path):
        """
        åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'å› æœåˆ†æ: {exposure_name} â†’ {mediator_name} â†’ {outcome_name}', 
                     fontsize=16, fontweight='bold')
        
        # 1. ç›¸å…³æ€§çƒ­å›¾
        data_df = pd.DataFrame({
            'Exposure': exposure_data,
            'Mediator': mediator_data,
            'Outcome': outcome_data
        })
        
        corr_matrix = data_df.corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                   ax=axes[0, 0], vmin=-1, vmax=1)
        axes[0, 0].set_title('å˜é‡ç›¸å…³æ€§çŸ©é˜µ')
        
        # 2. æ•£ç‚¹å›¾ - æš´éœ² vs ä¸­ä»‹
        axes[0, 1].scatter(exposure_data, mediator_data, alpha=0.6)
        axes[0, 1].set_xlabel('æš´éœ²å˜é‡')
        axes[0, 1].set_ylabel('ä¸­ä»‹å˜é‡')
        axes[0, 1].set_title(f'{exposure_name} vs {mediator_name}')
        
        # 3. æ•£ç‚¹å›¾ - ä¸­ä»‹ vs ç»“å±€
        axes[1, 0].scatter(mediator_data, outcome_data, alpha=0.6)
        axes[1, 0].set_xlabel('ä¸­ä»‹å˜é‡')
        axes[1, 0].set_ylabel('ç»“å±€å˜é‡')
        axes[1, 0].set_title(f'{mediator_name} vs {outcome_name}')
        
        # 4. ä¸­ä»‹æ•ˆåº”å›¾
        effects = [
            mediation_results['total_effect'],
            mediation_results['direct_effect'],
            mediation_results['indirect_effect']
        ]
        effect_names = ['æ€»æ•ˆåº”', 'ç›´æ¥æ•ˆåº”', 'é—´æ¥æ•ˆåº”']
        
        bars = axes[1, 1].bar(effect_names, effects)
        axes[1, 1].set_title('ä¸­ä»‹æ•ˆåº”åˆ†è§£')
        axes[1, 1].set_ylabel('æ•ˆåº”å¤§å°')
        axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.7)
        
        # ä¸ºæ¡å½¢å›¾æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, effect in zip(bars, effects):
            height = bar.get_height()
            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,
                           f'{effect:.4f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def analyze_trio(self, exposure_var, mediator_var, outcome_var):
        """
        åˆ†æä¸€ä¸ªA-B-Cä¸‰å…ƒç»„
        
        è¿”å›:
        - åˆ†æç»“æœå­—å…¸
        """
        self.current_analysis_id += 1
        
        # åˆ›å»ºåˆ†å±‚æ–‡ä»¶å¤¹ç»“æ„ï¼šA â†’ A-B â†’ A-B-C
        exposure_name = self.clean_variable_name(exposure_var['variable_name'])
        mediator_name = self.clean_variable_name(mediator_var['variable_name'])
        outcome_name = self.clean_variable_name(outcome_var['variable_name'])
        
        # å±‚æ¬¡1: A (æš´éœ²å˜é‡)
        a_folder_name = f"A_{exposure_name}"
        a_dir = self.results_base_dir / a_folder_name
        a_dir.mkdir(exist_ok=True)
        
        # å±‚æ¬¡2: A-B (æš´éœ²-ä¸­ä»‹ç»„åˆ)
        ab_folder_name = f"A_{exposure_name}__B_{mediator_name}"
        ab_dir = a_dir / ab_folder_name
        ab_dir.mkdir(exist_ok=True)
        
        # å±‚æ¬¡3: A-B-C (å…·ä½“åˆ†æ)
        abc_folder_name = f"A_{exposure_name}__B_{mediator_name}__C_{outcome_name}"
        analysis_id = f"analysis_{self.current_analysis_id:05d}_{abc_folder_name}"
        analysis_dir = ab_dir / analysis_id
        analysis_dir.mkdir(exist_ok=True)
        
        self.logger.info("\n" + "="*80)
        self.logger.info(f"å¼€å§‹åˆ†æ #{self.current_analysis_id}: {abc_folder_name}")
        self.logger.info(f"  è·¯å¾„: {a_folder_name}/{ab_folder_name}/{analysis_id}")
        self.logger.info(f"  æš´éœ²å˜é‡(A): {exposure_var['variable_name']} (æ¥è‡ª {exposure_var['file_name']})")
        self.logger.info(f"  ä¸­ä»‹å˜é‡(B): {mediator_var['variable_name']} (æ¥è‡ª {mediator_var['file_name']})")
        self.logger.info(f"  ç»“å±€å˜é‡(C): {outcome_var['variable_name']} (æ¥è‡ª {outcome_var['file_name']})")
        
        # åˆ›å»ºå½“å‰åˆ†æçš„æ—¥å¿—æ–‡ä»¶
        analysis_log_file = analysis_dir / "analysis.log"
        analysis_logger = logging.getLogger(f"analysis_{self.current_analysis_id}")
        analysis_logger.setLevel(logging.DEBUG)
        
        # æ¸…é™¤ä¹‹å‰çš„å¤„ç†å™¨
        for handler in analysis_logger.handlers[:]:
            analysis_logger.removeHandler(handler)
            
        # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(analysis_log_file, encoding='utf-8')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        analysis_logger.addHandler(file_handler)
        
        analysis_logger.info(f"å¼€å§‹åˆ†æ: {abc_folder_name}")
        analysis_logger.info(f"åˆ†æID: {analysis_id}")
        analysis_logger.info(f"åˆ†æè·¯å¾„: {a_folder_name}/{ab_folder_name}/{analysis_id}")
        
        try:
            # ç®€åŒ–æ—¥å¿—ï¼šåŠ è½½A-Bå˜é‡
            exposure_df = self.load_variable_data(exposure_var['file_name'], exposure_var['variable_name'], filter_base=False)
            mediator_df = self.load_variable_data(mediator_var['file_name'], mediator_var['variable_name'], filter_base=False)
            
            if any(df is None for df in [exposure_df, mediator_df]):
                analysis_logger.warning("è·³è¿‡ï¼šAæˆ–Bå˜é‡æ— æ•ˆ")
                self.skipped_data_invalid += 1
                return None
            
            # A-Bé‡åˆæ£€æŸ¥å·²åœ¨å¤–å±‚å¾ªç¯å®Œæˆï¼Œè¿™é‡Œç›´æ¥åŠ è½½ç»“å±€å˜é‡
            outcome_df = self.load_variable_data(outcome_var['file_name'], outcome_var['variable_name'], filter_base=False)
            
            if outcome_df is None:
                analysis_logger.warning("è·³è¿‡ï¼šCå˜é‡æ— æ•ˆ")
                self.skipped_data_invalid += 1
                return None
            
            # åˆå¹¶æ•°æ®
            merged_data = exposure_df.merge(mediator_df, on='RespondentSequenceNumber', how='inner')
            merged_data = merged_data.merge(outcome_df, on='RespondentSequenceNumber', how='inner')
            analysis_logger.info(f"åˆå¹¶æ ·æœ¬æ•°: {len(merged_data)}")
            if len(merged_data) > 0:
                final_min = merged_data['RespondentSequenceNumber'].min()
                final_max = merged_data['RespondentSequenceNumber'].max()
                analysis_logger.info(f"æœ€ç»ˆåˆå¹¶åIDèŒƒå›´: {final_min:.0f} - {final_max:.0f}")
                self.logger.info(f"  å…±åŒIDèŒƒå›´: {final_min:.0f} - {final_max:.0f} (æ ·æœ¬æ•°: {len(merged_data)})")
            
            if len(merged_data) < 100:  # æ ·æœ¬é‡å¤ªå°
                error_msg = f"æ ·æœ¬é‡ä¸è¶³ ({len(merged_data)})"
                self.logger.warning(f"  è·³è¿‡åˆ†æ #{self.current_analysis_id}: {error_msg}")
                analysis_logger.error(error_msg)
                self.failed_analyses += 1
                return None
            
            self.logger.info(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {len(merged_data)}")
            analysis_logger.info(f"æœ‰æ•ˆæ ·æœ¬æ•°: {len(merged_data)}")
            
            # æå–å˜é‡æ•°æ®
            exposure_data = merged_data[exposure_var['variable_name']]
            mediator_data = merged_data[mediator_var['variable_name']]
            outcome_data = merged_data[outcome_var['variable_name']]
            
            # åˆ¤æ–­å˜é‡ç±»å‹
            exposure_type = self.determine_variable_type(exposure_data)
            mediator_type = self.determine_variable_type(mediator_data)
            outcome_type = self.determine_variable_type(outcome_data)
            
            self.logger.info(f"  å˜é‡ç±»å‹ - æš´éœ²: {exposure_type}, ä¸­ä»‹: {mediator_type}, ç»“å±€: {outcome_type}")
            analysis_logger.info(f"å˜é‡ç±»å‹ - æš´éœ²: {exposure_type}, ä¸­ä»‹: {mediator_type}, ç»“å±€: {outcome_type}")
            
            # é¢„å¤„ç†æ•°æ®
            analysis_logger.info("å¼€å§‹é¢„å¤„ç†æ•°æ®...")
            exposure_processed = self.preprocess_variable(exposure_data, exposure_type)
            mediator_processed = self.preprocess_variable(mediator_data, mediator_type)
            outcome_processed = self.preprocess_variable(outcome_data, outcome_type)
            
            # æ£€æŸ¥é¢„å¤„ç†ç»“æœ
            if any(x is None for x in [exposure_processed, mediator_processed, outcome_processed]):
                error_msg = "æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜"
                self.logger.warning(f"  è·³è¿‡åˆ†æ #{self.current_analysis_id}: {error_msg}")
                analysis_logger.error(error_msg)
                self.failed_analyses += 1
                return None
        
        except Exception as e:
            error_msg = f"åˆ†æè¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            self.logger.error(f"  åˆ†æ #{self.current_analysis_id} å¤±è´¥: {error_msg}")
            analysis_logger.error(error_msg)
            self.failed_analyses += 1
            return None
        
        # è®¡ç®—ç›¸å…³æ€§
        analysis_logger.info("è®¡ç®—å˜é‡é—´ç›¸å…³æ€§...")
        corr_matrix, p_values = self.calculate_correlations(
            exposure_processed, mediator_processed, outcome_processed)
        
        analysis_logger.info(f"ç›¸å…³æ€§ç³»æ•°:")
        analysis_logger.info(f"  æš´éœ²-ä¸­ä»‹: {corr_matrix[0, 1]:.4f} (p={p_values[0, 1]:.4f})")
        analysis_logger.info(f"  ä¸­ä»‹-ç»“å±€: {corr_matrix[1, 2]:.4f} (p={p_values[1, 2]:.4f})")
        analysis_logger.info(f"  æš´éœ²-ç»“å±€: {corr_matrix[0, 2]:.4f} (p={p_values[0, 2]:.4f})")
        
        # ä¸­ä»‹åˆ†æ
        analysis_logger.info("è¿›è¡Œä¸­ä»‹åˆ†æ...")
        mediation_results = self.mediation_analysis(
            exposure_processed, mediator_processed, outcome_processed,
            exposure_type, mediator_type, outcome_type)
        
        analysis_logger.info(f"ä¸­ä»‹åˆ†æç»“æœ:")
        analysis_logger.info(f"  æ€»æ•ˆåº”: {mediation_results['total_effect']:.4f}")
        analysis_logger.info(f"  ç›´æ¥æ•ˆåº”: {mediation_results['direct_effect']:.4f}")
        analysis_logger.info(f"  é—´æ¥æ•ˆåº”: {mediation_results['indirect_effect']:.4f}")
        analysis_logger.info(f"  ä¸­ä»‹æ•ˆåº”æ¯”ä¾‹: {mediation_results['mediation_ratio']:.4f}")
        
        # åˆ›å»ºå¯è§†åŒ–
        analysis_logger.info("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        viz_path = analysis_dir / "visualization.png"
        self.create_visualization(
            exposure_processed, mediator_processed, outcome_processed,
            exposure_var['variable_name'], mediator_var['variable_name'], 
            outcome_var['variable_name'], mediation_results, viz_path)
        analysis_logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {viz_path}")
        
        # å¤åˆ¶å¯è§†åŒ–å›¾è¡¨åˆ°æˆåŠŸæ”¶é›†ç›®å½•
        try:
            # ä½¿ç”¨åˆ†å±‚ä¿¡æ¯åˆ›å»ºæ›´æœ‰æ„ä¹‰çš„æ–‡ä»¶å
            chart_name = f"{a_folder_name}__{ab_folder_name.split('__', 1)[1]}__{analysis_id.split('__')[-1]}_visualization.png"
            successful_chart_path = self.successful_charts_dir / chart_name
            shutil.copy2(viz_path, successful_chart_path)
            analysis_logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²å¤åˆ¶åˆ°æˆåŠŸæ”¶é›†ç›®å½•: {successful_chart_path}")
            self.logger.info(f"  âœ“ å›¾è¡¨å·²æ”¶é›†: {chart_name}")
            self.logger.info(f"  âœ“ è·¯å¾„: {a_folder_name}/{ab_folder_name}/{analysis_id}")
        except Exception as e:
            analysis_logger.warning(f"å¤åˆ¶å›¾è¡¨åˆ°æ”¶é›†ç›®å½•å¤±è´¥: {e}")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        analysis_logger.info("ä¿å­˜åˆ†æç»“æœ...")
        
        detailed_results = {
            'analysis_info': {
                'analysis_id': analysis_id,
                'folder_name': abc_folder_name,
                'full_path': f"{a_folder_name}/{ab_folder_name}/{analysis_id}",
                'timestamp': datetime.now().isoformat(),
                'status': 'completed'
            },
            'variables': {
                'exposure': {
                    'name': exposure_var['variable_name'],
                    'file': exposure_var['file_name'],
                    'type': exposure_type,
                    'category': exposure_var['category'],
                    'index': exposure_var['variable_index']
                },
                'mediator': {
                    'name': mediator_var['variable_name'],
                    'file': mediator_var['file_name'],
                    'type': mediator_type,
                    'category': mediator_var['category'],
                    'index': mediator_var['variable_index']
                },
                'outcome': {
                    'name': outcome_var['variable_name'],
                    'file': outcome_var['file_name'],
                    'type': outcome_type,
                    'category': outcome_var['category'],
                    'index': outcome_var['variable_index']
                }
            },
            'sample_size': len(merged_data),
            'correlations': {
                'exposure_mediator': float(corr_matrix[0, 1]),
                'mediator_outcome': float(corr_matrix[1, 2]),
                'exposure_outcome': float(corr_matrix[0, 2])
            },
            'p_values': {
                'exposure_mediator': float(p_values[0, 1]),
                'mediator_outcome': float(p_values[1, 2]),
                'exposure_outcome': float(p_values[0, 2])
            },
            'mediation_analysis': mediation_results,
            'data_summary': {
                'exposure': {
                    'mean': float(exposure_data.mean()),
                    'std': float(exposure_data.std()),
                    'min': float(exposure_data.min()),
                    'max': float(exposure_data.max()),
                    'unique_count': int(exposure_data.nunique())
                },
                'mediator': {
                    'mean': float(mediator_data.mean()),
                    'std': float(mediator_data.std()),
                    'min': float(mediator_data.min()),
                    'max': float(mediator_data.max()),
                    'unique_count': int(mediator_data.nunique())
                },
                'outcome': {
                    'mean': float(outcome_data.mean()),
                    'std': float(outcome_data.std()),
                    'min': float(outcome_data.min()),
                    'max': float(outcome_data.max()),
                    'unique_count': int(outcome_data.nunique())
                }
            },
            'file_paths': {
                'visualization': 'visualization.png',
                'data': 'data.csv',
                'log': 'analysis.log',
                'results': 'results.json'
            }
        }
        
        # ä¿å­˜JSONç»“æœ
        results_file = analysis_dir / "results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        analysis_logger.info(f"ç»“æœå·²ä¿å­˜: {results_file}")
        
        # ä¿å­˜CSVæ•°æ®
        data_file = analysis_dir / "data.csv"
        merged_data.to_csv(data_file, index=False, encoding='utf-8')
        analysis_logger.info(f"æ•°æ®å·²ä¿å­˜: {data_file}")
        
        # ä¿å­˜æ•°æ®æè¿°æ€§ç»Ÿè®¡
        stats_file = analysis_dir / "descriptive_stats.csv"
        stats_df = merged_data.describe(include='all')
        stats_df.to_csv(stats_file, encoding='utf-8')
        analysis_logger.info(f"æè¿°æ€§ç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
        
        # æ·»åŠ åˆ°æ€»ç»“æœ
        self.analysis_results.append(detailed_results)
        self.successful_analyses += 1
        
        self.logger.info(f"  âœ“ å®Œæˆåˆ†æ #{self.current_analysis_id}: {abc_folder_name}")
        analysis_logger.info("åˆ†æå®Œæˆï¼")
        
        # å…³é—­åˆ†ææ—¥å¿—å¤„ç†å™¨
        for handler in analysis_logger.handlers[:]:
            handler.close()
            analysis_logger.removeHandler(handler)
        
        return detailed_results
    
    def run_full_analysis(self, max_analyses=None):
        """
        è¿è¡Œå®Œæ•´çš„å› æœåˆ†æ
        
        å‚æ•°:
        - max_analyses: æœ€å¤§åˆ†ææ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        start_time = datetime.now()
        self.logger.info("å¼€å§‹å…¨é¢å› æœåˆ†æ...")
        
        total_possible = len(self.exposure_vars) * len(self.mediator_vars) * len(self.outcome_vars)
        self.logger.info(f"è®¡åˆ’åˆ†æç»„åˆæ•°: {len(self.exposure_vars)} Ã— {len(self.mediator_vars)} Ã— {len(self.outcome_vars)} = {total_possible}")
        
        if max_analyses:
            self.logger.info(f"é™åˆ¶æœ€å¤§åˆ†ææ•°é‡: {max_analyses}")
        
        total_combinations = 0
        
        for i, exposure_var in self.exposure_vars.iterrows():
            self.logger.info(f"\nå¤„ç†æš´éœ²å˜é‡ {i+1}/{len(self.exposure_vars)}: {exposure_var['variable_name']}")
            
            for j, mediator_var in self.mediator_vars.iterrows():
                # ç¡®ä¿ä¸­ä»‹å˜é‡å’Œæš´éœ²å˜é‡ä¸åŒ
                if mediator_var['variable_name'] == exposure_var['variable_name']:
                    continue
                
                # æå‰æ£€æŸ¥A-Bç»„åˆæ˜¯å¦æœ‰é‡åˆçš„RespondentSequenceNumber
                self.logger.info(f"  æ£€æŸ¥A-Bç»„åˆ: {exposure_var['variable_name']} vs {mediator_var['variable_name']}")
                
                try:
                    # åŠ è½½Aå’ŒBå˜é‡æ•°æ®æ¥æ£€æŸ¥IDé‡åˆ
                    exposure_df = self.load_variable_data(exposure_var['file_name'], exposure_var['variable_name'], filter_base=False)
                    mediator_df = self.load_variable_data(mediator_var['file_name'], mediator_var['variable_name'], filter_base=False)
                    
                    if exposure_df is None or mediator_df is None:
                        self.logger.warning(f"  âš ï¸ A-Bæ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æ•´ä¸ªA-Bç»„åˆ")
                        continue
                    
                    # æ£€æŸ¥IDé‡åˆ
                    exposure_ids = set(exposure_df['RespondentSequenceNumber'])
                    mediator_ids = set(mediator_df['RespondentSequenceNumber'])
                    ab_intersection = exposure_ids & mediator_ids
                    
                    self.logger.info(f"  Aå˜é‡IDæ•°: {len(exposure_ids)}, Bå˜é‡IDæ•°: {len(mediator_ids)}")
                    self.logger.info(f"  A-Bäº¤é›†æ•°é‡: {len(ab_intersection)}")
                    
                    if len(ab_intersection) == 0:
                        self.logger.info(f"  âš ï¸ A-Bæ— å…±åŒæ ·æœ¬ï¼Œè·³è¿‡æ•´ä¸ªA-Bç»„åˆï¼ˆèŠ‚çœæ‰€æœ‰Cå˜é‡æ£€æŸ¥ï¼‰")
                        self.skipped_ab_invalid += len(self.outcome_vars)  # ç»Ÿè®¡è·³è¿‡çš„A-B-Cç»„åˆæ•°
                        continue
                    
                    if len(ab_intersection) < 100:
                        self.logger.info(f"  âš ï¸ A-Bå…±åŒæ ·æœ¬å¤ªå°‘ ({len(ab_intersection)})ï¼Œè·³è¿‡æ•´ä¸ªA-Bç»„åˆ")
                        self.skipped_ab_invalid += len(self.outcome_vars)
                        continue
                    
                    # è¿›ä¸€æ­¥æ£€æŸ¥A-Båˆå¹¶åçš„å˜å¼‚æ€§
                    ab_merged = exposure_df.merge(mediator_df, on='RespondentSequenceNumber', how='inner')
                    if len(ab_merged) == 0:
                        self.logger.info(f"  âš ï¸ A-Båˆå¹¶åæ— æ•°æ®ï¼Œè·³è¿‡æ•´ä¸ªA-Bç»„åˆ")
                        self.skipped_ab_invalid += len(self.outcome_vars)
                        continue
                    
                    # æ£€æŸ¥å˜é‡å˜å¼‚æ€§ï¼ˆè¿™æ˜¯å…³é”®ä¼˜åŒ–ï¼ï¼‰
                    exposure_var_name = exposure_var['variable_name']
                    mediator_var_name = mediator_var['variable_name']
                    
                    exposure_unique = ab_merged[exposure_var_name].nunique()
                    mediator_unique = ab_merged[mediator_var_name].nunique()
                    
                    self.logger.info(f"  å˜å¼‚æ€§æ£€æŸ¥: Aå˜é‡ {exposure_unique} ä¸ªä¸åŒå€¼, Bå˜é‡ {mediator_unique} ä¸ªä¸åŒå€¼")
                    
                    if exposure_unique < 2:
                        self.logger.info(f"  âŒ Aå˜é‡æ— å˜å¼‚æ€§ (åªæœ‰ {exposure_unique} ä¸ªå€¼), è·³è¿‡æ•´ä¸ªA-Bç»„åˆ")
                        self.skipped_ab_invalid += len(self.outcome_vars)
                        continue
                    
                    if mediator_unique < 2:
                        self.logger.info(f"  âŒ Bå˜é‡æ— å˜å¼‚æ€§ (åªæœ‰ {mediator_unique} ä¸ªå€¼), è·³è¿‡æ•´ä¸ªA-Bç»„åˆ")
                        self.skipped_ab_invalid += len(self.outcome_vars)
                        continue
                    
                    self.logger.info(f"  âœ… A-Bç»„åˆæœ‰æ•ˆ ({len(ab_intersection)} å…±åŒæ ·æœ¬ï¼Œå˜å¼‚æ€§å……è¶³)ï¼Œå¼€å§‹æ£€æŸ¥æ‰€æœ‰Cå˜é‡")
                    
                    # åˆ›å»ºA-Bç»„åˆçš„æ–‡ä»¶å¤¹ç»“æ„ï¼ˆä¸ºåç»­åˆ†æåšå‡†å¤‡ï¼‰
                    exposure_name_clean = self.clean_variable_name(exposure_var['variable_name'])
                    mediator_name_clean = self.clean_variable_name(mediator_var['variable_name'])
                    a_folder_name_prep = f"A_{exposure_name_clean}"
                    ab_folder_name_prep = f"A_{exposure_name_clean}__B_{mediator_name_clean}"
                    
                    a_dir_prep = self.results_base_dir / a_folder_name_prep
                    ab_dir_prep = a_dir_prep / ab_folder_name_prep
                    a_dir_prep.mkdir(exist_ok=True)
                    ab_dir_prep.mkdir(exist_ok=True)
                    
                    self.logger.info(f"  ğŸ“ å·²å‡†å¤‡A-Bç»„åˆç›®å½•: {a_folder_name_prep}/{ab_folder_name_prep}")
                    
                except Exception as e:
                    self.logger.error(f"  A-Bé¢„æ£€æŸ¥å‡ºé”™: {e}ï¼Œè·³è¿‡æ•´ä¸ªA-Bç»„åˆ")
                    continue
                
                # åªæœ‰A-Bæœ‰æ•ˆé‡åˆæ—¶ï¼Œæ‰è¿›è¡Œæ‰€æœ‰Cå˜é‡çš„åˆ†æ
                for k, outcome_var in self.outcome_vars.iterrows():
                    # ç¡®ä¿ç»“å±€å˜é‡å’Œå‰ä¸¤ä¸ªå˜é‡ä¸åŒ
                    if (outcome_var['variable_name'] == exposure_var['variable_name'] or
                        outcome_var['variable_name'] == mediator_var['variable_name']):
                        continue
                    
                    total_combinations += 1
                    
                    if max_analyses and self.successful_analyses >= max_analyses:
                        self.logger.info(f"è¾¾åˆ°æœ€å¤§åˆ†ææ•°é‡é™åˆ¶ {max_analyses}")
                        break
                    
                    try:
                        result = self.analyze_trio(exposure_var, mediator_var, outcome_var)
                        
                        # æ¯10ä¸ªåˆ†æè¾“å‡ºè¿›åº¦
                        if total_combinations % 10 == 0:
                            success_rate = (self.successful_analyses / total_combinations) * 100
                            self.logger.info(f"è¿›åº¦: {total_combinations} ç»„åˆå®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1f}%")
                            
                    except Exception as e:
                        self.logger.error(f"åˆ†æå‡ºé”™: {e}")
                        self.failed_analyses += 1
                        continue
                
                if max_analyses and self.successful_analyses >= max_analyses:
                    break
            
            if max_analyses and self.successful_analyses >= max_analyses:
                break
        
        # åˆ†æå®Œæˆç»Ÿè®¡
        end_time = datetime.now()
        duration = end_time - start_time
        
        self.logger.info("\n" + "="*60)
        self.logger.info("åˆ†æå®Œæˆï¼")
        self.logger.info("="*60)
        self.logger.info(f"æ€»åˆ†æç»„åˆæ•°: {total_combinations}")
        self.logger.info(f"æˆåŠŸåˆ†ææ•°: {self.successful_analyses}")
        self.logger.info(f"å¤±è´¥åˆ†ææ•°: {self.failed_analyses}")
        self.logger.info(f"A-Bæ— æ•ˆè·³è¿‡æ•°: {self.skipped_ab_invalid}")
        self.logger.info(f"æ•°æ®æ— æ•ˆè·³è¿‡æ•°: {self.skipped_data_invalid}")
        total_skipped = self.skipped_ab_invalid + self.skipped_data_invalid
        self.logger.info(f"æˆåŠŸç‡: {(self.successful_analyses/total_combinations)*100:.1f}%")
        self.logger.info(f"æ€»è·³è¿‡ç‡: {(total_skipped/total_combinations)*100:.1f}%")
        self.logger.info(f"åˆ†æè€—æ—¶: {duration}")
        self.logger.info(f"ä¼˜åŒ–æ•ˆæœ: è·³è¿‡äº† {total_skipped} ä¸ªæ— æ•ˆç»„åˆï¼ŒèŠ‚çœäº†è®¡ç®—æ—¶é—´")
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        if self.successful_analyses > 0:
            self.generate_summary_report()
        else:
            self.logger.warning("æ²¡æœ‰æˆåŠŸçš„åˆ†æï¼Œè·³è¿‡ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆæ€»ç»“æŠ¥å‘Š...")
        
        summary_dir = self.results_base_dir / "summary"
        summary_dir.mkdir(exist_ok=True)
        
        if not self.analysis_results:
            self.logger.warning("æ²¡æœ‰åˆ†æç»“æœå¯ä¾›æ±‡æ€»")
            return
        
        # åˆ›å»ºæ€»ç»“DataFrame
        summary_data = []
        for result in self.analysis_results:
            summary_data.append({
                'analysis_id': result['analysis_info']['analysis_id'],
                'folder_name': result['analysis_info']['folder_name'],
                'exposure_name': result['variables']['exposure']['name'],
                'mediator_name': result['variables']['mediator']['name'],
                'outcome_name': result['variables']['outcome']['name'],
                'exposure_type': result['variables']['exposure']['type'],
                'mediator_type': result['variables']['mediator']['type'],
                'outcome_type': result['variables']['outcome']['type'],
                'sample_size': result['sample_size'],
                'exposure_mediator_corr': result['correlations']['exposure_mediator'],
                'mediator_outcome_corr': result['correlations']['mediator_outcome'],
                'exposure_outcome_corr': result['correlations']['exposure_outcome'],
                'total_effect': result['mediation_analysis']['total_effect'],
                'direct_effect': result['mediation_analysis']['direct_effect'],
                'indirect_effect': result['mediation_analysis']['indirect_effect'],
                'mediation_ratio': result['mediation_analysis']['mediation_ratio'],
                'timestamp': result['analysis_info']['timestamp']
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_file = summary_dir / "analysis_summary.csv"
        summary_df.to_csv(summary_file, index=False, encoding='utf-8')
        self.logger.info(f"åˆ†ææ±‡æ€»è¡¨å·²ä¿å­˜: {summary_file}")
        
        # ä¿å­˜å®Œæ•´ç»“æœJSON
        all_results_file = summary_dir / "all_results.json"
        with open(all_results_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        self.logger.info(f"å®Œæ•´ç»“æœå·²ä¿å­˜: {all_results_file}")
        
        # åˆ›å»ºåˆ†æç»Ÿè®¡æŠ¥å‘Š
        total_attempts = self.successful_analyses + self.failed_analyses + self.skipped_ab_invalid + self.skipped_data_invalid
        total_skipped = self.skipped_ab_invalid + self.skipped_data_invalid
        stats_report = {
            'analysis_statistics': {
                'total_successful': self.successful_analyses,
                'total_failed': self.failed_analyses,
                'total_skipped_ab_invalid': self.skipped_ab_invalid,
                'total_skipped_data_invalid': self.skipped_data_invalid,
                'total_attempts': total_attempts,
                'success_rate': (self.successful_analyses / total_attempts) * 100 if total_attempts > 0 else 0,
                'skip_rate': (total_skipped / total_attempts) * 100 if total_attempts > 0 else 0,
                'optimization_benefit': f"Saved {total_skipped} analysis attempts (overlap check + integer filter)",
                'total_variables_used': {
                    'exposure': len(self.exposure_vars),
                    'mediator': len(self.mediator_vars),
                    'outcome': len(self.outcome_vars)
                }
            },
            'data_quality_summary': {
                'average_sample_size': float(summary_df['sample_size'].mean()),
                'median_sample_size': float(summary_df['sample_size'].median()),
                'min_sample_size': int(summary_df['sample_size'].min()),
                'max_sample_size': int(summary_df['sample_size'].max())
            },
            'effect_size_summary': {
                'significant_mediation_count': len(summary_df[abs(summary_df['mediation_ratio']) > 0.1]),
                'strong_mediation_count': len(summary_df[abs(summary_df['mediation_ratio']) > 0.3]),
                'average_mediation_ratio': float(summary_df['mediation_ratio'].mean())
            }
        }
        
        stats_file = summary_dir / "analysis_statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_report, f, ensure_ascii=False, indent=2)
        self.logger.info(f"åˆ†æç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {stats_file}")
        
        # åˆ›å»ºæ€»ç»“å¯è§†åŒ–
        self.create_summary_visualization(summary_df, summary_dir)
        
        # åˆ›å»ºæˆåŠŸå›¾è¡¨ç´¢å¼•
        self.create_successful_charts_index()
        
        # ç”Ÿæˆæ±‡æ€»æ•°æ®è¡¨ï¼ˆå‰10000æ¡ï¼‰
        self.create_summary_table(summary_dir, max_rows=10000)
        
        self.logger.info(f"æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_dir}")
        self.logger.info(f"ç”Ÿæˆäº† {len(self.analysis_results)} ä¸ªæˆåŠŸåˆ†æçš„è¯¦ç»†æŠ¥å‘Š")
        self.logger.info(f"æˆåŠŸå›¾è¡¨æ”¶é›†åœ¨: {self.successful_charts_dir}")
        
        # ç»Ÿè®¡æˆåŠŸå›¾è¡¨æ•°é‡
        chart_files = list(self.successful_charts_dir.glob("*.png"))
        self.logger.info(f"å…±æ”¶é›†äº† {len(chart_files)} ä¸ªå¯è§†åŒ–å›¾è¡¨")
    
    def create_summary_visualization(self, summary_df, save_dir):
        """åˆ›å»ºæ€»ç»“å¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('NHANESå› æœåˆ†ææ€»ç»“', fontsize=16, fontweight='bold')
        
        # 1. ä¸­ä»‹æ•ˆåº”æ¯”ä¾‹åˆ†å¸ƒ
        axes[0, 0].hist(summary_df['mediation_ratio'], bins=50, alpha=0.7, edgecolor='black')
        axes[0, 0].set_xlabel('ä¸­ä»‹æ•ˆåº”æ¯”ä¾‹')
        axes[0, 0].set_ylabel('é¢‘æ¬¡')
        axes[0, 0].set_title('ä¸­ä»‹æ•ˆåº”æ¯”ä¾‹åˆ†å¸ƒ')
        axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
        
        # 2. æ ·æœ¬é‡åˆ†å¸ƒ
        axes[0, 1].hist(summary_df['sample_size'], bins=50, alpha=0.7, edgecolor='black')
        axes[0, 1].set_xlabel('æ ·æœ¬é‡')
        axes[0, 1].set_ylabel('é¢‘æ¬¡')
        axes[0, 1].set_title('æ ·æœ¬é‡åˆ†å¸ƒ')
        
        # 3. æ•ˆåº”å¤§å°æ•£ç‚¹å›¾
        axes[1, 0].scatter(summary_df['total_effect'], summary_df['indirect_effect'], alpha=0.6)
        axes[1, 0].set_xlabel('æ€»æ•ˆåº”')
        axes[1, 0].set_ylabel('é—´æ¥æ•ˆåº”')
        axes[1, 0].set_title('æ€»æ•ˆåº” vs é—´æ¥æ•ˆåº”')
        axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.7)
        axes[1, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)
        
        # 4. ç›¸å…³æ€§çƒ­å›¾
        corr_cols = ['exposure_mediator_corr', 'mediator_outcome_corr', 'exposure_outcome_corr']
        corr_matrix = summary_df[corr_cols].corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                   ax=axes[1, 1], vmin=-1, vmax=1)
        axes[1, 1].set_title('ç›¸å…³æ€§æ¨¡å¼çš„ç›¸å…³æ€§')
        
        plt.tight_layout()
        plt.savefig(save_dir / "summary_visualization.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_summary_table(self, summary_dir, max_rows=10000):
        """åˆ›å»ºæ±‡æ€»æ•°æ®è¡¨"""
        if not self.analysis_results:
            return
            
        self.logger.info("åˆ›å»ºæ±‡æ€»æ•°æ®è¡¨...")
        
        # å‡†å¤‡æ±‡æ€»æ•°æ®
        summary_data = []
        for i, result in enumerate(self.analysis_results[:max_rows]):
            try:
                row = {
                    'Analysis_ID': result['analysis_info']['analysis_id'],
                    'A_Variable': result['variables']['exposure']['name'],
                    'B_Variable': result['variables']['mediator']['name'], 
                    'C_Variable': result['variables']['outcome']['name'],
                    'A_File': result['variables']['exposure']['file_name'],
                    'B_File': result['variables']['mediator']['file_name'],
                    'C_File': result['variables']['outcome']['file_name'],
                    'Sample_Size': result['sample_size'],
                    'A_Type': result['variables']['exposure']['type'],
                    'B_Type': result['variables']['mediator']['type'],
                    'C_Type': result['variables']['outcome']['type'],
                    'Total_Effect': result['mediation_analysis']['total_effect'],
                    'Direct_Effect': result['mediation_analysis']['direct_effect'],
                    'Indirect_Effect': result['mediation_analysis']['indirect_effect'],
                    'Mediation_Ratio': result['mediation_analysis']['mediation_ratio'],
                    'A_B_Correlation': result['correlations']['exposure_mediator']['coefficient'],
                    'B_C_Correlation': result['correlations']['mediator_outcome']['coefficient'], 
                    'A_C_Correlation': result['correlations']['exposure_outcome']['coefficient'],
                    'A_B_P_Value': result['correlations']['exposure_mediator']['p_value'],
                    'B_C_P_Value': result['correlations']['mediator_outcome']['p_value'],
                    'A_C_P_Value': result['correlations']['exposure_outcome']['p_value'],
                    'Timestamp': result['analysis_info']['timestamp'][:19],  # å»æ‰æ¯«ç§’
                    'Status': result['analysis_info']['status']
                }
                summary_data.append(row)
            except Exception as e:
                self.logger.warning(f"å¤„ç†ç»“æœ {i} æ—¶å‡ºé”™: {e}")
                continue
        
        if summary_data:
            # åˆ›å»ºDataFrame
            summary_df = pd.DataFrame(summary_data)
            
            # ä¿å­˜ä¸ºCSV
            csv_path = summary_dir / f"summary_table_{len(summary_data)}_results.csv"
            summary_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            # ä¿å­˜ä¸ºExcel
            excel_path = summary_dir / f"summary_table_{len(summary_data)}_results.xlsx"
            summary_df.to_excel(excel_path, index=False)
            
            self.logger.info(f"æ±‡æ€»æ•°æ®è¡¨å·²ä¿å­˜: {csv_path}")
            self.logger.info(f"æ±‡æ€»æ•°æ®è¡¨å·²ä¿å­˜: {excel_path}")
            self.logger.info(f"æ•°æ®è¡¨åŒ…å« {len(summary_data)} è¡Œï¼Œ{len(summary_df.columns)} åˆ—")
        else:
            self.logger.warning("æ— æœ‰æ•ˆæ•°æ®ç”Ÿæˆæ±‡æ€»è¡¨")
    
    def create_successful_charts_index(self):
        """åˆ›å»ºæˆåŠŸå›¾è¡¨çš„ç´¢å¼•æ–‡ä»¶"""
        self.logger.info("åˆ›å»ºæˆåŠŸå›¾è¡¨ç´¢å¼•...")
        
        chart_index = []
        chart_files = list(self.successful_charts_dir.glob("*.png"))
        
        for chart_file in sorted(chart_files):
            # ä»æ–‡ä»¶åä¸­æå–ä¿¡æ¯ï¼ˆæ–°çš„åˆ†å±‚å‘½åæ ¼å¼ï¼‰
            file_name = chart_file.name
            if file_name.endswith("_visualization.png"):
                # æ–°æ ¼å¼: A_exposure__B_mediator__C_outcome_visualization.png
                # éœ€è¦é‡æ–°æ„é€ analysis_idæ¥åŒ¹é…
                base_name = file_name.replace("_visualization.png", "")
                
                # æŸ¥æ‰¾å¯¹åº”çš„åˆ†æç»“æœï¼ˆé€šè¿‡folder_nameåŒ¹é…ï¼‰
                matching_result = None
                for result in self.analysis_results:
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«åˆ†æçš„folder_nameä¿¡æ¯
                    if (result['analysis_info']['folder_name'] in base_name or 
                        base_name in result['analysis_info'].get('full_path', '')):
                        matching_result = result
                        break
                
                if matching_result:
                    chart_info = {
                        'chart_file': file_name,
                        'analysis_id': matching_result['analysis_info']['analysis_id'],
                        'folder_name': matching_result['analysis_info']['folder_name'],
                        'full_path': matching_result['analysis_info'].get('full_path', ''),
                        'exposure_variable': matching_result['variables']['exposure']['name'],
                        'mediator_variable': matching_result['variables']['mediator']['name'],
                        'outcome_variable': matching_result['variables']['outcome']['name'],
                        'sample_size': matching_result['sample_size'],
                        'mediation_ratio': matching_result['mediation_analysis']['mediation_ratio'],
                        'total_effect': matching_result['mediation_analysis']['total_effect'],
                        'indirect_effect': matching_result['mediation_analysis']['indirect_effect'],
                        'timestamp': matching_result['analysis_info']['timestamp']
                    }
                    chart_index.append(chart_info)
        
        # ä¿å­˜å›¾è¡¨ç´¢å¼•CSV
        if chart_index:
            chart_index_df = pd.DataFrame(chart_index)
            index_file = self.successful_charts_dir / "charts_index.csv"
            chart_index_df.to_csv(index_file, index=False, encoding='utf-8')
            self.logger.info(f"å›¾è¡¨ç´¢å¼•å·²ä¿å­˜: {index_file}")
            
            # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†ç´¢å¼•
            index_json_file = self.successful_charts_dir / "charts_index.json"
            with open(index_json_file, 'w', encoding='utf-8') as f:
                json.dump(chart_index, f, ensure_ascii=False, indent=2)
            self.logger.info(f"å›¾è¡¨è¯¦ç»†ç´¢å¼•å·²ä¿å­˜: {index_json_file}")
            
            # åˆ›å»ºå›¾è¡¨æ¦‚è§ˆHTML
            self.create_charts_overview_html(chart_index)
        else:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æˆåŠŸçš„å›¾è¡¨æ–‡ä»¶")
    
    def create_charts_overview_html(self, chart_index):
        """åˆ›å»ºå›¾è¡¨æ¦‚è§ˆçš„HTMLæ–‡ä»¶"""
        try:
            html_content = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NHANESå› æœåˆ†æå›¾è¡¨æ¦‚è§ˆ</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1 { color: #333; text-align: center; margin-bottom: 30px; }
        .chart-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 20px; }
        .chart-item { border: 1px solid #ddd; border-radius: 8px; padding: 15px; background-color: #fafafa; }
        .chart-item img { width: 100%; height: auto; border-radius: 5px; margin-bottom: 10px; }
        .chart-info h3 { margin: 0 0 10px 0; color: #2c3e50; font-size: 16px; }
        .chart-info p { margin: 5px 0; font-size: 14px; color: #555; }
        .effect-value { font-weight: bold; }
        .positive { color: #27ae60; }
        .negative { color: #e74c3c; }
        .stats { background-color: #ecf0f1; padding: 15px; border-radius: 5px; margin-bottom: 20px; }
        .stats h2 { margin-top: 0; color: #2c3e50; }
    </style>
</head>
<body>
    <div class="container">
        <h1>NHANESå› æœåˆ†æå›¾è¡¨æ¦‚è§ˆ</h1>
        
        <div class="stats">
            <h2>åˆ†æç»Ÿè®¡</h2>
            <p><strong>æˆåŠŸåˆ†ææ•°é‡:</strong> {total_charts}</p>
            <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {generation_time}</p>
        </div>
        
        <div class="chart-grid">
""".format(
                total_charts=len(chart_index),
                generation_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            )
            
            for chart in chart_index:
                effect_class = "positive" if chart['mediation_ratio'] > 0 else "negative"
                
                html_content += f"""
            <div class="chart-item">
                <img src="{chart['chart_file']}" alt="åˆ†æå›¾è¡¨">
                <div class="chart-info">
                    <h3>{chart['folder_name']}</h3>
                    <p><strong>æ–‡ä»¶è·¯å¾„:</strong> <small>{chart.get('full_path', 'N/A')}</small></p>
                    <p><strong>æš´éœ²å˜é‡:</strong> {chart['exposure_variable']}</p>
                    <p><strong>ä¸­ä»‹å˜é‡:</strong> {chart['mediator_variable']}</p>
                    <p><strong>ç»“å±€å˜é‡:</strong> {chart['outcome_variable']}</p>
                    <p><strong>æ ·æœ¬é‡:</strong> {chart['sample_size']}</p>
                    <p><strong>ä¸­ä»‹æ•ˆåº”æ¯”ä¾‹:</strong> <span class="effect-value {effect_class}">{chart['mediation_ratio']:.4f}</span></p>
                    <p><strong>æ€»æ•ˆåº”:</strong> <span class="effect-value">{chart['total_effect']:.4f}</span></p>
                    <p><strong>é—´æ¥æ•ˆåº”:</strong> <span class="effect-value">{chart['indirect_effect']:.4f}</span></p>
                    <p><strong>åˆ†æID:</strong> {chart['analysis_id']}</p>
                </div>
            </div>
"""
            
            html_content += """
        </div>
    </div>
</body>
</html>
"""
            
            html_file = self.successful_charts_dir / "charts_overview.html"
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            self.logger.info(f"å›¾è¡¨æ¦‚è§ˆHTMLå·²åˆ›å»º: {html_file}")
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºHTMLæ¦‚è§ˆå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("NHANESå› æœæ¨æ–­è‡ªåŠ¨åŒ–åˆ†æç³»ç»Ÿ")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = NHANESCausalAnalyzer()
        
        # è¿è¡Œåˆ†æï¼ˆé™åˆ¶å‰10000ä¸ªç»„åˆï¼‰
        analyzer.run_full_analysis(max_analyses=10000)
        
        analyzer.logger.info("\n" + "="*60)
        analyzer.logger.info("åˆ†æç³»ç»Ÿè¿è¡Œå®Œæˆï¼")
        analyzer.logger.info(f"ç»“æœä¿å­˜åœ¨ {analyzer.results_base_dir} ç›®å½•ä¸­")
        analyzer.logger.info(f"æ—¥å¿—æ–‡ä»¶: {analyzer.log_file}")
        
        # è¾“å‡ºæˆåŠŸå›¾è¡¨æ”¶é›†ä¿¡æ¯
        chart_files = list(analyzer.successful_charts_dir.glob("*.png"))
        if chart_files:
            analyzer.logger.info(f"æˆåŠŸç”Ÿæˆå›¾è¡¨æ•°é‡: {len(chart_files)}")
            analyzer.logger.info(f"å›¾è¡¨æ”¶é›†ç›®å½•: {analyzer.successful_charts_dir}")
            analyzer.logger.info(f"å›¾è¡¨æ¦‚è§ˆé¡µé¢: {analyzer.successful_charts_dir / 'charts_overview.html'}")
        else:
            analyzer.logger.info("æœªç”ŸæˆæˆåŠŸçš„å›¾è¡¨")
        
        analyzer.logger.info("="*60)
        
    except Exception as e:
        print(f"ç³»ç»Ÿè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 